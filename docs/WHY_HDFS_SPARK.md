# PostgreSQL vs HDFS/Spark 완벽 비교 가이드

> 언제, 왜 HDFS/Spark를 선택하는가? 아키텍처부터 I/O 패턴까지 완벽 분석

---

## 📋 이 문서의 목적

1. **PostgreSQL 분산 옵션**과 한계 이해
2. **HDFS/Spark 선택 이유** 명확화
3. **I/O 패턴 차이**로 인한 스토리지 요구사항 이해
4. **상황별 최적 시스템** 선택 가이드

---

## 1️⃣ PostgreSQL 분산/확장 옵션

### 1.1 Streaming Replication (읽기 분산)

```
┌─────────────────────────────────────────────────────────────────────┐
│                    Streaming Replication                             │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│              ┌─────────────┐                                        │
│              │   Primary   │◀── Write (모든 쓰기)                   │
│              │  (Master)   │                                        │
│              └──────┬──────┘                                        │
│                     │ WAL 복제                                       │
│           ┌─────────┼─────────┐                                     │
│           ▼         ▼         ▼                                     │
│     ┌─────────┐ ┌─────────┐ ┌─────────┐                            │
│     │ Replica │ │ Replica │ │ Replica │◀── Read (읽기 분산)        │
│     └─────────┘ └─────────┘ └─────────┘                            │
│                                                                      │
│  장점: 읽기 부하 분산, 고가용성                                      │
│  한계: Write는 여전히 단일 노드 ❌                                   │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

**적합한 경우:**
- 읽기 비율이 높은 서비스 (90% Read, 10% Write)
- 고가용성 필요 (Primary 장애 시 Replica 승격)

**한계:**
- Write 성능은 확장 불가
- 대용량 쓰기 작업에 부적합

---

### 1.2 Citus (분산 PostgreSQL)

```
┌─────────────────────────────────────────────────────────────────────┐
│                         Citus 아키텍처                               │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│                      ┌─────────────────┐                            │
│                      │   Coordinator   │◀── SQL 쿼리                │
│                      │   (라우팅)       │                            │
│                      └────────┬────────┘                            │
│                               │                                      │
│              ┌────────────────┼────────────────┐                    │
│              ▼                ▼                ▼                    │
│        ┌──────────┐    ┌──────────┐    ┌──────────┐                │
│        │ Worker 1 │    │ Worker 2 │    │ Worker 3 │                │
│        │ Shard A  │    │ Shard B  │    │ Shard C  │                │
│        │ Shard D  │    │ Shard E  │    │ Shard F  │                │
│        └──────────┘    └──────────┘    └──────────┘                │
│                                                                      │
│  샤딩 키: user_id, tenant_id 등으로 데이터 분산                      │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

**장점:**
- Write/Read 모두 분산 가능
- PostgreSQL 완벽 호환 (SQL 그대로 사용)
- ACID 트랜잭션 지원
- 실시간 조회 (ms 단위)

**한계:**
- TB급 한계 (PB급 어려움)
- 샤딩 키 설계 필수 (잘못 설계하면 성능 저하)
- Cross-shard 쿼리 느림 (여러 샤드 JOIN)
- 저장 비용 높음 (SSD 권장)

---

### 1.3 기타 옵션 요약

| 옵션 | 설명 | Write 분산 | 한계 |
|------|------|-----------|------|
| **Partitioning** | 단일 노드 내 테이블 분할 | ❌ | 단일 노드 한계 |
| **PgPool-II** | Connection pooling + 읽기 분산 | ❌ | Write 단일 |
| **Pgbouncer** | Connection pooling | ❌ | 분산 아님 |
| **TimescaleDB** | 시계열 데이터 최적화 | 제한적 | 시계열만 |

---

## 2️⃣ I/O 패턴의 근본적 차이

### 2.1 Random I/O vs Sequential I/O

```
┌─────────────────────────────────────────────────────────────────────┐
│                    I/O 패턴 비교                                     │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  Random I/O (PostgreSQL)                                            │
│  ┌─────────────────────────────────────────────────────┐            │
│  │ 블록1 │ 블록2 │ 블록3 │ 블록4 │ 블록5 │ ... │ 블록N │            │
│  └───┬───────────────┬─────────────┬───────────────────┘            │
│      ↑               ↑             ↑                                │
│    읽기1           읽기2         읽기3                               │
│                                                                      │
│  → 디스크 헤드가 여기저기 점프 (Seek Time 발생)                      │
│  → HDD: 매번 헤드 이동 필요 (느림)                                  │
│  → SSD: 전자적 접근 (빠름)                                          │
│                                                                      │
│  ─────────────────────────────────────────────────────────────────  │
│                                                                      │
│  Sequential I/O (HDFS/Spark)                                        │
│  ┌─────────────────────────────────────────────────────┐            │
│  │ 블록1 │ 블록2 │ 블록3 │ 블록4 │ 블록5 │ ... │ 블록N │            │
│  └───┬─────┬─────┬─────┬─────┬─────────────────────────┘            │
│      ↓     ↓     ↓     ↓     ↓                                      │
│    읽기1→읽기2→읽기3→읽기4→읽기5→ ...                                │
│                                                                      │
│  → 처음부터 끝까지 순차적으로 읽음                                   │
│  → HDD: 헤드 이동 없이 연속 읽기 (빠름)                             │
│  → SSD: 더 빠르지만, HDD도 충분                                     │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

---

### 2.2 HDD vs SSD 성능 차이

| 항목 | HDD | SSD | 차이 |
|------|-----|-----|------|
| **Random Read Latency** | 5~10ms | 0.05~0.1ms | **100배** |
| **Random Write Latency** | 5~10ms | 0.05~0.1ms | **100배** |
| **Random IOPS** | 100~200 | 10,000~100,000 | **100~500배** |
| **Sequential Read** | 100~200 MB/s | 500~5,000 MB/s | 5~25배 |
| **Sequential Write** | 100~200 MB/s | 500~3,000 MB/s | 5~15배 |

**핵심 포인트:**
- **Random I/O**: SSD가 100배 이상 빠름
- **Sequential I/O**: SSD가 빠르지만, HDD도 쓸만함

---

### 2.3 PostgreSQL의 I/O 패턴 (왜 SSD가 필요한가?)

```
┌─────────────────────────────────────────────────────────────────────┐
│                    PostgreSQL 데이터 조회 과정                       │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  쿼리: SELECT * FROM logs WHERE id = 12345                          │
│                                                                      │
│  Step 1: B-Tree 인덱스 탐색                                         │
│  ┌─────────────────────────────────────────────────────┐            │
│  │                    [Root Node]                      │            │
│  │                    id: 1~100000                     │            │
│  │                         │                           │            │
│  │            ┌────────────┼────────────┐              │            │
│  │            ▼            ▼            ▼              │  Random    │
│  │      [1~10000]    [10001~50000]  [50001~100000]    │  Read #1   │
│  │            │                                        │            │
│  │            ▼                                        │  Random    │
│  │      [10001~12000] → [12001~15000] → ...           │  Read #2   │
│  │            │                                        │            │
│  │            ▼                                        │  Random    │
│  │      [Leaf: id=12345 → Page 7823]                  │  Read #3   │
│  └─────────────────────────────────────────────────────┘            │
│                                                                      │
│  Step 2: 데이터 페이지 접근                                         │
│  ┌─────────────────────────────────────────────────────┐            │
│  │  Page 7823에서 실제 데이터 읽기                     │  Random    │
│  │  → 디스크의 완전히 다른 위치                        │  Read #4   │
│  └─────────────────────────────────────────────────────┘            │
│                                                                      │
│  총 Random Read: 4회                                                │
│                                                                      │
│  HDD: 4회 × 10ms = 40ms (1초에 25개 쿼리)                           │
│  SSD: 4회 × 0.1ms = 0.4ms (1초에 2,500개 쿼리)                      │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

**PostgreSQL이 SSD 필수인 이유:**

1. **B-Tree 인덱스 탐색**: 루트 → 브랜치 → 리프 (3~4번 Random Read)
2. **데이터 페이지 접근**: 인덱스가 가리키는 위치로 점프 (1번 Random Read)
3. **동시 접속**: 100명이 동시에 조회하면 100배 부하
4. **UPDATE/DELETE**: 인덱스 + 데이터 모두 Random Write

---

### 2.4 HDFS/Spark의 I/O 패턴 (왜 HDD도 괜찮은가?)

```
┌─────────────────────────────────────────────────────────────────────┐
│                    HDFS/Spark 데이터 처리 과정                       │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  쿼리: SELECT * FROM logs WHERE level = 'ERROR'                     │
│                                                                      │
│  Step 1: Parquet 파일 전체 스캔                                     │
│  ┌─────────────────────────────────────────────────────┐            │
│  │                                                     │            │
│  │  ┌────────────────────────────────────────────┐    │            │
│  │  │         Parquet File (128MB 블록)          │    │            │
│  │  │                                            │    │            │
│  │  │  [Row Group 1] → [Row Group 2] → [RG 3]   │    │ Sequential │
│  │  │       ↓              ↓            ↓        │    │ Read       │
│  │  │    처음부터 ───────────────────→ 끝까지    │    │            │
│  │  │                                            │    │            │
│  │  └────────────────────────────────────────────┘    │            │
│  │                                                     │            │
│  └─────────────────────────────────────────────────────┘            │
│                                                                      │
│  Step 2: 컬럼 기반 필터링 (Parquet 장점)                            │
│  ┌─────────────────────────────────────────────────────┐            │
│  │                                                     │            │
│  │  Parquet 구조:                                      │            │
│  │  ┌─────────┬─────────┬─────────┬─────────┐         │            │
│  │  │ level   │ message │ host    │ time    │ ← 컬럼별│            │
│  │  │ 컬럼    │ 컬럼    │ 컬럼    │ 컬럼    │   저장  │            │
│  │  └─────────┴─────────┴─────────┴─────────┘         │            │
│  │       ↓                                             │            │
│  │  level 컬럼만 읽어서 'ERROR' 필터링                 │ Sequential │
│  │  → 필요한 컬럼만 순차 읽기                          │ Read       │
│  │                                                     │            │
│  └─────────────────────────────────────────────────────┘            │
│                                                                      │
│  Step 3: 분산 병렬 처리                                             │
│  ┌─────────────────────────────────────────────────────┐            │
│  │                                                     │            │
│  │  Worker 1: 파일1, 파일4, 파일7 처리 (Sequential)   │            │
│  │  Worker 2: 파일2, 파일5, 파일8 처리 (Sequential)   │            │
│  │  Worker 3: 파일3, 파일6, 파일9 처리 (Sequential)   │            │
│  │                                                     │            │
│  │  → 각 Worker가 자기 파일을 처음부터 끝까지 읽음     │            │
│  │  → Random I/O 없음!                                 │            │
│  │                                                     │            │
│  └─────────────────────────────────────────────────────┘            │
│                                                                      │
│  HDD 성능 계산:                                                     │
│  - 1GB Parquet 파일                                                 │
│  - Sequential Read: 150MB/s                                         │
│  - 소요 시간: 1GB ÷ 150MB/s ≈ 7초                                  │
│  - 3대 분산: 7초 ÷ 3 ≈ 2.3초                                       │
│                                                                      │
│  → HDD로도 충분한 성능!                                             │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

**HDFS/Spark가 HDD도 괜찮은 이유:**

1. **큰 블록 크기**: 128MB 단위로 저장 (PostgreSQL은 8KB)
2. **순차 읽기**: 파일을 처음부터 끝까지 쭉 읽음
3. **컬럼 기반 저장**: 필요한 컬럼만 순차 읽기
4. **분산 처리**: 여러 노드가 각자 순차 읽기
5. **인덱스 없음**: B-Tree 탐색 같은 Random 접근 없음

---

### 2.5 구체적 성능 비교 예시

```
┌─────────────────────────────────────────────────────────────────────┐
│              100만 건 중 조건 검색 성능 비교                         │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  시나리오: level = 'ERROR' 인 로그 찾기                             │
│  데이터: 100만 건, 약 1GB                                           │
│                                                                      │
│  ─────────────────────────────────────────────────────────────────  │
│                                                                      │
│  PostgreSQL + HDD (인덱스 사용)                                     │
│  ├── 인덱스 탐색: 4번 Random Read × 10ms = 40ms                     │
│  ├── 결과 1000건 조회: 1000번 Random Read × 10ms = 10,000ms        │
│  └── 총: ~10초 (끔찍)                                               │
│                                                                      │
│  PostgreSQL + SSD (인덱스 사용)                                     │
│  ├── 인덱스 탐색: 4번 Random Read × 0.1ms = 0.4ms                   │
│  ├── 결과 1000건 조회: 1000번 Random Read × 0.1ms = 100ms          │
│  └── 총: ~100ms (양호)                                              │
│                                                                      │
│  ─────────────────────────────────────────────────────────────────  │
│                                                                      │
│  HDFS/Spark + HDD (풀 스캔)                                         │
│  ├── 1GB Sequential Read: 150MB/s → 7초                            │
│  ├── 3노드 분산: 7초 ÷ 3 ≈ 2.3초                                   │
│  ├── 컬럼 pruning (level만): 약 100MB → 0.7초                      │
│  └── 총: ~1~2초 (대용량에서 효율적)                                 │
│                                                                      │
│  HDFS/Spark + SSD (풀 스캔)                                         │
│  ├── 1GB Sequential Read: 500MB/s → 2초                            │
│  ├── 3노드 분산: 2초 ÷ 3 ≈ 0.7초                                   │
│  └── 총: ~0.5~1초 (더 빠르지만 HDD 대비 가성비 낮음)                │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

---

### 2.6 스토리지 선택 가이드

| 시스템 | I/O 패턴 | 권장 스토리지 | 이유 |
|--------|----------|--------------|------|
| **PostgreSQL** | Random | **SSD 필수** | 인덱스 탐색, 포인터 점프 |
| **HDFS/Spark** | Sequential | **HDD OK** | 큰 블록 순차 읽기 |
| **Kafka** | Sequential | HDD OK | 로그 형태 순차 쓰기/읽기 |
| **Redis** | Random (메모리) | RAM | 메모리 DB |

---

## 3️⃣ 데이터 저장 구조의 차이

### 3.1 PostgreSQL: Row 기반 + B-Tree 인덱스

```
┌─────────────────────────────────────────────────────────────────────┐
│                    PostgreSQL 저장 구조                              │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  Row 기반 저장 (Heap)                                               │
│  ┌─────────────────────────────────────────────────────┐            │
│  │ Page 1 (8KB)                                        │            │
│  │ ┌─────────────────────────────────────────────────┐ │            │
│  │ │ Row1: id=1, level=INFO, msg=..., host=..., ts   │ │            │
│  │ │ Row2: id=2, level=ERROR, msg=..., host=..., ts  │ │            │
│  │ │ Row3: id=3, level=DEBUG, msg=..., host=..., ts  │ │            │
│  │ └─────────────────────────────────────────────────┘ │            │
│  └─────────────────────────────────────────────────────┘            │
│                                                                      │
│  → 한 Row의 모든 컬럼이 연속 저장                                   │
│  → 특정 Row 접근: 해당 Page로 Random 점프                           │
│                                                                      │
│  B-Tree 인덱스                                                      │
│  ┌─────────────────────────────────────────────────────┐            │
│  │              [Root]                                 │            │
│  │            id: 1~1M                                 │            │
│  │          /    |    \                                │            │
│  │     [Branch] [Branch] [Branch]                      │            │
│  │        /        |        \                          │            │
│  │   [Leaf]    [Leaf]    [Leaf]                        │            │
│  │   id→PageID id→PageID id→PageID                     │            │
│  └─────────────────────────────────────────────────────┘            │
│                                                                      │
│  조회 과정:                                                         │
│  1. Root → Branch → Leaf (3번 Random Read)                         │
│  2. Leaf에서 PageID 확인                                            │
│  3. 해당 Page로 점프 (1번 Random Read)                              │
│  4. Page 내에서 Row 찾기                                            │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 3.2 HDFS/Parquet: 컬럼 기반 + 파티션

```
┌─────────────────────────────────────────────────────────────────────┐
│                    HDFS/Parquet 저장 구조                           │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  파티션 기반 디렉토리 구조                                          │
│  /data/logs/                                                        │
│  ├── year=2026/                                                     │
│  │   ├── month=01/                                                  │
│  │   │   ├── day=10/                                                │
│  │   │   │   ├── part-00000.parquet (128MB)                        │
│  │   │   │   ├── part-00001.parquet (128MB)                        │
│  │   │   │   └── part-00002.parquet (128MB)                        │
│  │   │   ├── day=11/                                                │
│  │   │   │   └── ...                                                │
│  │   │   └── day=12/                                                │
│  │   │       └── ...                                                │
│  │   └── month=02/                                                  │
│  │       └── ...                                                    │
│  └── year=2025/                                                     │
│      └── ...                                                        │
│                                                                      │
│  → 날짜 조건 쿼리: 해당 디렉토리만 스캔 (Partition Pruning)         │
│  → WHERE day = '12' → day=12 폴더만 읽음                           │
│                                                                      │
│  ─────────────────────────────────────────────────────────────────  │
│                                                                      │
│  Parquet 파일 내부 (컬럼 기반)                                      │
│  ┌─────────────────────────────────────────────────────┐            │
│  │ part-00000.parquet                                  │            │
│  │                                                     │            │
│  │ ┌─────────────────────────────────────────────────┐ │            │
│  │ │ Row Group 1 (약 50MB)                           │ │            │
│  │ │ ┌─────────┬─────────┬─────────┬─────────┐       │ │            │
│  │ │ │  level  │ message │  host   │  time   │       │ │            │
│  │ │ │  Column │ Column  │ Column  │ Column  │       │ │            │
│  │ │ │ (압축) │ (압축)  │ (압축)  │ (압축)  │       │ │            │
│  │ │ └─────────┴─────────┴─────────┴─────────┘       │ │            │
│  │ └─────────────────────────────────────────────────┘ │            │
│  │                                                     │            │
│  │ ┌─────────────────────────────────────────────────┐ │            │
│  │ │ Row Group 2 (약 50MB)                           │ │            │
│  │ │ ┌─────────┬─────────┬─────────┬─────────┐       │ │            │
│  │ │ │  level  │ message │  host   │  time   │       │ │            │
│  │ │ └─────────┴─────────┴─────────┴─────────┘       │ │            │
│  │ └─────────────────────────────────────────────────┘ │            │
│  │                                                     │            │
│  │ [Footer: 메타데이터, 통계 정보]                     │            │
│  └─────────────────────────────────────────────────────┘            │
│                                                                      │
│  → SELECT level, time: level, time 컬럼만 읽음 (Sequential)        │
│  → WHERE level = 'ERROR': Footer 통계로 Row Group Skip 가능        │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 3.3 저장 구조 비교 요약

| 항목 | PostgreSQL | HDFS/Parquet |
|------|------------|--------------|
| **저장 단위** | Row (행) | Column (열) |
| **블록 크기** | 8KB | 128MB |
| **인덱스** | B-Tree (포인터 기반) | 파티션 + 통계 기반 |
| **특정 Row 조회** | ✅ 빠름 (인덱스) | ❌ 느림 (풀 스캔) |
| **컬럼 선택 조회** | ❌ 전체 Row 읽음 | ✅ 해당 컬럼만 |
| **집계 (SUM, COUNT)** | ⚠️ 느림 | ✅ 컬럼 압축 + 병렬 |
| **압축률** | 낮음 | **높음** (같은 타입 연속) |

---

## 4️⃣ PostgreSQL (Citus) vs HDFS/Spark 상세 비교

### 4.1 기능 비교

| 항목 | PostgreSQL (Citus) | HDFS/Spark |
|------|-------------------|------------|
| **데이터 규모** | TB급 (최대 ~100TB) | **PB급** (무제한) |
| **Write 분산** | ✅ 샤딩 | ✅ 분산 저장 |
| **Read 분산** | ✅ 병렬 쿼리 | ✅ 병렬 처리 |
| **실시간 조회** | ✅ **ms 단위** | ❌ 초~분 단위 |
| **배치 처리** | ⚠️ 가능하나 느림 | ✅ **최적화됨** |
| **SQL 호환** | ✅ **완벽** | ⚠️ SparkSQL (제한적) |
| **ACID** | ✅ **완벽 지원** | ❌ 제한적 |
| **스키마** | 엄격 (변경 어려움) | **유연** |
| **데이터 형식** | 정형 데이터만 | **다양** (JSON, Parquet, 이미지) |
| **스트리밍** | ❌ 없음 | ✅ Spark Streaming |
| **ML/분석** | ❌ 제한적 | ✅ MLlib |

### 4.2 비용 비교

| 항목 | PostgreSQL (Citus) | HDFS/Spark |
|------|-------------------|------------|
| **스토리지 종류** | SSD 필수 | HDD 가능 |
| **1TB 저장 비용** | ~$100/월 | ~$20/월 |
| **100TB 저장 비용** | ~$10,000/월 | ~$2,000/월 |
| **1PB 저장 비용** | 비현실적 | ~$20,000/월 |
| **라이선스** | Citus Enterprise 유료 | 오픈소스 무료 |

### 4.3 운영 복잡도

| 항목 | PostgreSQL | HDFS/Spark |
|------|------------|------------|
| **초기 설정** | 쉬움 | 복잡 |
| **확장** | 수직 확장 쉬움 / 수평 어려움 | 수평 확장 쉬움 |
| **백업** | pg_dump | 복제 기반 |
| **모니터링** | 성숙한 도구 | 다양한 도구 필요 |
| **필요 인력** | DBA | 데이터 엔지니어 |

---

## 5️⃣ Citus가 따라갈 수 없는 영역

```
┌─────────────────────────────────────────────────────────────────────┐
│              HDFS/Spark가 필수인 상황                                │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  1. PB급 데이터                                                     │
│     └── Citus 한계: ~100TB (실용적)                                 │
│     └── HDFS: PB급 확장 가능 (Facebook 300PB+)                     │
│                                                                      │
│  2. 저장 비용                                                       │
│     └── PostgreSQL: SSD 필수 → 비쌈                                │
│     └── HDFS: HDD 가능 → 5배 저렴                                  │
│                                                                      │
│  3. 비정형 데이터                                                   │
│     └── PostgreSQL: 테이블 스키마 필수                              │
│     └── HDFS: 로그, JSON, 이미지, 동영상 그대로 저장               │
│                                                                      │
│  4. 배치 분석 최적화                                                │
│     └── PostgreSQL: OLTP 최적화 (작은 트랜잭션)                     │
│     └── Spark: OLAP 최적화 (대용량 스캔, 집계)                     │
│                                                                      │
│  5. 에코시스템                                                      │
│     └── PostgreSQL: SQL 중심                                        │
│     └── Hadoop: Hive, Presto, Flink, Kafka, ML 연동               │
│                                                                      │
│  6. 콜드 데이터 장기 보관                                           │
│     └── PostgreSQL: 비싼 SSD에 계속 보관                           │
│     └── HDFS: 저렴한 HDD로 아카이브                                │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

---

## 6️⃣ 실제 아키텍처 패턴

### 6.1 Lambda Architecture (권장)

```
┌─────────────────────────────────────────────────────────────────────┐
│                    Lambda Architecture                               │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│                         Data Source                                  │
│                             │                                        │
│                             ▼                                        │
│                     ┌──────────────┐                                │
│                     │    Kafka     │                                │
│                     └──────┬───────┘                                │
│                            │                                         │
│              ┌─────────────┴─────────────┐                          │
│              ▼                           ▼                          │
│     ┌─────────────────┐        ┌─────────────────┐                 │
│     │   Speed Layer   │        │   Batch Layer   │                 │
│     │   (실시간)       │        │   (배치 처리)    │                 │
│     ├─────────────────┤        ├─────────────────┤                 │
│     │  PostgreSQL     │        │  HDFS + Spark   │                 │
│     │  (SSD)          │        │  (HDD)          │                 │
│     │                 │        │                 │                 │
│     │  • 최근 7일     │        │  • 전체 히스토리 │                 │
│     │  • 실시간 조회  │        │  • 대용량 분석  │                 │
│     │  • ms 응답      │        │  • 일별 리포트  │                 │
│     │  • OLTP         │        │  • OLAP         │                 │
│     └────────┬────────┘        └────────┬────────┘                 │
│              │                          │                           │
│              └──────────┬───────────────┘                           │
│                         ▼                                           │
│                  ┌─────────────┐                                    │
│                  │ Query API   │                                    │
│                  │ (통합 조회)  │                                    │
│                  └─────────────┘                                    │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 6.2 데이터 생명주기

```
┌─────────────────────────────────────────────────────────────────────┐
│                    데이터 생명주기                                   │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  시간 ──────────────────────────────────────────────────────▶       │
│                                                                      │
│  [실시간]        [7일 후]         [30일 후]        [1년 후]          │
│     │               │                │                │             │
│     ▼               ▼                ▼                ▼             │
│  ┌──────┐      ┌──────┐        ┌──────────┐    ┌──────────┐        │
│  │ 생성 │ ───▶ │ Hot  │ ────▶  │   Warm   │───▶│   Cold   │        │
│  └──────┘      │ Data │        │   Data   │    │   Data   │        │
│                └──────┘        └──────────┘    └──────────┘        │
│                   │                 │               │               │
│                   ▼                 ▼               ▼               │
│              PostgreSQL          HDFS           HDFS              │
│              (SSD)             (HDD)          (Archive)           │
│                                                                      │
│  접근 빈도:   매우 높음          중간            낮음               │
│  응답 요구:   ms                초              분                 │
│  비용:       높음              중간            낮음               │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

---

## 7️⃣ 선택 가이드

### 7.1 데이터 규모별

| 데이터 규모 | 권장 시스템 | 이유 |
|------------|------------|------|
| < 100GB | PostgreSQL (단일) | 단순, 빠름, 관리 쉬움 |
| 100GB ~ 10TB | Citus 또는 HDFS | 상황에 따라 선택 |
| 10TB ~ 100TB | HDFS/Spark + PostgreSQL (Hot) | 비용, 확장성 |
| > 100TB | **HDFS/Spark 필수** | Citus 한계 초과 |

### 7.2 사용 패턴별

| 패턴 | 권장 시스템 | 이유 |
|------|------------|------|
| 실시간 조회 중심 | PostgreSQL | ms 응답 |
| 배치 분석 중심 | HDFS/Spark | 대용량 최적화 |
| 혼합 워크로드 | 둘 다 (Lambda) | 각각의 장점 활용 |
| 로그/이벤트 저장 | HDFS | 비정형, 대용량, 저렴 |
| 트랜잭션 처리 | PostgreSQL | ACID 필수 |

### 7.3 비용 최적화

```
목표: 100TB 데이터, 월 비용 최소화

Option A: PostgreSQL만
- 스토리지: 100TB SSD = ~$10,000/월
- 서버: 고사양 = ~$5,000/월
- 총: ~$15,000/월

Option B: HDFS/Spark만
- 스토리지: 100TB HDD = ~$2,000/월
- 서버: commodity = ~$3,000/월
- 총: ~$5,000/월

Option C: 하이브리드 (권장)
- PostgreSQL (10TB Hot): ~$1,500/월
- HDFS (100TB Cold): ~$2,000/월
- 총: ~$3,500/월 ✅ 최적
```

---

## 8️⃣ 실제 사례

### PostgreSQL 사용 사례

| 회사 | 용도 | 규모 | 선택 이유 |
|------|------|------|----------|
| Instagram | 메인 DB | ~TB급 | 실시간 조회, ACID |
| Notion | 문서 저장 | ~TB급 | 트랜잭션, SQL |
| GitLab | 메타데이터 | ~TB급 | 관계형 데이터 |

### HDFS/Spark 사용 사례

| 회사 | 용도 | 규모 | 선택 이유 |
|------|------|------|----------|
| Facebook | 로그/분석 | 300+ PB | 대용량, 비용 |
| Netflix | 추천/분석 | 100+ PB | ML, 배치 처리 |
| Uber | 실시간 분석 | 100+ PB | 스트리밍, 확장성 |
| LinkedIn | 데이터 레이크 | 수십 PB | 다양한 데이터 |

---

## 9️⃣ 이 프로젝트의 의미

### 벤치마크 목표

```
┌─────────────────────────────────────────────────────────────────────┐
│                    우리가 증명하려는 것                              │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  1. 소량 데이터 (Read Performance)                                  │
│     → PostgreSQL이 빠르고 간단                                      │
│     → 이미 검증 완료 (350배 빠름)                                   │
│                                                                      │
│  2. 대용량 데이터 (Write Performance)                               │
│     → 자원 한정 시 PostgreSQL 병목                                  │
│     → 분산 시스템(HDFS/Spark)이 안정적                             │
│     → 벤치마크 진행 예정                                            │
│                                                                      │
│  3. 확장성                                                          │
│     → PostgreSQL: 단일 노드 한계                                    │
│     → HDFS/Spark: 노드 추가로 선형 확장                            │
│                                                                      │
│  결론:                                                               │
│  "작게 시작해서 PostgreSQL, 커지면 HDFS/Spark로 확장"               │
│  이것이 실제 스타트업 → 대기업 성장 패턴                            │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

---

## 🔗 관련 문서

- [BENCHMARK.md](BENCHMARK.md) - 조회 성능 비교 (1차 결과)
- [BENCHMARK_WRITE_PERFORMANCE.md](BENCHMARK_WRITE_PERFORMANCE.md) - 쓰기 성능 비교 설계
- [ARCHITECTURE.md](ARCHITECTURE.md) - 시스템 아키텍처
- [TROUBLESHOOTING.md](TROUBLESHOOTING.md) - 트러블슈팅 가이드