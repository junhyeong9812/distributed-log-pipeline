# íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ

> Distributed Log Pipeline êµ¬ì¶• ì¤‘ ë°œìƒí•œ ë¬¸ì œì™€ í•´ê²° ë°©ë²•ì„ ì •ë¦¬í•œ ë¬¸ì„œì…ë‹ˆë‹¤.

---

## ğŸ“‹ ëª©ì°¨

### 1ì°¨ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… (Docker Compose + Kubernetes í™˜ê²½ êµ¬ì¶•)

| ë²ˆí˜¸ | ë¬¸ì œ | í™˜ê²½ |
|------|------|------|
| 1 | Kafka í† í”½ íŒŒí‹°ì…˜ ì„¤ì • ë¬¸ì œ | Docker Compose |
| 2 | Spark Streaming íŒŒí‹°ì…˜ ì €ì¥ ë¬¸ì œ | Docker Compose |
| 3 | Spark Job ë¦¬ì†ŒìŠ¤ ì ìœ  ë¬¸ì œ | Docker Compose |
| 4 | Airflow DB ì´ˆê¸°í™” ë° Executor ì„¤ì • | Docker Compose |
| 5 | DataNodeê°€ NameNodeì— ë“±ë¡ ì‹¤íŒ¨ | ë¶„ì‚° í™˜ê²½ |
| 6 | Spark Worker í¬íŠ¸ ì¶©ëŒ | ë¶„ì‚° í™˜ê²½ |
| 7 | Sparkì—ì„œ HDFS DataNode ì—°ê²° ì‹¤íŒ¨ | ë¶„ì‚° í™˜ê²½ |
| 8 | Docker ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬ ë¬¸ì œ | ë¶„ì‚° í™˜ê²½ |
| 9 | network_mode: hostì—ì„œ extra_hosts ë¬´ì‹œ | ë¶„ì‚° í™˜ê²½ |
| 10 | Spark Worker ìê¸° í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨ | ë¶„ì‚° í™˜ê²½ |
| 11 | Spark Worker IPv6 í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨ | ë¶„ì‚° í™˜ê²½ |
| 12 | DataNode ê°„ ë¸”ë¡ ë³µì œ ì‹¤íŒ¨ | ë¶„ì‚° í™˜ê²½ |
| 13 | Airflowì—ì„œ Spark Master ì—°ê²° ì‹¤íŒ¨ | ë¶„ì‚° í™˜ê²½ |
| 14 | Spark Master Service ì´ë¦„ ì¶©ëŒ | Kubernetes |
| 15 | Airflow DAG ë””ë ‰í† ë¦¬ ì¬ê·€ ë£¨í”„ | Kubernetes |
| 16 | Airflow initContainer ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨ | Kubernetes |
| 17 | Kafka Replication Factor ì˜¤ë¥˜ | Kubernetes |
| 18 | Generatorì—ì„œ Backend ì—°ê²° ì‹¤íŒ¨ | Kubernetes |
| 19 | Generator Settings ì†ì„± ëˆ„ë½ | Kubernetes |
| 20 | HDFS Cluster ID ë¶ˆì¼ì¹˜ | Kubernetes |
| 21 | Spark Executor í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨ | Kubernetes |

---

## Docker Compose í™˜ê²½ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: Kafka í† í”½ íŒŒí‹°ì…˜ ì„¤ì • ë¬¸ì œ

**ì¦ìƒ:**
```
ì˜ˆìƒ: íŒŒí‹°ì…˜ 3ê°œë¡œ ë¶„ì‚° ì €ì¥
ì‹¤ì œ: íŒŒí‹°ì…˜ 1ê°œì— ëª¨ë“  ë°ì´í„° ì €ì¥
```

**ì›ì¸:**
- Generatorê°€ Backendë³´ë‹¤ ë¨¼ì € ë°ì´í„° ì „ì†¡
- Kafkaê°€ í† í”½ ìë™ ìƒì„± (AUTO_CREATE_TOPICS=true)
- ê¸°ë³¸ê°’ íŒŒí‹°ì…˜ 1ê°œë¡œ ìƒì„±ë˜ì–´ KafkaConfig ì„¤ì • ë¬´ì‹œë¨

**í•´ê²°:**

ë°©ë²• 1: ì„œë¹„ìŠ¤ ì‹œì‘ ìˆœì„œ ë³´ì¥
```yaml
generator:
  depends_on:
    backend:
      condition: service_healthy
```

ë°©ë²• 2: í† í”½ ì‚¬ì „ ìƒì„±
```bash
docker exec kafka /opt/kafka/bin/kafka-topics.sh \
  --create \
  --topic logs.raw \
  --partitions 3 \
  --replication-factor 1 \
  --bootstrap-server localhost:9092
```

ë°©ë²• 3: ë³¼ë¥¨ ì´ˆê¸°í™” í›„ ì¬ì‹œì‘
```bash
docker compose down -v
docker compose up -d
```

---

### ë¬¸ì œ 2: Spark Streaming íŒŒí‹°ì…˜ ì €ì¥ ë¬¸ì œ

**ì¦ìƒ:**
```
ì˜ˆìƒ: /data/logs/raw/year=2026/month=1/day=11/hour=19/
ì‹¤ì œ: /data/logs/raw/year=__HIVE_DEFAULT_PARTITION__/...
```

**ì›ì¸:**
- Generatorê°€ ë³´ë‚´ëŠ” timestamp í˜•ì‹: Unix timestamp (1768123166.291045)
- Sparkì—ì„œ ISO í˜•ì‹ìœ¼ë¡œ íŒŒì‹± ì‹œë„ â†’ ì‹¤íŒ¨ â†’ null â†’ __HIVE_DEFAULT_PARTITION__

**í•´ê²°:**

```python
# ìˆ˜ì • ì „ (ISO í˜•ì‹)
.withColumn("event_time", to_timestamp(col("timestamp"), "yyyy-MM-dd'T'HH:mm:ss.SSSXXX"))

# ìˆ˜ì • í›„ (Unix timestamp)
.withColumn("event_time", from_unixtime(col("timestamp")).cast("timestamp"))
```

ìŠ¤í‚¤ë§ˆë„ ë³€ê²½:
```python
# ìˆ˜ì • ì „
StructField("timestamp", StringType(), True)

# ìˆ˜ì • í›„
StructField("timestamp", DoubleType(), True)
```

---

### ë¬¸ì œ 3: Spark Job ë¦¬ì†ŒìŠ¤ ì ìœ  ë¬¸ì œ

**ì¦ìƒ:**
```
WARN TaskSchedulerImpl: Initial job has not accepted any resources; 
check your cluster UI to ensure that workers are registered and have sufficient resources
```

**ì›ì¸:**
- ì´ì „ Spark Jobì´ ë¹„ì •ìƒ ì¢…ë£Œë˜ë©´ì„œ Worker ë¦¬ì†ŒìŠ¤ ê³„ì† ì ìœ 

**í•´ê²°:**

ë°©ë²• 1: Spark ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘
```bash
docker compose restart spark-master spark-worker
```

ë°©ë²• 2: Spark Master UIì—ì„œ ì§ì ‘ ì¢…ë£Œ
```
http://<MASTER_IP>:8082 â†’ Running Applications â†’ (kill) í´ë¦­
```

ë°©ë²• 3: ì „ì²´ í´ëŸ¬ìŠ¤í„° ì¬ì‹œì‘
```bash
docker compose down
docker compose up -d
```

---

### ë¬¸ì œ 4: Airflow DB ì´ˆê¸°í™” ë° Executor ì„¤ì •

**ì¦ìƒ 1:**
```
ERROR: You need to initialize the database. Please run `airflow db init`.
```

**ì¦ìƒ 2:**
```
airflow.exceptions.AirflowConfigException: error: cannot use SQLite with the LocalExecutor
```

**ì›ì¸:**
- DBê°€ ë¶ˆì™„ì „í•˜ê²Œ ìƒì„±ë¨
- SQLiteëŠ” ë™ì‹œ ì“°ê¸° ë¯¸ì§€ì›, LocalExecutorì™€ í˜¸í™˜ ë¶ˆê°€

**í•´ê²°:**

```yaml
environment:
  - AIRFLOW__CORE__EXECUTOR=SequentialExecutor  # LocalExecutor â†’ SequentialExecutor
  - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
```

ë³¼ë¥¨ ì‚­ì œ í›„ ì¬ì‹œì‘:
```bash
docker compose down -v
docker compose up -d
```

---

## ë¶„ì‚° í™˜ê²½ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 5: DataNodeê°€ NameNodeì— ë“±ë¡ ì‹¤íŒ¨

**ì¦ìƒ:**
```bash
docker exec namenode hdfs dfsadmin -report
# Live datanodes (0)
```

**ì—ëŸ¬ ë¡œê·¸:**
```
ERROR datanode.DataNode: Initialization failed for Block pool...
Datanode denied communication with namenode because hostname cannot be resolved
```

**ì›ì¸:**
- NameNodeê°€ DataNode IPì— ëŒ€í•´ ì—­ë°©í–¥ DNS ì¡°íšŒ ì‹œë„
- í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨ë¡œ ì—°ê²° ê±°ë¶€

**í•´ê²°:**

docker-compose.master.yml namenodeì— ì¶”ê°€:
```yaml
environment:
  - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
```

> í™˜ê²½ë³€ìˆ˜ ë³€í™˜ ê·œì¹™: `.` â†’ `_`, `-` â†’ `___`

---

### ë¬¸ì œ 6: Spark Worker í¬íŠ¸ ì¶©ëŒ

**ì¦ìƒ:**
```
Error response from daemon: Bind for 0.0.0.0:8081 failed: port is already allocated
```

**í•´ê²°:**

docker-compose.worker.ymlì—ì„œ í¬íŠ¸ ë³€ê²½:
```yaml
spark-worker:
  ports:
    - "10000:8081"
```

---

### ë¬¸ì œ 7: Sparkì—ì„œ HDFS DataNode ì—°ê²° ì‹¤íŒ¨

**ì¦ìƒ:**
```
java.net.ConnectException: Connection refused
ERROR: File could only be written to 0 of the 1 minReplication nodes.
```

**ì›ì¸:**
- DataNode 9866 í¬íŠ¸ ë¯¸ë…¸ì¶œ
- HDFS ë°ì´í„° íë¦„: Client â†’ NameNode â†’ Client â†’ DataNode ì§ì ‘ ì—°ê²°

**í•´ê²°:**

docker-compose.worker.ymlì— í¬íŠ¸ ì¶”ê°€:
```yaml
datanode:
  ports:
    - "9864:9864"  # HTTP
    - "9866:9866"  # ë°ì´í„° ì „ì†¡ (í•„ìˆ˜!)
    - "9867:9867"  # IPC
```

---

### ë¬¸ì œ 8: Docker ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬ ë¬¸ì œ

**ì¦ìƒ:**
```
java.io.EOFException: Unexpected EOF while trying to read response from server
WARN TaskSchedulerImpl: Initial job has not accepted any resources
```

**ì›ì¸:**
- Docker bridge ë„¤íŠ¸ì›Œí¬ê°€ ì»¨í…Œì´ë„ˆ ê²©ë¦¬
- DataNode/Spark Workerê°€ ë‚´ë¶€ IP (172.x.x.x) ë³´ê³ 
- Masterê°€ í•´ë‹¹ ë‚´ë¶€ IPë¡œ ì ‘ê·¼ ì‹œë„ â†’ ì‹¤íŒ¨

**í•´ê²°:**

Workerì—ì„œ `network_mode: host` ì‚¬ìš©:
```yaml
datanode:
  network_mode: host

spark-worker:
  network_mode: host
```

---

### ë¬¸ì œ 9: network_mode: hostì—ì„œ extra_hosts ë¬´ì‹œ

**ì¦ìƒ:**
```
java.nio.channels.UnresolvedAddressException
```

docker-compose.ymlì— extra_hosts ì„¤ì •í–ˆì§€ë§Œ ì—¬ì „íˆ í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨

**ì›ì¸:**
- `network_mode: host` ì‚¬ìš© ì‹œ ì»¨í…Œì´ë„ˆê°€ í˜¸ìŠ¤íŠ¸ ë„¤íŠ¸ì›Œí¬ ì§ì ‘ ì‚¬ìš©
- Dockerì˜ extra_hosts ì„¤ì • ë¬´ì‹œë¨
- ì»¨í…Œì´ë„ˆê°€ í˜¸ìŠ¤íŠ¸ PCì˜ /etc/hosts ì§ì ‘ ì°¸ì¡°

**í•´ê²°:**

Worker PCì˜ /etc/hostsì— ì§ì ‘ ì¶”ê°€:
```bash
# Worker 1 (192.168.55.158)
echo "192.168.55.9 worker2" | sudo tee -a /etc/hosts
echo "192.168.55.114 jun-Victus-by-HP-Gaming-Laptop-16-r0xxx.local" | sudo tee -a /etc/hosts

# Worker 2 (192.168.55.9)
echo "192.168.55.158 worker1" | sudo tee -a /etc/hosts
echo "192.168.55.114 jun-Victus-by-HP-Gaming-Laptop-16-r0xxx.local" | sudo tee -a /etc/hosts
```

---

### ë¬¸ì œ 10: Spark Worker ìê¸° í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨

**ì¦ìƒ:**
```
java.net.UnknownHostException: jun: jun: Try again
    at java.net.InetAddress.getLocalHost(InetAddress.java:1507)
```

**ì›ì¸:**
- Spark Worker ì‹œì‘ ì‹œ ìì‹ ì˜ í˜¸ìŠ¤íŠ¸ëª…ì„ IPë¡œ í•´ì„ ì‹œë„
- /etc/hostsì— ìê¸° ìì‹ ì˜ í˜¸ìŠ¤íŠ¸ëª… ì—†ìœ¼ë©´ ì‹¤íŒ¨

**í•´ê²°:**

ê° Worker PCì˜ /etc/hostsì— ìê¸° ìì‹  ì¶”ê°€:
```bash
# í˜¸ìŠ¤íŠ¸ëª… í™•ì¸
hostname

# /etc/hostsì— ì¶”ê°€
echo "127.0.0.1 $(hostname)" | sudo tee -a /etc/hosts
```

---

### ë¬¸ì œ 11: Spark Worker IPv6 í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨

**ì¦ìƒ:**
```
java.net.UnknownHostException: jun: Try again
    at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
```

**ì›ì¸:**
- Javaê°€ ê¸°ë³¸ì ìœ¼ë¡œ IPv6ë¡œ ë¨¼ì € ì¡°íšŒ
- /etc/hostsì— í˜¸ìŠ¤íŠ¸ëª… ìˆì–´ë„ IPv6 ì¡°íšŒ ì‹¤íŒ¨

**í•´ê²°:**

docker-composeì—ì„œ IPv4 ê°•ì œ ì‚¬ìš©:
```yaml
spark-worker:
  environment:
    - SPARK_LOCAL_IP=192.168.55.158
    - SPARK_WORKER_OPTS=-Djava.net.preferIPv4Stack=true
  network_mode: host
```

---

### ë¬¸ì œ 12: DataNode ê°„ ë¸”ë¡ ë³µì œ ì‹¤íŒ¨

**ì¦ìƒ:**
```
java.io.IOException: Got error, status=ERROR, ack with firstBadLink as 192.168.55.9:9866
WARN DataStreamer: Excluding datanode DatanodeInfoWithStorage[192.168.55.9:9866...]
```

**ì›ì¸:**
- HDFS ë³µì œ íŒ©í„° 2 ì„¤ì •
- DataNode ê°„ ë„¤íŠ¸ì›Œí¬ í†µì‹  ë¬¸ì œë¡œ ë³µì œ ì‹¤íŒ¨

**í•´ê²°:**

ë³µì œ íŒ©í„°ë¥¼ 1ë¡œ ë³€ê²½:
```yaml
# Master
namenode:
  environment:
    - HDFS_CONF_dfs_replication=1

# Worker
datanode:
  environment:
    - HDFS_CONF_dfs_replication=1
```

| ë³µì œ íŒ©í„° | ì¥ì  | ë‹¨ì  |
|-----------|------|------|
| 1 | ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ì—†ìŒ | ì¥ì•  ì‹œ ë°ì´í„° ì†ì‹¤ |
| 2 | 1ëŒ€ ì¥ì•  í—ˆìš© | DataNode ê°„ í†µì‹  í•„ìš” |
| 3 | 2ëŒ€ ì¥ì•  í—ˆìš© | ë” ë§ì€ í†µì‹  í•„ìš” |

---

### ë¬¸ì œ 13: Airflowì—ì„œ Spark Master ì—°ê²° ì‹¤íŒ¨

**ì¦ìƒ:**
```
java.net.UnknownHostException: spark-master
ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive!
```

**ì›ì¸:**
- Spark Masterê°€ `network_mode: host`ë¡œ ë³€ê²½ë¨
- Airflow DAGì—ì„œ `spark://spark-master:7077`ë¡œ ì—°ê²° ì‹œë„
- Docker ì„œë¹„ìŠ¤ëª… í•´ì„ ë¶ˆê°€

**í•´ê²°:**

Airflow DAG íŒŒì¼ì—ì„œ ì‹¤ì œ IP ì‚¬ìš©:
```python
# ìˆ˜ì • ì „
--master spark://spark-master:7077

# ìˆ˜ì • í›„
--master spark://192.168.55.114:7077
```

---

## Kubernetes í™˜ê²½ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 14: Spark Master Service ì´ë¦„ ì¶©ëŒ

**ì¦ìƒ:**
```
java.lang.NumberFormatException: For input string: "tcp://10.43.220.131:8080"
```

**ì›ì¸:**
- K8sê°€ Service ì´ë¦„ìœ¼ë¡œ í™˜ê²½ë³€ìˆ˜ ìë™ ìƒì„±
- `spark-master` Service â†’ `SPARK_MASTER_PORT=tcp://10.43.220.131:8080`
- Sparkê°€ ì´ ê°’ì„ ìˆ«ìë¡œ íŒŒì‹± ì‹œë„ â†’ ì‹¤íŒ¨

**í•´ê²°:**

Service ì´ë¦„ì„ `spark-master-svc`ë¡œ ë³€ê²½:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: spark-master-svc  # spark-master â†’ spark-master-svc
```

í™˜ê²½ë³€ìˆ˜ ëª…ì‹œì  ì„¤ì •:
```yaml
env:
  - name: SPARK_MASTER_PORT
    value: "7077"
  - name: SPARK_MASTER_WEBUI_PORT
    value: "8080"
```

---

### ë¬¸ì œ 15: Airflow DAG ë””ë ‰í† ë¦¬ ì¬ê·€ ë£¨í”„

**ì¦ìƒ:**
```
RuntimeError: Detected recursive loop when walking DAG directory /opt/airflow/dags:
/opt/airflow/dags/..2026_01_12_01_22_40.195680184 has appeared more than once.
```

**ì›ì¸:**
- ConfigMapì„ DAG ë””ë ‰í† ë¦¬ì— ì§ì ‘ ë§ˆìš´íŠ¸
- ConfigMapì˜ ì‹¬ë³¼ë¦­ ë§í¬ êµ¬ì¡°ê°€ ë¬´í•œ ë£¨í”„ ìœ ë°œ

**í•´ê²°:**

initContainerì—ì„œ DAG íŒŒì¼ì„ PVCë¡œ ë³µì‚¬:
```yaml
initContainers:
  - name: init-dags
    image: busybox
    command:
      - sh
      - -c
      - |
        mkdir -p /opt/airflow/dags
        cat > /opt/airflow/dags/manual_pipeline.py << 'PYEND'
        from airflow import DAG
        ...
        PYEND
    volumeMounts:
      - name: airflow-data
        mountPath: /opt/airflow
```

---

### ë¬¸ì œ 16: Airflow initContainer ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨

**ì¦ìƒ:**
```
sh: can't create /opt/airflow/dags/manual_pipeline.py: nonexistent directory
```

**ì›ì¸:**
- PVC ë§ˆìš´íŠ¸ ì‹œ ë¹ˆ ë””ë ‰í† ë¦¬ë¡œ ì‹œì‘
- /opt/airflow/dags ë””ë ‰í† ë¦¬ ë¯¸ì¡´ì¬

**í•´ê²°:**

initContainerì—ì„œ mkdir ë¨¼ì € ì‹¤í–‰:
```yaml
command:
  - sh
  - -c
  - |
    mkdir -p /opt/airflow/dags
    cat > /opt/airflow/dags/manual_pipeline.py << 'PYEND'
    ...
    PYEND
```

---

### ë¬¸ì œ 17: Kafka Replication Factor ì˜¤ë¥˜

**ì¦ìƒ:**
```
InvalidReplicationFactorException: Unable to replicate the partition 2 time(s): 
only 1 broker(s) are registered.
```

**ì›ì¸:**
- KafkaConfig.javaì—ì„œ replicas(2) ì„¤ì •
- K8sì—ì„œ Kafka broker 1ê°œë§Œ ì‹¤í–‰

**í•´ê²°:**

KafkaConfig.java ìˆ˜ì •:
```java
return TopicBuilder.name("logs.raw")
        .partitions(3)
        .replicas(1)  // 2 â†’ 1
        .build();
```

ì´ë¯¸ì§€ ì¬ë¹Œë“œ ë° ë°°í¬:
```bash
./gradlew build -x test
docker build -t log-pipeline-backend:latest .
docker save log-pipeline-backend:latest | sudo k3s ctr images import -
kubectl rollout restart deployment/backend -n log-pipeline
```

---

### ë¬¸ì œ 18: Generatorì—ì„œ Backend ì—°ê²° ì‹¤íŒ¨

**ì¦ìƒ:**
```
ERROR:app.scheduler:Failed to send logs: All connection attempts failed
```

**ì›ì¸:**
- config.pyì—ì„œ backend_urlì´ localhostë¡œ ì„¤ì •
- K8sì—ì„œëŠ” Service DNS ì´ë¦„ ì‚¬ìš© í•„ìš”

**í•´ê²°:**

config.py ìˆ˜ì •:
```python
backend_url: str = os.getenv("BACKEND_URL", "http://localhost:8081")
```

K8s Deployment í™˜ê²½ë³€ìˆ˜:
```yaml
env:
  - name: BACKEND_URL
    value: "http://backend-svc.log-pipeline.svc.cluster.local:8081"
```

---

### ë¬¸ì œ 19: Generator Settings ì†ì„± ëˆ„ë½

**ì¦ìƒ:**
```
AttributeError: 'Settings' object has no attribute 'services'
```

**ì›ì¸:**
- config.py ìˆ˜ì • ì‹œ ê¸°ì¡´ ì†ì„±ë“¤ ëˆ„ë½

**í•´ê²°:**

ê¸°ì¡´ ì„¤ì • ìœ ì§€í•˜ë©´ì„œ K8s í™˜ê²½ë³€ìˆ˜ë§Œ ì¶”ê°€:
```python
class Settings(BaseSettings):
    backend_url: str = os.getenv("BACKEND_URL", "http://localhost:8081")
    backend_timeout: int = 30
    log_interval_seconds: int = 5
    event_interval_seconds: int = 10
    batch_size: int = 100
    
    # ì´ ì†ì„±ë“¤ ìœ ì§€ í•„ìˆ˜!
    services: list = ["api-gateway", "user-service", "order-service", "payment-service"]
    log_levels: list = ["INFO", "DEBUG", "WARN", "ERROR"]
    event_types: list = ["CLICK", "VIEW", "PURCHASE", "LOGIN", "LOGOUT", "SEARCH"]
    error_rate: float = 0.05
```

---

### ë¬¸ì œ 20: HDFS Cluster ID ë¶ˆì¼ì¹˜

**ì¦ìƒ:**
```
java.io.IOException: Incompatible clusterIDs in /hadoop/dfs/data: 
namenode clusterID = CID-35773007-...; datanode clusterID = CID-8054f9f7-...
```

**ì›ì¸:**
- NameNode ì¬ìƒì„± ì‹œ ìƒˆ Cluster ID ë°œê¸‰
- DataNodeëŠ” ê¸°ì¡´ Cluster ID ë³´ìœ 

**í•´ê²°:**

Worker ë…¸ë“œì—ì„œ DataNode ë°ì´í„° ì‚­ì œ:
```bash
sudo rm -rf /data/hdfs/datanode/*
```

DataNode ì¬ì‹œì‘:
```bash
kubectl rollout restart daemonset/datanode -n log-pipeline
```

---

### ë¬¸ì œ 21: Spark Executor í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨

**ì¦ìƒ:**
```
java.net.UnknownHostException: spark-master-5885b7bc-6lck4
Failed to connect to spark-master-5885b7bc-6lck4:33405
```

**ì›ì¸:**
- Spark Masterê°€ Pod ì´ë¦„ì„ Driver URLë¡œ ì‚¬ìš©
- Workerì—ì„œ Pod ì´ë¦„ DNS í•´ì„ ë¶ˆê°€

**í•´ê²°:**

Headless Service + hostname/subdomainìœ¼ë¡œ DNS ì´ë¦„ ë¶€ì—¬:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-master
spec:
  template:
    spec:
      hostname: spark-master
      subdomain: spark-headless
      containers:
        - env:
            - name: SPARK_PUBLIC_DNS
              value: "spark-master.spark-headless.log-pipeline.svc.cluster.local"
---
apiVersion: v1
kind: Service
metadata:
  name: spark-headless
spec:
  clusterIP: None  # Headless Service
  selector:
    app: spark-master
```

ê²°ê³¼:
- Pod DNS: `spark-master.spark-headless.log-pipeline.svc.cluster.local`
- Workerì—ì„œ í•´ì„ ê°€ëŠ¥

---

## ğŸ” ì¼ë°˜ì ì¸ ë””ë²„ê¹… ëª…ë ¹ì–´

### Docker Compose

```bash
# ì „ì²´ ë¡œê·¸
docker compose logs -f

# íŠ¹ì • ì„œë¹„ìŠ¤ ë¡œê·¸
docker compose logs -f <service>

# ì»¨í…Œì´ë„ˆ ì ‘ì†
docker exec -it <container> bash

# ë¦¬ì†ŒìŠ¤ í™•ì¸
docker stats
```

### Kubernetes

```bash
# Pod ìƒíƒœ
kubectl get pods -n log-pipeline

# Pod ìƒì„¸ ì •ë³´
kubectl describe pod -n log-pipeline <pod>

# Pod ë¡œê·¸
kubectl logs -n log-pipeline <pod> --tail=50

# Pod ì ‘ì†
kubectl exec -it -n log-pipeline <pod> -- bash

# ì´ë²¤íŠ¸ í™•ì¸
kubectl get events -n log-pipeline --sort-by='.lastTimestamp'
```

### HDFS

```bash
# í´ëŸ¬ìŠ¤í„° ìƒíƒœ
hdfs dfsadmin -report

# íŒŒì¼ ëª©ë¡
hdfs dfs -ls -R /

# íŒŒì¼ ë‚´ìš© í™•ì¸
hdfs dfs -cat /path/to/file
```

### Kafka

```bash
# í† í”½ ëª©ë¡
kafka-topics.sh --list --bootstrap-server localhost:9092

# í† í”½ ìƒì„¸
kafka-topics.sh --describe --topic <topic> --bootstrap-server localhost:9092

# ë©”ì‹œì§€ í™•ì¸
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <topic> --from-beginning --max-messages 5
```

### Spark

```bash
# ì• í”Œë¦¬ì¼€ì´ì…˜ ëª©ë¡
# Spark UI: http://<master>:8080

# ë¡œê·¸ í™•ì¸
# Workerì˜ /spark/work/<app-id>/<executor-id>/stderr
```

---

### ë¬¸ì œ 22: Docker ë©€í‹°ìŠ¤í…Œì´ì§€ ë¹Œë“œ COPY ì‹¤íŒ¨

**ì¦ìƒ:**
```
COPY --from=build /app/build/libs/*.jar app.jar
When using COPY with more than one source file, the destination must be a directory and end with a /
```

**ì›ì¸:**
- Docker COPY ëª…ë ¹ì–´ì—ì„œ ì™€ì¼ë“œì¹´ë“œ(`*.jar`) ì‚¬ìš© ì‹œ ì—¬ëŸ¬ íŒŒì¼ ë§¤ì¹­ ê°€ëŠ¥
- ì—¬ëŸ¬ íŒŒì¼ì„ ë‹¨ì¼ íŒŒì¼ëª…(`app.jar`)ìœ¼ë¡œ ë³µì‚¬ ë¶ˆê°€
- ë¹Œë“œ ê²°ê³¼ë¬¼ì´ ì—¬ëŸ¬ jar íŒŒì¼ì¼ ìˆ˜ ìˆìŒ

**í•´ê²°:**
Dockerfileì—ì„œ ì •í™•í•œ jar íŒŒì¼ëª… ì§€ì •:
```dockerfile
# ìˆ˜ì • ì „
COPY --from=build /app/build/libs/*.jar app.jar

# ìˆ˜ì • í›„
COPY --from=build /app/build/libs/pipeline-0.0.1-SNAPSHOT.jar app.jar
```

**ì „ì²´ Dockerfile:**
```dockerfile
FROM eclipse-temurin:17-jdk-alpine AS build

WORKDIR /app

COPY gradle/ gradle/
COPY gradlew .
COPY build.gradle .
COPY settings.gradle .
COPY src/ src/

RUN chmod +x gradlew
RUN ./gradlew build -x test

FROM eclipse-temurin:17-jre-alpine

WORKDIR /app

COPY --from=build /app/build/libs/pipeline-0.0.1-SNAPSHOT.jar app.jar

EXPOSE 8081

ENTRYPOINT ["java", "-jar", "app.jar"]
```

**ì¬ë¹Œë“œ:**
```bash
cd ~/project/distributed-log-pipeline/backend
docker build --no-cache -t log-pipeline-backend:latest .
docker save log-pipeline-backend:latest | sudo k3s ctr images import -
kubectl rollout restart deployment/backend -n log-pipeline
```

---

### ë¬¸ì œ 23: Backend PostgreSQL í™˜ê²½ë³€ìˆ˜ ëˆ„ë½ (K8s)

**ì¦ìƒ:**
- Backend ë¡œê·¸ì— JPA/Hibernate ì´ˆê¸°í™” ë¡œê·¸ ì—†ìŒ
- PostgreSQLì— ë°ì´í„° ì €ì¥ ì•ˆ ë¨
- Kafkaë§Œ ë™ì‘

**ì›ì¸:**
- backend.yamlì— PostgreSQL ê´€ë ¨ í™˜ê²½ë³€ìˆ˜ ë¯¸ì„¤ì •
- Spring Bootê°€ datasource ì„¤ì • ì—†ì´ ì‹œì‘

**í™•ì¸:**
```bash
kubectl describe pod -n log-pipeline -l app=backend | grep -A 20 "Environment:"
# SPRING_DATASOURCE_URL ì—†ìŒ
```

**í•´ê²°:**
backend.yamlì— í™˜ê²½ë³€ìˆ˜ ì¶”ê°€:
```yaml
env:
  - name: SPRING_KAFKA_BOOTSTRAP_SERVERS
    value: "kafka.log-pipeline.svc.cluster.local:9092"
  - name: SPRING_DATASOURCE_URL
    value: "jdbc:postgresql://postgres-svc.log-pipeline.svc.cluster.local:5432/logs"
  - name: SPRING_DATASOURCE_USERNAME
    value: "admin"
  - name: SPRING_DATASOURCE_PASSWORD
    value: "admin123"
  - name: SPRING_JPA_HIBERNATE_DDL_AUTO
    value: "update"
```

**ì¬ë°°í¬:**
```bash
kubectl apply -f ~/project/distributed-log-pipeline/kubernetes/apps/backend.yaml
kubectl rollout restart deployment/backend -n log-pipeline
```

**í™•ì¸:**
```bash
kubectl logs -n log-pipeline deployment/backend | grep -i hikari
# HikariPool-1 - Start completed. ì¶œë ¥ë˜ë©´ ì„±ê³µ
```
```

---

## ë‹¤ìŒ ë‹¨ê³„ ì •ë¦¬
```
í˜„ì¬ ìƒíƒœ:
âœ… PostgreSQL ì €ì¥ ë™ì‘
âœ… Kafka ì €ì¥ ë™ì‘
â¬œ HDFS ì €ì¥ (Spark Streaming ì‹¤í–‰ í•„ìš”)
â¬œ Query API í…ŒìŠ¤íŠ¸
â¬œ k6 ë¶€í•˜ í…ŒìŠ¤íŠ¸

ìˆœì„œ:
1. Spark Streaming ì‹¤í–‰ â†’ HDFS ì €ì¥
2. Query API ë™ì‘ í™•ì¸
3. k6 ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± ë° í…ŒìŠ¤íŠ¸
4. ëŒ€ìš©ëŸ‰ ë°ì´í„° í…ŒìŠ¤íŠ¸ (Generator ì†ë„ ì¡°ì ˆ)