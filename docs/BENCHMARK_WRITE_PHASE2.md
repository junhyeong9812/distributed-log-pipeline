# Write Performance ë²¤ì¹˜ë§ˆí¬ Phase 2

> Backend ë°°ì¹˜ ì²˜ë¦¬ ê°œì„  í›„ ì¬í…ŒìŠ¤íŠ¸

---

## ğŸ“‹ Phase 1 ê²°ê³¼ ìš”ì•½

### ë°œê²¬ëœ ë¬¸ì œ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Phase 1 ë³‘ëª© ë¶„ì„                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  ëª©í‘œ: 90ë§Œê±´/ë¶„                                                    â”‚
â”‚  ì‹¤ì œ: 20ë§Œê±´/ë¶„ (22% ë‹¬ì„±)                                         â”‚
â”‚                                                                      â”‚
â”‚  ë³‘ëª©: Backend (Spring Boot + JPA)                                  â”‚
â”‚  ì¦ìƒ: "maximum number of running instances reached" ìŠ¤í‚µ ë°œìƒ      â”‚
â”‚                                                                      â”‚
â”‚  ì›ì¸:                                                               â”‚
â”‚  1. JPA saveAll()ì´ ì‹¤ì œë¡œëŠ” ë‹¨ê±´ INSERT ë°˜ë³µ                       â”‚
â”‚  2. Kafka ì „ì†¡ ë™ê¸° ëŒ€ê¸°                                            â”‚
â”‚  3. IDENTITY ì „ëµìœ¼ë¡œ ì¸í•œ ë°°ì¹˜ ë¹„í™œì„±í™”                            â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### í˜„ì¬ ì½”ë“œ ë¬¸ì œì 

```java
// DataService.java í˜„ì¬ ì½”ë“œ
@Transactional
public void processLogs(List<LogEvent> logEvents) {
    List<LogEntity> entities = logEvents.stream()
            .map(event -> LogEntity.builder()...build())
            .toList();

    logRepository.saveAll(entities);  // âš ï¸ ì‹¤ì œë¡œëŠ” ë‹¨ê±´ INSERT ë°˜ë³µ!

    logEvents.forEach(kafkaProducerService::sendLog);  // âš ï¸ ë™ê¸° ì „ì†¡
}
```

### Hibernate ë°°ì¹˜ê°€ ì•ˆ ë˜ëŠ” ì´ìœ : IDENTITY ì „ëµì˜ í•¨ì •

```java
// LogEntity.java
@Id
@GeneratedValue(strategy = GenerationType.IDENTITY)  // âš ï¸ ë¬¸ì œ!
private Long id;
```

#### IDENTITY ì „ëµì˜ ë™ì‘ ë°©ì‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    IDENTITY ì „ëµ ë™ì‘                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  1. INSERT INTO logs (...) VALUES (...);                            â”‚
â”‚                    â†“                                                â”‚
â”‚  2. DBê°€ AUTO_INCREMENTë¡œ ID ìƒì„± (PostgreSQL: SERIAL)              â”‚
â”‚                    â†“                                                â”‚
â”‚  3. ìƒì„±ëœ IDë¥¼ JPAê°€ ì¦‰ì‹œ ê°€ì ¸ì™€ì•¼ í•¨                              â”‚
â”‚     â†’ entity.getId() í˜¸ì¶œ ê°€ëŠ¥í•´ì•¼ í•˜ë‹ˆê¹Œ                           â”‚
â”‚                    â†“                                                â”‚
â”‚  4. SELECT currval('logs_id_seq') ë˜ëŠ” RETURNING id                 â”‚
â”‚                    â†“                                                â”‚
â”‚  5. ë‹¤ìŒ INSERT ì „ì— ì´ ê³¼ì • ì™„ë£Œ í•„ìš”                              â”‚
â”‚                    â†“                                                â”‚
â”‚  ê²°ë¡ : ë°°ì¹˜ ë¶ˆê°€ëŠ¥! (ìˆœì°¨ ì²˜ë¦¬ ê°•ì œ)                                â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### saveAll() ë‚´ë¶€ ë™ì‘ (ì‹¤ì œ)

```java
// ìš°ë¦¬ê°€ í˜¸ì¶œí•œ ì½”ë“œ
logRepository.saveAll(entities);  // 5000ê±´ ì €ì¥ ìš”ì²­

// Hibernate ë‚´ë¶€ì—ì„œ ì‹¤ì œë¡œ ì¼ì–´ë‚˜ëŠ” ì¼
for (LogEntity entity : entities) {
    // 1. INSERT ì‹¤í–‰
    INSERT INTO logs (timestamp, level, service, ...) VALUES (?, ?, ?, ...);
    
    // 2. ìƒì„±ëœ ID ì¡°íšŒ (IDENTITY ì „ëµ ë•Œë¬¸ì— í•„ìˆ˜)
    SELECT currval('logs_id_seq');
    
    // 3. entityì— ID ì„¤ì •
    entity.setId(generatedId);
}

// ê²°ê³¼: 5000ê±´ Ã— 2ì¿¼ë¦¬ = 10,000ë²ˆ DB ì™•ë³µ!
```

#### ID ì „ëµë³„ ë°°ì¹˜ ê°€ëŠ¥ ì—¬ë¶€

| ID ì „ëµ | ë°°ì¹˜ ê°€ëŠ¥ | ì´ìœ  |
|---------|----------|------|
| **IDENTITY** | âŒ ë¶ˆê°€ | DBê°€ ID ìƒì„± â†’ INSERT í›„ ì¦‰ì‹œ ì¡°íšŒ í•„ìš” |
| **SEQUENCE** | âœ… ê°€ëŠ¥ | allocationSizeë¡œ ID ë¯¸ë¦¬ í• ë‹¹ |
| **TABLE** | âœ… ê°€ëŠ¥ | ë³„ë„ í…Œì´ë¸”ì—ì„œ ID ë¯¸ë¦¬ í• ë‹¹ |
| **UUID** | âœ… ê°€ëŠ¥ | ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ìƒì„± (DB ì˜ì¡´ X) |

#### application.ymlì˜ ë°°ì¹˜ ì„¤ì •ì´ ë¬´ì‹œëœ ì´ìœ 

```yaml
# application.yml - ì´ ì„¤ì •ì´ ìˆì—ˆì§€ë§Œ...
spring:
  jpa:
    properties:
      hibernate:
        jdbc:
          batch_size: 100      # ë¬´ì‹œë¨!
        order_inserts: true    # ë¬´ì‹œë¨!
        order_updates: true    # ë¬´ì‹œë¨!
```

**ì´ìœ **: IDENTITY ì „ëµì„ ì‚¬ìš©í•˜ë©´ Hibernateê°€ **ìë™ìœ¼ë¡œ ë°°ì¹˜ë¥¼ ë¹„í™œì„±í™”**í•¨.
ì„¤ì •ì´ ìˆì–´ë„ ë¬´ì‹œë¨!

#### JDBC BatchëŠ” ì™œ ê°€ëŠ¥í•œê°€?

```java
// JDBC Batch - ID ë°˜í™˜ ì•ˆ í•¨
jdbcTemplate.batchUpdate(sql, new BatchPreparedStatementSetter() {
    @Override
    public void setValues(PreparedStatement ps, int i) throws SQLException {
        // ID ì»¬ëŸ¼ ì—†ì´ INSERT
        ps.setDouble(1, timestamp);
        ps.setString(2, level);
        // ...
    }
    
    @Override
    public int getBatchSize() {
        return 5000;
    }
});

// ì‹¤ì œ DBì— ì „ì†¡ë˜ëŠ” ì¿¼ë¦¬ (1ë²ˆì— ë¬¶ì–´ì„œ)
INSERT INTO logs (timestamp, level, ...) VALUES 
    (?, ?, ...),
    (?, ?, ...),
    (?, ?, ...),
    ... (5000ê°œ)
;

// ID ë°˜í™˜ ì•ˆ í•¨ â†’ ë°°ì¹˜ ê°€ëŠ¥!
```

#### ì„±ëŠ¥ ë¹„êµ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    JPA saveAll() vs JDBC Batch                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  JPA saveAll() + IDENTITY (í˜„ì¬):                                   â”‚
â”‚  â”œâ”€â”€ 5000ê±´ INSERT                                                  â”‚
â”‚  â”œâ”€â”€ DB ì™•ë³µ: 10,000íšŒ (INSERT + SELECT ID)                        â”‚
â”‚  â”œâ”€â”€ ë„¤íŠ¸ì›Œí¬ ì§€ì—°: 10,000 Ã— 0.5ms = 5,000ms                       â”‚
â”‚  â””â”€â”€ ì´ ì‹œê°„: ~5~7ì´ˆ                                                â”‚
â”‚                                                                      â”‚
â”‚  JDBC batchUpdate() (ê°œì„ ):                                         â”‚
â”‚  â”œâ”€â”€ 5000ê±´ INSERT                                                  â”‚
â”‚  â”œâ”€â”€ DB ì™•ë³µ: 1íšŒ (ë°°ì¹˜ë¡œ ë¬¶ìŒ)                                     â”‚
â”‚  â”œâ”€â”€ ë„¤íŠ¸ì›Œí¬ ì§€ì—°: 1 Ã— 50ms = 50ms                                â”‚
â”‚  â””â”€â”€ ì´ ì‹œê°„: ~0.05ì´ˆ                                               â”‚
â”‚                                                                      â”‚
â”‚  ì„±ëŠ¥ í–¥ìƒ: 100~150ë°°!                                              â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ìš°ë¦¬ ìƒí™©ì—ì„œ JDBC Batch ì„ íƒ ì´ìœ 

| ê³ ë ¤ì‚¬í•­ | JPA saveAll() | JDBC Batch |
|----------|---------------|------------|
| INSERT í›„ ID í•„ìš”? | âœ… ë°˜í™˜ë¨ | âŒ ë°˜í™˜ ì•ˆ ë¨ |
| ë°°ì¹˜ ì²˜ë¦¬ | âŒ ë¶ˆê°€ (IDENTITY) | âœ… ê°€ëŠ¥ |
| ì„±ëŠ¥ | ëŠë¦¼ | ë¹ ë¦„ |
| ì½”ë“œ ë³µì¡ë„ | ê°„ë‹¨ | ì•½ê°„ ë³µì¡ |

**ìš°ë¦¬ ê²½ìš°**:
- INSERT í›„ IDê°€ í•„ìš” ì—†ìŒ (ë¡œê·¸ ìˆ˜ì§‘ ìš©ë„)
- ì„±ëŠ¥ì´ ìµœìš°ì„ 
- â†’ **JDBC Batch ì„ íƒ**

---

## ğŸ”§ ê°œì„  ë°©ì•ˆ

### 1. JDBC Batch INSERT (ê¶Œì¥)

JPA ëŒ€ì‹  ì§ì ‘ JDBC ë°°ì¹˜ ì‚¬ìš©:

```java
@Service
@RequiredArgsConstructor
public class DataService {

    private final JdbcTemplate jdbcTemplate;
    private final KafkaTemplate<String, Object> kafkaTemplate;

    @Transactional
    public void processLogs(List<LogEvent> logEvents) {
        // 1. JDBC Batch INSERT (1ë²ˆ ì™•ë³µ)
        String sql = "INSERT INTO logs (timestamp, level, service, host, message, metadata, created_at) " +
                     "VALUES (?, ?, ?, ?, ?, ?::jsonb, NOW())";
        
        jdbcTemplate.batchUpdate(sql, new BatchPreparedStatementSetter() {
            @Override
            public void setValues(PreparedStatement ps, int i) throws SQLException {
                LogEvent event = logEvents.get(i);
                ps.setDouble(1, event.getTimestamp().toEpochMilli() / 1000.0);
                ps.setString(2, event.getLevel());
                ps.setString(3, event.getService());
                ps.setString(4, event.getHost());
                ps.setString(5, event.getMessage());
                ps.setString(6, toJson(event.getMetadata()));
            }
            
            @Override
            public int getBatchSize() {
                return logEvents.size();
            }
        });
        
        // 2. Kafka ë¹„ë™ê¸° ë°°ì¹˜ ì „ì†¡
        logEvents.forEach(event -> {
            kafkaTemplate.send("logs.raw", event.getService(), event);
            // ê²°ê³¼ ëŒ€ê¸° ì•ˆ í•¨ (ë¹„ë™ê¸°)
        });
    }
}
```

### 2. Kafka ë¹„ë™ê¸° ì²˜ë¦¬

```java
// KafkaProducerService.java ê°œì„ 
public void sendLogAsync(LogEvent logEvent) {
    String key = logEvent.getService();
    
    // ê²°ê³¼ ëŒ€ê¸° ì•ˆ í•¨ - Fire and Forget
    kafkaTemplate.send(logsTopic, key, logEvent);
    
    // ERROR ë ˆë²¨ë§Œ ì•Œë¦¼ (ë¹„ë™ê¸°)
    if ("ERROR".equalsIgnoreCase(logEvent.getLevel())) {
        kafkaTemplate.send(alertsTopic, key, logEvent);
    }
}

// ë°°ì¹˜ ì „ì†¡
public void sendLogsAsync(List<LogEvent> logEvents) {
    logEvents.forEach(this::sendLogAsync);
}
```

### 3. ID ì „ëµ ë³€ê²½ (ì„ íƒ)

SEQUENCE ì „ëµìœ¼ë¡œ ë³€ê²½í•˜ë©´ JPA ë°°ì¹˜ë„ ê°€ëŠ¥:

```java
@Id
@GeneratedValue(strategy = GenerationType.SEQUENCE, generator = "logs_seq")
@SequenceGenerator(name = "logs_seq", sequenceName = "logs_id_seq", allocationSize = 100)
private Long id;
```

í•˜ì§€ë§Œ JDBC Batchê°€ ë” ë¹ ë¥´ë¯€ë¡œ ê¶Œì¥í•˜ì§€ ì•ŠìŒ.

---

## ğŸ“ ìˆ˜ì • íŒŒì¼ ëª©ë¡

### 1. DataService.java

```
ë³€ê²½ ë‚´ìš©:
- JPA saveAll() â†’ JDBC batchUpdate()
- Kafka ë™ê¸° ì „ì†¡ â†’ ë¹„ë™ê¸° ì „ì†¡
- ObjectMapper ì¶”ê°€ (JSON ë³€í™˜)
```

### 2. KafkaProducerService.java

```
ë³€ê²½ ë‚´ìš©:
- sendLog() â†’ sendLogAsync() (ê²°ê³¼ ëŒ€ê¸° ì œê±°)
- sendLogsAsync() ë°°ì¹˜ ë©”ì„œë“œ ì¶”ê°€
```

### 3. application.yml

```
ë³€ê²½ ë‚´ìš©:
- Kafka producer linger.ms ì„¤ì • (ë°°ì¹˜ ì „ì†¡)
- Kafka producer batch.size ì„¤ì •
```

---

## ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 

### Before (Phase 1)

| í•­ëª© | ê°’ |
|------|-----|
| INSERT ë°©ì‹ | ë‹¨ê±´ 5000ë²ˆ |
| DB ì™•ë³µ | 5000íšŒ |
| Kafka ì „ì†¡ | ë™ê¸° ëŒ€ê¸° |
| ì²˜ë¦¬ëŸ‰ | ~20ë§Œê±´/ë¶„ |

### After (Phase 2)

| í•­ëª© | ê°’ |
|------|-----|
| INSERT ë°©ì‹ | ë°°ì¹˜ 1ë²ˆ |
| DB ì™•ë³µ | 1íšŒ |
| Kafka ì „ì†¡ | ë¹„ë™ê¸° |
| ì˜ˆìƒ ì²˜ë¦¬ëŸ‰ | ~100ë§Œê±´/ë¶„ |

### ì„±ëŠ¥ ê°œì„  ì˜ˆìƒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ì˜ˆìƒ ì„±ëŠ¥ ê°œì„                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  í˜„ì¬: 5000ê±´ INSERT                                                â”‚
â”‚  â”œâ”€â”€ DB ì™•ë³µ: 5000íšŒ Ã— 1ms = 5000ms                                 â”‚
â”‚  â”œâ”€â”€ Kafka ì „ì†¡: 5000íšŒ Ã— 0.5ms = 2500ms                           â”‚
â”‚  â””â”€â”€ ì´: ~7500ms (ë„ˆë¬´ ëŠë¦¼)                                        â”‚
â”‚                                                                      â”‚
â”‚  ê°œì„  í›„: 5000ê±´ INSERT                                             â”‚
â”‚  â”œâ”€â”€ DB ì™•ë³µ: 1íšŒ Ã— 50ms = 50ms                                    â”‚
â”‚  â”œâ”€â”€ Kafka ì „ì†¡: ë¹„ë™ê¸° (ëŒ€ê¸° ì—†ìŒ)                                 â”‚
â”‚  â””â”€â”€ ì´: ~50ms (150ë°° ê°œì„ )                                        â”‚
â”‚                                                                      â”‚
â”‚  ì˜ˆìƒ ì²˜ë¦¬ëŸ‰: 5000ê±´ / 0.05ì´ˆ = 100,000ê±´/ì´ˆ = 600ë§Œê±´/ë¶„           â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê³„íš

### Phase 2-1: ê¸°ë³¸ í…ŒìŠ¤íŠ¸

```bash
# ë™ì¼ ì¡°ê±´ìœ¼ë¡œ ì¬í…ŒìŠ¤íŠ¸
curl -X POST "http://192.168.55.114:30800/control/start?batch_size=5000&log_interval=0.5&event_interval=1"
```

ëª©í‘œ: 90ë§Œê±´/ë¶„ ë‹¬ì„±

### Phase 2-2: í•œê³„ í…ŒìŠ¤íŠ¸

```bash
# ë¶€í•˜ ë” ì¦ê°€
curl -X POST "http://192.168.55.114:30800/control/start?batch_size=10000&log_interval=0.3&event_interval=0.5"
```

ëª©í‘œ: Backend í•œê³„ì  í™•ì¸

### ì¸¡ì • ì§€í‘œ

| ì§€í‘œ | Phase 1 | Phase 2 ëª©í‘œ |
|------|---------|-------------|
| ì²˜ë¦¬ëŸ‰ | 20ë§Œê±´/ë¶„ | 90ë§Œê±´/ë¶„+ |
| ìŠ¤í‚µ ë°œìƒ | ìˆìŒ | ì—†ìŒ |
| ì—ëŸ¬ìœ¨ | 0% | 0% |
| DB ì¿¼ë¦¬ ì‹œê°„ | 67ms | <20ms |

---

## ğŸ“ ì½”ë“œ ë³€ê²½ ìƒì„¸

### DataService.java ì „ì²´ ì½”ë“œ

```java
package com.pipeline.service;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.pipeline.model.ActivityEvent;
import com.pipeline.model.LogEvent;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.jdbc.core.BatchPreparedStatementSetter;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.util.List;

@Slf4j
@Service
@RequiredArgsConstructor
public class DataService {

    private final JdbcTemplate jdbcTemplate;
    private final KafkaProducerService kafkaProducerService;
    private final ObjectMapper objectMapper;

    private String toJson(Object obj) {
        if (obj == null) return "{}";
        try {
            return objectMapper.writeValueAsString(obj);
        } catch (JsonProcessingException e) {
            return "{}";
        }
    }

    @Transactional
    public void processLog(LogEvent logEvent) {
        String sql = "INSERT INTO logs (timestamp, level, service, host, message, metadata, created_at) " +
                     "VALUES (?, ?, ?, ?, ?, ?::jsonb, NOW())";
        
        jdbcTemplate.update(sql,
                logEvent.getTimestamp().toEpochMilli() / 1000.0,
                logEvent.getLevel(),
                logEvent.getService(),
                logEvent.getHost(),
                logEvent.getMessage(),
                toJson(logEvent.getMetadata())
        );

        kafkaProducerService.sendLogAsync(logEvent);
    }

    @Transactional
    public void processLogs(List<LogEvent> logEvents) {
        String sql = "INSERT INTO logs (timestamp, level, service, host, message, metadata, created_at) " +
                     "VALUES (?, ?, ?, ?, ?, ?::jsonb, NOW())";
        
        jdbcTemplate.batchUpdate(sql, new BatchPreparedStatementSetter() {
            @Override
            public void setValues(PreparedStatement ps, int i) throws SQLException {
                LogEvent event = logEvents.get(i);
                ps.setDouble(1, event.getTimestamp().toEpochMilli() / 1000.0);
                ps.setString(2, event.getLevel());
                ps.setString(3, event.getService());
                ps.setString(4, event.getHost());
                ps.setString(5, event.getMessage());
                ps.setString(6, toJson(event.getMetadata()));
            }
            
            @Override
            public int getBatchSize() {
                return logEvents.size();
            }
        });

        log.debug("Batch inserted {} logs", logEvents.size());
        kafkaProducerService.sendLogsAsync(logEvents);
    }

    @Transactional
    public void processActivity(ActivityEvent activityEvent) {
        String sql = "INSERT INTO events (event_id, timestamp, user_id, session_id, event_type, event_data, device, created_at) " +
                     "VALUES (?, ?, ?, ?, ?, ?::jsonb, ?::jsonb, NOW())";
        
        jdbcTemplate.update(sql,
                activityEvent.getEventId(),
                activityEvent.getTimestamp().toEpochMilli() / 1000.0,
                activityEvent.getUserId(),
                activityEvent.getSessionId(),
                activityEvent.getEventType(),
                toJson(activityEvent.getEventData()),
                toJson(activityEvent.getDevice())
        );

        kafkaProducerService.sendActivityAsync(activityEvent);
    }

    @Transactional
    public void processActivities(List<ActivityEvent> activityEvents) {
        String sql = "INSERT INTO events (event_id, timestamp, user_id, session_id, event_type, event_data, device, created_at) " +
                     "VALUES (?, ?, ?, ?, ?, ?::jsonb, ?::jsonb, NOW())";
        
        jdbcTemplate.batchUpdate(sql, new BatchPreparedStatementSetter() {
            @Override
            public void setValues(PreparedStatement ps, int i) throws SQLException {
                ActivityEvent event = activityEvents.get(i);
                ps.setString(1, event.getEventId());
                ps.setDouble(2, event.getTimestamp().toEpochMilli() / 1000.0);
                ps.setString(3, event.getUserId());
                ps.setString(4, event.getSessionId());
                ps.setString(5, event.getEventType());
                ps.setString(6, toJson(event.getEventData()));
                ps.setString(7, toJson(event.getDevice()));
            }
            
            @Override
            public int getBatchSize() {
                return activityEvents.size();
            }
        });

        log.debug("Batch inserted {} events", activityEvents.size());
        kafkaProducerService.sendActivitiesAsync(activityEvents);
    }
}
```

### KafkaProducerService.java ì „ì²´ ì½”ë“œ

```java
package com.pipeline.service;

import com.pipeline.model.ActivityEvent;
import com.pipeline.model.LogEvent;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;

import java.util.List;

@Slf4j
@Service
@RequiredArgsConstructor
public class KafkaProducerService {

    private final KafkaTemplate<String, Object> kafkaTemplate;

    @Value("${kafka.topics.logs}")
    private String logsTopic;

    @Value("${kafka.topics.events}")
    private String eventsTopic;

    @Value("${kafka.topics.alerts}")
    private String alertsTopic;

    // ë‹¨ê±´ ë¹„ë™ê¸° ì „ì†¡
    public void sendLogAsync(LogEvent logEvent) {
        String key = logEvent.getService();
        kafkaTemplate.send(logsTopic, key, logEvent);
        
        if ("ERROR".equalsIgnoreCase(logEvent.getLevel())) {
            kafkaTemplate.send(alertsTopic, key, logEvent);
        }
    }

    // ë°°ì¹˜ ë¹„ë™ê¸° ì „ì†¡
    public void sendLogsAsync(List<LogEvent> logEvents) {
        logEvents.forEach(this::sendLogAsync);
    }

    // ë‹¨ê±´ ë¹„ë™ê¸° ì „ì†¡
    public void sendActivityAsync(ActivityEvent activityEvent) {
        String key = activityEvent.getUserId();
        kafkaTemplate.send(eventsTopic, key, activityEvent);
    }

    // ë°°ì¹˜ ë¹„ë™ê¸° ì „ì†¡
    public void sendActivitiesAsync(List<ActivityEvent> activityEvents) {
        activityEvents.forEach(this::sendActivityAsync);
    }
}
```

### application.yml ì¶”ê°€ ì„¤ì •

```yaml
spring:
  kafka:
    producer:
      # ë°°ì¹˜ ì „ì†¡ ìµœì í™”
      properties:
        linger.ms: 5          # 5ms ëŒ€ê¸° í›„ ë°°ì¹˜ ì „ì†¡
        batch.size: 65536     # 64KB ë°°ì¹˜ í¬ê¸°
        buffer.memory: 33554432  # 32MB ë²„í¼
```

---

## ğŸ”„ ë°°í¬ ì ˆì°¨

```bash
# 1. ì½”ë“œ ìˆ˜ì •
# DataService.java, KafkaProducerService.java êµì²´

# 2. ë¹Œë“œ
cd ~/project/distributed-log-pipeline/backend
./gradlew build -x test

# 3. Docker ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t log-pipeline-backend:latest .

# 4. k3sì— ë¡œë“œ
docker save log-pipeline-backend:latest | sudo k3s ctr images import -

# 5. Pod ì¬ì‹œì‘
kubectl delete pod -n log-pipeline -l app=backend

# 6. í™•ì¸
kubectl get pods -n log-pipeline | grep backend
kubectl logs deployment/backend -n log-pipeline --tail=20
```

---

## ğŸ§ª Phase 2 í…ŒìŠ¤íŠ¸ ê²°ê³¼

### í…ŒìŠ¤íŠ¸ ì¼ì‹œ
- 2026ë…„ 1ì›” 12ì¼ 19:28 ~ 19:38

### í™˜ê²½ (Phase 1ê³¼ ë™ì¼)
| ë…¸ë“œ | ì—­í•  | ë¦¬ì†ŒìŠ¤ ì œí•œ |
|------|------|------------|
| Master | PostgreSQL, Kafka, NameNode | - |
| Worker 1 | DataNode, Spark Worker | 2 CPU, 2GB |
| Worker 2 | DataNode, Spark Worker | 2 CPU, 2GB |

---

### Phase 2-1: 90ë§Œê±´/ë¶„ (ëª©í‘œ ë‹¬ì„± í…ŒìŠ¤íŠ¸)

| ì„¤ì • | ê°’ |
|------|-----|
| batch_size | 5,000 |
| log_interval | 0.5ì´ˆ |
| event_interval | 1ì´ˆ |
| ëª©í‘œ ì²˜ë¦¬ëŸ‰ | 900,000ê±´/ë¶„ |

**ê²°ê³¼:**

| ì§€í‘œ | PostgreSQL | HDFS |
|------|------------|------|
| 10ì´ˆë‹¹ ì¦ê°€ëŸ‰ | ~110,000ê±´ | ~110,000ê±´ |
| ì‹¤ì œ ì²˜ë¦¬ëŸ‰ | **~660,000ê±´/ë¶„** (logs) | **~660,000ê±´/ë¶„** (logs) |
| ì´ ì²˜ë¦¬ëŸ‰ (events í¬í•¨) | **~900,000ê±´/ë¶„** âœ… | **~900,000ê±´/ë¶„** âœ… |
| ì—ëŸ¬ | 0 | 0 |
| ìŠ¤í‚µ | ì—†ìŒ âœ… | - |

**âœ… ëª©í‘œ ë‹¬ì„±! Phase 1 ëŒ€ë¹„ 4.5ë°° ì„±ëŠ¥ í–¥ìƒ**

---

### Phase 2-2: 180ë§Œê±´/ë¶„ (2ë°° ë¶€í•˜)

| ì„¤ì • | ê°’ |
|------|-----|
| batch_size | 10,000 |
| log_interval | 0.5ì´ˆ |
| event_interval | 1ì´ˆ |
| ëª©í‘œ ì²˜ë¦¬ëŸ‰ | 1,800,000ê±´/ë¶„ |

**ê²°ê³¼:**

| ì§€í‘œ | PostgreSQL | HDFS |
|------|------------|------|
| ì²˜ë¦¬ëŸ‰ | ~180ë§Œê±´/ë¶„ âœ… | ~180ë§Œê±´/ë¶„ âœ… |
| ì¿¼ë¦¬ ì‹œê°„ | 345ms | 3,665ms |
| ì—ëŸ¬ | 0 | 0 |
| Pod ìƒíƒœ | ì•ˆì • | ì•ˆì • |

**ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰:**

| Pod | CPU | Memory |
|-----|-----|--------|
| PostgreSQL | 330m (16%) | 230Mi (11%) |
| Backend | 378m (19%) | 315Mi |
| Kafka | 194m | 989Mi |
| Spark Worker | 182m | 1,283Mi |

**âœ… ë‘˜ ë‹¤ ì•ˆì •ì ìœ¼ë¡œ ì²˜ë¦¬!**

---

### Phase 2-3: 360ë§Œê±´/ë¶„ (ê·¹í•œ í…ŒìŠ¤íŠ¸)

| ì„¤ì • | ê°’ |
|------|-----|
| batch_size | 20,000 |
| log_interval | 0.5ì´ˆ |
| event_interval | 1ì´ˆ |
| ëª©í‘œ ì²˜ë¦¬ëŸ‰ | 3,600,000ê±´/ë¶„ |

**ê²°ê³¼:**

| ì§€í‘œ | PostgreSQL | HDFS/Spark |
|------|------------|------------|
| ìµœì¢… ì €ì¥ëŸ‰ | **5,148,600ê±´** âœ… | **3,998,600ê±´** âŒ |
| ìƒíƒœ | ê³„ì† ë™ì‘ | **ì‚¬ë§** |
| ì¿¼ë¦¬ ì‹œê°„ | 405ms â†’ 743ms | 3,872ms |
| ì—ëŸ¬ | 0 | DataNode excluded |

**HDFS/Spark ì‚¬ë§ ì›ì¸:**

```
Error: File could only be written to 0 of the 1 minReplication nodes.
       There are 2 datanode(s) running and 
       2 node(s) are excluded in this operation.

ì›ì¸: DataNodeê°€ ì“°ê¸° ì†ë„ë¥¼ ëª» ë”°ë¼ê°€ì„œ "excluded" ì²˜ë¦¬ë¨
     â†’ ì“¸ ìˆ˜ ìˆëŠ” ë…¸ë“œê°€ 0ê°œ
     â†’ Parquet íŒŒì¼ ì €ì¥ ì‹¤íŒ¨
     â†’ Spark Streaming ì‚¬ë§
```

**PostgreSQL ìƒíƒœ:**
- ì¿¼ë¦¬ ì‹œê°„ ì¦ê°€ (318ms â†’ 743ms) í•˜ì§€ë§Œ ê³„ì† ë™ì‘
- 514ë§Œê±´ê¹Œì§€ ì €ì¥ ì™„ë£Œ
- Pod Restart ì—†ìŒ

---

### Phase 2 ê²°ê³¼ ìš”ì•½

| ë¶€í•˜ | PostgreSQL | HDFS/Spark | ìŠ¹ì |
|------|------------|------------|------|
| 90ë§Œê±´/ë¶„ | âœ… ì•ˆì • | âœ… ì•ˆì • | ë¬´ìŠ¹ë¶€ |
| 180ë§Œê±´/ë¶„ | âœ… ì•ˆì • | âœ… ì•ˆì • | ë¬´ìŠ¹ë¶€ |
| 360ë§Œê±´/ë¶„ | âœ… **ë™ì‘** | âŒ **ì‚¬ë§** | **PostgreSQL** |

---

### Phase 1 vs Phase 2 ë¹„êµ

| ì§€í‘œ | Phase 1 (JPA) | Phase 2 (JDBC Batch) | ê°œì„ ìœ¨ |
|------|---------------|---------------------|--------|
| ìµœëŒ€ ì²˜ë¦¬ëŸ‰ | 20ë§Œê±´/ë¶„ | **180ë§Œê±´/ë¶„+** | **9ë°°** |
| ìŠ¤í‚µ ë°œìƒ | ìˆìŒ | ì—†ìŒ | âœ… |
| PostgreSQL í•œê³„ | ë¯¸ë„ë‹¬ | **360ë§Œê±´/ë¶„ì—ì„œë„ ë™ì‘** | âœ… |
| HDFS í•œê³„ | ë¯¸ë„ë‹¬ | **360ë§Œê±´/ë¶„ì—ì„œ ì‚¬ë§** | í™•ì¸ë¨ |

---

### í•µì‹¬ ë°œê²¬

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Phase 2 í•µì‹¬ ë°œê²¬                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  1. JDBC Batch INSERT íš¨ê³¼                                          â”‚
â”‚     - JPA saveAll() â†’ JDBC batchUpdate()                           â”‚
â”‚     - ì„±ëŠ¥ 9ë°° í–¥ìƒ (20ë§Œ â†’ 180ë§Œê±´/ë¶„)                            â”‚
â”‚     - ê·¼ë³¸ ì›ì¸: IDENTITY ì „ëµì˜ ë°°ì¹˜ ë¹„í™œì„±í™” ë¬¸ì œ í•´ê²°            â”‚
â”‚                                                                      â”‚
â”‚  2. PostgreSQLì˜ ë†€ë¼ìš´ ì„±ëŠ¥                                        â”‚
â”‚     - 360ë§Œê±´/ë¶„ì—ì„œë„ ê³„ì† ë™ì‘                                    â”‚
â”‚     - ì¿¼ë¦¬ ì‹œê°„ ì¦ê°€í•˜ì§€ë§Œ ì•ˆì •ì                                    â”‚
â”‚     - ë‹¨ì¼ ë…¸ë“œ + SSDì˜ í˜                                          â”‚
â”‚                                                                      â”‚
â”‚  3. HDFS/Sparkì˜ í•œê³„                                               â”‚
â”‚     - DataNode I/O ë³‘ëª©                                             â”‚
â”‚     - 360ë§Œê±´/ë¶„ì—ì„œ ì‚¬ë§                                           â”‚
â”‚     - ë¶„ì‚° ì‹œìŠ¤í…œë„ ìì› í•œê³„ ìˆìŒ                                  â”‚
â”‚                                                                      â”‚
â”‚  4. ì´ í™˜ê²½ì˜ ê²°ë¡                                                   â”‚
â”‚     - ì†Œ~ì¤‘ê·œëª¨: PostgreSQL ìš°ì„¸                                    â”‚
â”‚     - ëŒ€ê·œëª¨ (PBê¸‰): ì—¬ì „íˆ HDFS í•„ìš” (ìˆ˜í‰ í™•ì¥)                  â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ ë‹¤ìŒ ë‹¨ê³„: Phase 3 (ë¦¬ì†ŒìŠ¤ í™•ì¥ í…ŒìŠ¤íŠ¸)

### í™•ì¥ ê³„íš

| ì»´í¬ë„ŒíŠ¸ | í˜„ì¬ | í™•ì¥ | ëª©ì  |
|----------|------|------|------|
| DataNode | 2CPU/2GB | 3CPU/3GB | I/O ì²˜ë¦¬ëŸ‰ ì¦ê°€ |
| Spark Worker | 2CPU/2GB | 3CPU/3GB | ì²˜ë¦¬ ì†ë„ ì¦ê°€ |

### ì˜ˆìƒ ê²°ê³¼
- HDFSê°€ 360ë§Œê±´/ë¶„ ë²„í‹¸ ìˆ˜ ìˆëŠ”ì§€ í™•ì¸
- PostgreSQL vs HDFS ê³µì •í•œ ë¹„êµ

---

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

- [BENCHMARK_WRITE_RESULT.md](BENCHMARK_WRITE_RESULT.md) - Phase 1 ê²°ê³¼
- [WHY_HDFS_SPARK.md](WHY_HDFS_SPARK.md) - ì‹œìŠ¤í…œ ì„ íƒ ê°€ì´ë“œ
- [ARCHITECTURE.md](ARCHITECTURE.md) - ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜