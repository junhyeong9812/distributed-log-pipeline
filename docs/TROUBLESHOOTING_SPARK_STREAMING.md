# Spark Streaming íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ

> DataNode Excluded ë¬¸ì œ ë° Kafka ë°±í”„ë ˆì…” ì²˜ë¦¬

---

## ğŸš¨ ë¬¸ì œ ìƒí™©

### ì—ëŸ¬ ë©”ì‹œì§€

```
org.apache.hadoop.ipc.RemoteException(java.io.IOException): 
File /data/logs/raw/year=2026/month=1/day=12/hour=11/part-00000-xxx.snappy.parquet 
could only be written to 0 of the 1 minReplication nodes. 
There are 2 datanode(s) running and 2 node(s) are excluded in this operation.
```

### ë°œìƒ ì›ì¸

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DataNode Excluded ë°œìƒ ê³¼ì •                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  1. Generatorê°€ 180ë§Œê±´/ë¶„ ì†ë„ë¡œ Kafkaì— ì „ì†¡                      â”‚
â”‚                        â†“                                            â”‚
â”‚  2. Spark Streamingì´ Kafkaì—ì„œ ëŒ€ëŸ‰ ë©”ì‹œì§€ ì½ìŒ                    â”‚
â”‚                        â†“                                            â”‚
â”‚  3. HDFS DataNodeì— Parquet íŒŒì¼ ì“°ê¸° ì‹œë„                         â”‚
â”‚                        â†“                                            â”‚
â”‚  4. DataNodeê°€ ì“°ê¸° ì†ë„ë¥¼ ëª» ë”°ë¼ê°                                â”‚
â”‚                        â†“                                            â”‚
â”‚  5. DataNode ì‘ë‹µ ì§€ì—° â†’ NameNodeê°€ "excluded" ì²˜ë¦¬                â”‚
â”‚                        â†“                                            â”‚
â”‚  6. ì“¸ ìˆ˜ ìˆëŠ” ë…¸ë“œ 0ê°œ â†’ Spark Job ì‹¤íŒ¨                           â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì¶”ê°€ ë¬¸ì œ: Kafka ë©”ì‹œì§€ ë°±ë¡œê·¸

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Kafka ë°±ë¡œê·¸ ë¬¸ì œ                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  Spark Streaming ì‚¬ë§ ë™ì•ˆ:                                         â”‚
â”‚  - GeneratorëŠ” ê³„ì† Kafkaì— ë©”ì‹œì§€ ì „ì†¡                             â”‚
â”‚  - Kafkaì— ìˆ˜ë°±ë§Œ ê±´ì˜ ë©”ì‹œì§€ ì ì²´                                  â”‚
â”‚                                                                      â”‚
â”‚  Spark ì¬ì‹œì‘ ì‹œ:                                                   â”‚
â”‚  - ë°€ë¦° ë©”ì‹œì§€ í•œêº¼ë²ˆì— ì²˜ë¦¬ ì‹œë„                                   â”‚
â”‚  - ë‹¤ì‹œ DataNode excluded ë°œìƒ                                      â”‚
â”‚  - ë¬´í•œ ë£¨í”„!                                                       â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… í•´ê²° ë°©ë²•

### 1. Spark Streaming ë°°ì¹˜ ì œí•œ ì„¤ì •

#### ë°©ë²• A: spark-submit ì˜µì…˜ìœ¼ë¡œ ì œí•œ

```bash
kubectl exec -it deployment/spark-master -n log-pipeline -- \
  /spark/bin/spark-submit \
  --master spark://spark-master-svc:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 \
  --conf spark.streaming.kafka.maxRatePerPartition=10000 \
  --conf spark.sql.streaming.maxBatchDuration=30s \
  /opt/spark-jobs/raw_to_hdfs.py
```

| ì˜µì…˜ | ì„¤ëª… |
|------|------|
| `maxRatePerPartition` | íŒŒí‹°ì…˜ë‹¹ ì´ˆë‹¹ ìµœëŒ€ ë ˆì½”ë“œ ìˆ˜ |
| `maxBatchDuration` | ë°°ì¹˜ ìµœëŒ€ ì²˜ë¦¬ ì‹œê°„ |

#### ë°©ë²• B: raw_to_hdfs.py ì½”ë“œ ìˆ˜ì • (ê¶Œì¥)

```python
# raw_to_hdfs.py ìˆ˜ì •

df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_servers) \
    .option("subscribe", "logs.raw") \
    .option("startingOffsets", "earliest") \
    .option("maxOffsetsPerTrigger", 50000)  # â­ ë°°ì¹˜ë‹¹ ìµœëŒ€ 5ë§Œê±´ ì œí•œ
    .load()
```

| ì˜µì…˜ | ê°’ | ì„¤ëª… |
|------|-----|------|
| `maxOffsetsPerTrigger` | 50000 | ë°°ì¹˜ë‹¹ ìµœëŒ€ ì²˜ë¦¬ ê±´ìˆ˜ |

#### íš¨ê³¼

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ë°°ì¹˜ ì œí•œ íš¨ê³¼                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  Before (ì œí•œ ì—†ìŒ):                                                â”‚
â”‚  - Kafkaì— 100ë§Œê±´ ì ì²´                                             â”‚
â”‚  - Spark: "100ë§Œê±´ í•œë²ˆì— ì²˜ë¦¬í•˜ì!"                                â”‚
â”‚  - DataNode: ğŸ’€ ì‚¬ë§                                                â”‚
â”‚                                                                      â”‚
â”‚  After (5ë§Œê±´ ì œí•œ):                                                â”‚
â”‚  - Kafkaì— 100ë§Œê±´ ì ì²´                                             â”‚
â”‚  - Spark: "5ë§Œê±´ì”© 20ë²ˆ ë‚˜ëˆ ì„œ ì²˜ë¦¬í•˜ì"                            â”‚
â”‚  - DataNode: âœ… ì•ˆì •ì  ì²˜ë¦¬                                         â”‚
â”‚  - 20ë°°ì¹˜ Ã— 10ì´ˆ = ì•½ 3ë¶„ì— ë”°ë¼ì¡ìŒ                                â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2. DataNode ë¦¬ì†ŒìŠ¤ í™•ì¥ (ì¶”ê°€ ëŒ€ì‘)

```yaml
# kubernetes/hdfs/datanode.yaml
resources:
  requests:
    memory: "2Gi"
    cpu: "2"
  limits:
    memory: "4Gi"
    cpu: "3"
```

---

## ğŸ“ ê¶Œì¥ ì„¤ì •ê°’

### ì²˜ë¦¬ëŸ‰ë³„ maxOffsetsPerTrigger ì„¤ì •

| ëª©í‘œ ì²˜ë¦¬ëŸ‰ | maxOffsetsPerTrigger | ë°°ì¹˜ ì£¼ê¸° | DataNode ë¶€í•˜ |
|-------------|---------------------|-----------|---------------|
| ì•ˆì • (ëŠë¦¼) | 10,000 | 10ì´ˆ | ë‚®ìŒ |
| ê· í˜• | 50,000 | 10ì´ˆ | ì¤‘ê°„ |
| ê³ ì† | 100,000 | 10ì´ˆ | ë†’ìŒ |
| ìµœëŒ€ | ì œí•œ ì—†ìŒ | 10ì´ˆ | âš ï¸ ìœ„í—˜ |

### í™˜ê²½ë³„ ê¶Œì¥ê°’

| í™˜ê²½ | DataNode ë¦¬ì†ŒìŠ¤ | ê¶Œì¥ maxOffsetsPerTrigger |
|------|----------------|---------------------------|
| ê°œë°œ (2CPU/2GB) | ì œí•œì  | 10,000 ~ 30,000 |
| í…ŒìŠ¤íŠ¸ (3CPU/4GB) | ë³´í†µ | 50,000 ~ 100,000 |
| í”„ë¡œë•ì…˜ (8CPU/16GB+) | ì¶©ë¶„ | 200,000+ ë˜ëŠ” ì œí•œ ì—†ìŒ |

---

## ğŸ”§ raw_to_hdfs.py ìˆ˜ì • ì˜ˆì‹œ

### ìˆ˜ì • ì „

```python
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_servers) \
    .option("subscribe", "logs.raw") \
    .option("startingOffsets", "earliest") \
    .load()
```

### ìˆ˜ì • í›„

```python
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_servers) \
    .option("subscribe", "logs.raw") \
    .option("startingOffsets", "earliest") \
    .option("maxOffsetsPerTrigger", 50000)  # ë°°ì¹˜ë‹¹ ìµœëŒ€ 5ë§Œê±´
    .load()
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### Kafka ì ì²´ëŸ‰ í™•ì¸

```bash
kubectl exec -it deployment/kafka -n log-pipeline -- \
  /opt/kafka/bin/kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --group spark-kafka-source-* \
  --describe
```

### Spark Streaming ì§„í–‰ ìƒí™©

```bash
# Spark Master ë¡œê·¸
kubectl logs deployment/spark-master -n log-pipeline --tail=50
```

### DataNode ìƒíƒœ

```bash
kubectl top pods -n log-pipeline | grep datanode
```

---

## ğŸš€ ë³µêµ¬ ì ˆì°¨

### ìƒí™©: Spark ì‚¬ë§ + Kafkaì— ë©”ì‹œì§€ ì ì²´

```bash
# 1. Generator ì¤‘ì§€ (ì¶”ê°€ ì ì²´ ë°©ì§€)
curl -X POST "http://192.168.55.114:30800/control/stop"

# 2. ë°°ì¹˜ ì œí•œ ê±¸ê³  Spark ì¬ì‹¤í–‰
kubectl exec -it deployment/spark-master -n log-pipeline -- \
  /spark/bin/spark-submit \
  --master spark://spark-master-svc:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 \
  --conf spark.streaming.kafka.maxRatePerPartition=10000 \
  /opt/spark-jobs/raw_to_hdfs.py

# 3. HDFSê°€ PostgreSQL ë”°ë¼ì¡ì„ ë•Œê¹Œì§€ ëŒ€ê¸°
watch -n 30 ~/project/distributed-log-pipeline/benchmark_monitor.sh

# 4. ë”°ë¼ì¡ìœ¼ë©´ Generator ì¬ì‹œì‘ (ë‚®ì€ ì†ë„ë¡œ)
curl -X POST "http://192.168.55.114:30800/control/start?batch_size=5000&log_interval=0.5&event_interval=1"
```

---

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

- [BENCHMARK_WRITE_PHASE2.md](BENCHMARK_WRITE_PHASE2.md) - Write ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
- [BENCHMARK_PHASE4.md](BENCHMARK_PHASE4.md) - 1ì–µê±´ ì ì¬ í…ŒìŠ¤íŠ¸
- [WHY_HDFS_SPARK.md](WHY_HDFS_SPARK.md) - ì‹œìŠ¤í…œ ì„ íƒ ê°€ì´ë“œ

---

## ğŸ“Œ í•µì‹¬ ìš”ì•½

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    í•µì‹¬ í¬ì¸íŠ¸                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  ë¬¸ì œ: DataNodeê°€ ì“°ê¸° ì†ë„ë¥¼ ëª» ë”°ë¼ê°                             â”‚
â”‚                                                                      â”‚
â”‚  ì›ì¸: Sparkê°€ Kafka ë©”ì‹œì§€ë¥¼ í•œë²ˆì— ë„ˆë¬´ ë§ì´ ì²˜ë¦¬                 â”‚
â”‚                                                                      â”‚
â”‚  í•´ê²°: maxOffsetsPerTriggerë¡œ ë°°ì¹˜ë‹¹ ì²˜ë¦¬ëŸ‰ ì œí•œ                    â”‚
â”‚                                                                      â”‚
â”‚  ê¶Œì¥ê°’: 50,000ê±´/ë°°ì¹˜ (í™˜ê²½ì— ë”°ë¼ ì¡°ì ˆ)                           â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```