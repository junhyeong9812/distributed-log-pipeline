# Worker ë…¸ë“œ ì„¤ì • ê°€ì´ë“œ

> ë¦¬ëˆ…ìŠ¤ A (192.168.55.158), ë¦¬ëˆ…ìŠ¤ B (192.168.55.9)ì—ì„œ ì‹¤í–‰í•˜ëŠ” Worker ë…¸ë“œ ì„¤ì • ê°€ì´ë“œì…ë‹ˆë‹¤.

---

## ğŸ“‹ ì‚¬ì „ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´ ê¶Œì¥ ì‚¬ì–‘
- CPU: 2ì½”ì–´ ì´ìƒ
- RAM: 4GB ì´ìƒ
- ì €ì¥ê³µê°„: 30GB ì´ìƒ

### ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­
- Ubuntu 20.04+ ë˜ëŠ” ìœ ì‚¬ Linux ë°°í¬íŒ
- Docker 20.10+
- Git

### ë„¤íŠ¸ì›Œí¬ ìš”êµ¬ì‚¬í•­
- Master ë…¸ë“œ (192.168.55.114)ì™€ ë™ì¼ ë„¤íŠ¸ì›Œí¬
- í•„ìš”í•œ í¬íŠ¸ê°€ ë°©í™”ë²½ì—ì„œ í—ˆìš©ë˜ì–´ì•¼ í•¨

---

## ğŸ³ Docker ì„¤ì¹˜

```bash
# Docker ì„¤ì¹˜
curl -fsSL https://get.docker.com | sh

# í˜„ì¬ ì‚¬ìš©ìë¥¼ docker ê·¸ë£¹ì— ì¶”ê°€
sudo usermod -aG docker $USER

# ë¡œê·¸ì•„ì›ƒ í›„ ë‹¤ì‹œ ë¡œê·¸ì¸í•˜ì—¬ ê·¸ë£¹ ì ìš©
newgrp docker

# ì„¤ì¹˜ í™•ì¸
docker --version
docker compose version
```

---

## ğŸ“ í”„ë¡œì íŠ¸ í´ë¡ 

```bash
# í”„ë¡œì íŠ¸ í´ë¡ 
git clone https://github.com/your-repo/distributed-log-pipeline.git
cd distributed-log-pipeline
```

---

## ğŸ–¥ï¸ Docker Compose ë¶„ì‚° í™˜ê²½ (Worker)

### 1. /etc/hosts ì„¤ì •

Worker ë…¸ë“œì—ì„œëŠ” í˜¸ìŠ¤íŠ¸ëª… í•´ì„ì„ ìœ„í•´ /etc/hosts ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.

#### Worker 1 (192.168.55.158)

```bash
# ìê¸° ìì‹ ì˜ í˜¸ìŠ¤íŠ¸ëª… ì¶”ê°€
echo "127.0.0.1 $(hostname)" | sudo tee -a /etc/hosts

# Master í˜¸ìŠ¤íŠ¸ëª… ì¶”ê°€
echo "192.168.55.114 jun-Victus-by-HP-Gaming-Laptop-16-r0xxx.local" | sudo tee -a /etc/hosts

# ë‹¤ë¥¸ Worker ì¶”ê°€
echo "192.168.55.9 worker2" | sudo tee -a /etc/hosts
```

#### Worker 2 (192.168.55.9)

```bash
# ìê¸° ìì‹ ì˜ í˜¸ìŠ¤íŠ¸ëª… ì¶”ê°€
echo "127.0.0.1 $(hostname)" | sudo tee -a /etc/hosts

# Master í˜¸ìŠ¤íŠ¸ëª… ì¶”ê°€
echo "192.168.55.114 jun-Victus-by-HP-Gaming-Laptop-16-r0xxx.local" | sudo tee -a /etc/hosts

# ë‹¤ë¥¸ Worker ì¶”ê°€
echo "192.168.55.158 worker1" | sudo tee -a /etc/hosts
```

### 2. Worker ì„œë¹„ìŠ¤ ì‹œì‘

#### Worker 1

```bash
cd deploy
docker compose -f docker-compose.worker1.yml up -d
```

#### Worker 2

```bash
cd deploy
docker compose -f docker-compose.worker2.yml up -d
```

### 3. ì„œë¹„ìŠ¤ í™•ì¸

```bash
docker compose -f docker-compose.worker1.yml ps  # Worker 1
docker compose -f docker-compose.worker2.yml ps  # Worker 2
```

### Workerì—ì„œ ì‹¤í–‰ë˜ëŠ” ì„œë¹„ìŠ¤

| ì„œë¹„ìŠ¤ | í¬íŠ¸ | ì„¤ëª… |
|--------|------|------|
| HDFS DataNode | 9864, 9866, 9867 | ë¶„ì‚° íŒŒì¼ ì‹œìŠ¤í…œ ë°ì´í„° ë…¸ë“œ |
| Spark Worker | 8081 | Spark ì›Œì»¤ ë…¸ë“œ |

---

## â˜¸ï¸ Kubernetes í™˜ê²½ (k3s Worker)

### 1. Masterì—ì„œ í† í° í™•ì¸

Master ë…¸ë“œì—ì„œ ë‹¤ìŒ ëª…ë ¹ ì‹¤í–‰:

```bash
sudo cat /var/lib/rancher/k3s/server/node-token
```

### 2. k3s Agent ì„¤ì¹˜

Worker ë…¸ë“œì—ì„œ ì‹¤í–‰ (í† í° ê°’ êµì²´ í•„ìš”):

```bash
curl -sfL https://get.k3s.io | K3S_URL=https://192.168.55.114:6443 K3S_TOKEN=<í† í°ê°’> sh -
```

ì˜ˆì‹œ:
```bash
curl -sfL https://get.k3s.io | K3S_URL=https://192.168.55.114:6443 K3S_TOKEN=K10abc123def456::server:xyz789 sh -
```

### 3. ì„¤ì¹˜ í™•ì¸

Master ë…¸ë“œì—ì„œ:

```bash
kubectl get nodes
```

ì˜ˆìƒ ì¶œë ¥:
```
NAME         STATUS   ROLES                  AGE
jun-victus   Ready    control-plane,master   10m
jun          Ready    <none>                 2m
jun-mini1    Ready    <none>                 2m
```

### 4. HDFS DataNode ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±

Worker ë…¸ë“œì—ì„œ:

```bash
sudo mkdir -p /data/hdfs/datanode
sudo chmod 777 /data/hdfs/datanode
```

---

## ğŸ”§ Docker Compose Worker ì„¤ì • íŒŒì¼

### docker-compose.worker1.yml (192.168.55.158)

```yaml
services:
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://192.168.55.114:9000
      - HDFS_CONF_dfs_replication=1
      - HDFS_CONF_dfs_datanode_use_datanode_hostname=false
      - HDFS_CONF_dfs_client_use_datanode_hostname=false
    volumes:
      - datanode_data:/hadoop/dfs/data
    network_mode: host

  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker
    environment:
      - SPARK_MASTER=spark://192.168.55.114:7077
      - SPARK_LOCAL_IP=192.168.55.158
      - SPARK_WORKER_OPTS=-Djava.net.preferIPv4Stack=true
    volumes:
      - ./spark-jobs:/opt/spark-jobs
    network_mode: host

volumes:
  datanode_data:
```

### docker-compose.worker2.yml (192.168.55.9)

```yaml
services:
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://192.168.55.114:9000
      - HDFS_CONF_dfs_replication=1
      - HDFS_CONF_dfs_datanode_use_datanode_hostname=false
      - HDFS_CONF_dfs_client_use_datanode_hostname=false
    volumes:
      - datanode_data:/hadoop/dfs/data
    network_mode: host

  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker
    environment:
      - SPARK_MASTER=spark://192.168.55.114:7077
      - SPARK_LOCAL_IP=192.168.55.9
      - SPARK_WORKER_OPTS=-Djava.net.preferIPv4Stack=true
    volumes:
      - ./spark-jobs:/opt/spark-jobs
    network_mode: host

volumes:
  datanode_data:
```

---

## ğŸ” ë¡œê·¸ í™•ì¸

### Docker Compose

```bash
# DataNode ë¡œê·¸
docker logs -f datanode

# Spark Worker ë¡œê·¸
docker logs -f spark-worker
```

### Kubernetes

Master ë…¸ë“œì—ì„œ:

```bash
# DataNode ë¡œê·¸
kubectl logs -n log-pipeline daemonset/datanode --tail=50

# Spark Worker ë¡œê·¸
kubectl logs -n log-pipeline daemonset/spark-worker --tail=50
```

---

## âœ… ì—°ê²° í™•ì¸

### HDFS DataNode ë“±ë¡ í™•ì¸

Master ë…¸ë“œì—ì„œ:

```bash
# Docker Compose
docker exec namenode hdfs dfsadmin -report

# Kubernetes
kubectl exec -n log-pipeline deployment/namenode -- hdfs dfsadmin -report
```

ì˜ˆìƒ ì¶œë ¥:
```
Live datanodes (2):
  Name: 192.168.55.158:9866
  Name: 192.168.55.9:9866
```

### Spark Worker ë“±ë¡ í™•ì¸

ë¸Œë¼ìš°ì €ì—ì„œ:
- Docker Compose: http://192.168.55.114:8082
- Kubernetes: http://192.168.55.114:30082

Workers (2) í‘œì‹œ í™•ì¸

---

## ğŸ›‘ ì„œë¹„ìŠ¤ ì¤‘ì§€

### Docker Compose

```bash
# Worker 1
docker compose -f docker-compose.worker1.yml down

# Worker 2
docker compose -f docker-compose.worker2.yml down
```

### Kubernetes

Master ë…¸ë“œì—ì„œ:

```bash
# ì „ì²´ ë¦¬ì†ŒìŠ¤ ì‚­ì œ
kubectl delete namespace log-pipeline
```

Worker ë…¸ë“œì—ì„œ:

```bash
# k3s agent ì œê±°
/usr/local/bin/k3s-agent-uninstall.sh
```

---

## ğŸ§¹ ë°ì´í„° ì •ë¦¬

HDFS Cluster ID ë¶ˆì¼ì¹˜ ë“±ì˜ ë¬¸ì œ ë°œìƒ ì‹œ:

```bash
# DataNode ë°ì´í„° ì‚­ì œ
sudo rm -rf /data/hdfs/datanode/*

# ì„œë¹„ìŠ¤ ì¬ì‹œì‘
docker compose -f docker-compose.worker1.yml down
docker compose -f docker-compose.worker1.yml up -d
```

---

## â“ ë¬¸ì œ í•´ê²°

ìì„¸í•œ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…ì€ [TROUBLESHOOTING.md](TROUBLESHOOTING.md)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.

íŠ¹íˆ ë‹¤ìŒ ë¬¸ì œë“¤ì„ í™•ì¸í•˜ì„¸ìš”:
- ë¬¸ì œ 9: network_mode: hostì—ì„œ extra_hosts ë¬´ì‹œë¨
- ë¬¸ì œ 10: Spark Worker ìê¸° í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨
- ë¬¸ì œ 11: Spark Worker IPv6 í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨
- ë¬¸ì œ 20: HDFS Cluster ID ë¶ˆì¼ì¹˜