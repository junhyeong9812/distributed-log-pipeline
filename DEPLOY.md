# ë¶„ì‚° í™˜ê²½ ë°°í¬ ê°€ì´ë“œ

## ğŸ“‹ ì‹œìŠ¤í…œ êµ¬ì„±

| PC | IP | ì—­í•  | ì„œë¹„ìŠ¤ |
|----|-----|------|--------|
| ë…¸íŠ¸ë¶ | 192.168.55.114 | Master | Kafka, NameNode, Spark Master, Airflow, Grafana |
| ë¦¬ëˆ…ìŠ¤ A | 192.168.55.158 | Worker 1 | DataNode, Spark Worker |
| ë¦¬ëˆ…ìŠ¤ B | 192.168.55.9 | Worker 2 | DataNode, Spark Worker |

## ğŸ”§ ì‚¬ì „ ìš”êµ¬ì‚¬í•­

ëª¨ë“  PCì— í•„ìš”:
- Ubuntu 20.04+ ë˜ëŠ” í˜¸í™˜ Linux
- Docker 20.10+
- Docker Compose V2
- Git
- ìµœì†Œ 4GB RAM, 20GB ë””ìŠ¤í¬

## ğŸ“¦ Step 1: Master ë…¸ë“œ ì„¤ì • (ë…¸íŠ¸ë¶)

### 1-1: í”„ë¡œì íŠ¸ Clone (ìµœì´ˆ 1íšŒ)
```bash
cd ~/project
git clone <your-repo-url> distributed-log-pipeline
cd distributed-log-pipeline
```

### 1-2: Master ì„œë¹„ìŠ¤ ì‹œì‘
```bash
cd deploy
docker compose -f docker-compose.master.yml up -d
```

### 1-3: ìƒíƒœ í™•ì¸
```bash
docker compose -f docker-compose.master.yml ps
```

### 1-4: ì„œë¹„ìŠ¤ URL

| ì„œë¹„ìŠ¤ | URL |
|--------|-----|
| Kafka UI | http://192.168.55.114:8080 |
| HDFS NameNode | http://192.168.55.114:9870 |
| Spark Master | http://192.168.55.114:8082 |
| Airflow | http://192.168.55.114:8084 |
| Grafana | http://192.168.55.114:3000 |
| Prometheus | http://192.168.55.114:9090 |

---

## ğŸ“¦ Step 2: Worker ë…¸ë“œ ì„¤ì • (ë¦¬ëˆ…ìŠ¤ A, B)

### 2-1: ì˜ì¡´ì„± ì„¤ì¹˜ (ìµœì´ˆ 1íšŒ)
```bash
# ìŠ¤í¬ë¦½íŠ¸ ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰
curl -fsSL https://raw.githubusercontent.com/<your-repo>/main/scripts/setup-worker.sh | bash
```

ë˜ëŠ” ìˆ˜ë™ ì„¤ì¹˜:
```bash
# Docker ì„¤ì¹˜
curl -fsSL https://get.docker.com | sh
sudo usermod -aG docker $USER
newgrp docker

# Git ì„¤ì¹˜
sudo apt update && sudo apt install -y git
```

### 2-2: í”„ë¡œì íŠ¸ Clone
```bash
mkdir -p ~/project
cd ~/project
git clone <your-repo-url> distributed-log-pipeline
cd distributed-log-pipeline
```

### 2-3: Worker ì„œë¹„ìŠ¤ ì‹œì‘
```bash
cd deploy
docker compose -f docker-compose.worker.yml up -d
```

### 2-4: ìƒíƒœ í™•ì¸
```bash
docker compose -f docker-compose.worker.yml ps
```

---

## âœ… Step 3: ì—°ê²° í™•ì¸

### Masterì—ì„œ Worker ì—°ê²° í™•ì¸

#### HDFS DataNode í™•ì¸
```bash
# NameNode UIì—ì„œ í™•ì¸
http://192.168.55.114:9870
# Datanodes íƒ­ì—ì„œ 2ê°œ ë…¸ë“œ í™•ì¸
```

ë˜ëŠ” CLI:
```bash
docker exec namenode hdfs dfsadmin -report
```

#### Spark Worker í™•ì¸
```bash
# Spark Master UIì—ì„œ í™•ì¸
http://192.168.55.114:8082
# Workers ì„¹ì…˜ì—ì„œ 2ê°œ ì›Œì»¤ í™•ì¸
```

---

## ğŸš€ Step 4: ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸

### 4-1: Kafkaì— ë°ì´í„° ìŒ“ì´ëŠ”ì§€ í™•ì¸
```bash
# Kafka UI: http://192.168.55.114:8080
# logs.raw í† í”½ í™•ì¸
```

### 4-2: Spark Streaming Job ì‹¤í–‰
```bash
docker exec spark-master /spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 \
  /opt/spark-jobs/streaming/raw_to_hdfs.py
```

### 4-3: HDFSì— ë°ì´í„° ì €ì¥ í™•ì¸
```bash
docker exec namenode hdfs dfs -ls -R /data/logs/raw
```

### 4-4: Airflow DAG ì‹¤í–‰
```bash
# Airflow UI: http://192.168.55.114:8084
# manual_log_pipeline íŠ¸ë¦¬ê±°
```

---

## ğŸ›‘ ì„œë¹„ìŠ¤ ì¤‘ì§€

### Master
```bash
cd ~/project/distributed-log-pipeline/deploy
docker compose -f docker-compose.master.yml down
```

### Worker
```bash
cd ~/project/distributed-log-pipeline/deploy
docker compose -f docker-compose.worker.yml down
```

### ë³¼ë¥¨ê¹Œì§€ ì‚­ì œ (ë°ì´í„° ì´ˆê¸°í™”)
```bash
docker compose -f docker-compose.master.yml down -v
docker compose -f docker-compose.worker.yml down -v
```

---

## âš ï¸ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### Workerê°€ Masterì— ì—°ê²° ì•ˆ ë¨

1. **ë„¤íŠ¸ì›Œí¬ í™•ì¸**
```bash
ping 192.168.55.114
```

2. **ë°©í™”ë²½ í™•ì¸**
```bash
# Masterì—ì„œ í¬íŠ¸ ì—´ê¸°
sudo ufw allow 9000   # HDFS
sudo ufw allow 7077   # Spark
sudo ufw allow 9092   # Kafka
```

3. **Docker ë„¤íŠ¸ì›Œí¬ í™•ì¸**
```bash
docker network ls
```

### Spark Worker ì—°ê²° ì‹¤íŒ¨
```bash
# Workerì—ì„œ Spark Master ì ‘ê·¼ í™•ì¸
curl http://192.168.55.114:8082
```

### HDFS DataNode ì—°ê²° ì‹¤íŒ¨
```bash
# Workerì—ì„œ NameNode ì ‘ê·¼ í™•ì¸
curl http://192.168.55.114:9870
```
