services:
  # ================================
  # Kafka (KRaft 모드 - Zookeeper 없이)
  # ================================
  kafka:
    image: apache/kafka:3.7.0
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      - KAFKA_NODE_ID=1
      - KAFKA_PROCESS_ROLES=broker,controller
      - KAFKA_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://192.168.55.114:9092
      - KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CONTROLLER_QUORUM_VOTERS=1@localhost:9093
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1
      - KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1
      - KAFKA_AUTO_CREATE_TOPICS_ENABLE=true
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - pipeline-network

  # ================================
  # PostgreSQL (벤치마크용)
  # ================================
  postgres:
    image: postgres:15
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=logs
      - POSTGRES_USER=admin
      - POSTGRES_PASSWORD=admin123
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - pipeline-network

  # ================================
  # Java Backend (수집 서버)
  # ================================
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: backend
    ports:
      - "8081:8081"
    environment:
      - KAFKA_SERVERS=kafka:9092
      - POSTGRES_URL=jdbc:postgresql://postgres:5432/logs
      - POSTGRES_USER=admin
      - POSTGRES_PASSWORD=admin123
    depends_on:
      - kafka
      - postgres
    networks:
      - pipeline-network

  # ================================
  # Python Generator (데이터 생성기)
  # ================================
  generator:
    build:
      context: ./generator
      dockerfile: Dockerfile
    container_name: generator
    ports:
      - "8085:8000"
    environment:
      - GENERATOR_BACKEND_URL=http://backend:8081
      - GENERATOR_LOG_INTERVAL_SECONDS=5
      - GENERATOR_EVENT_INTERVAL_SECONDS=10
      - GENERATOR_BATCH_SIZE=100
    depends_on:
      - backend
    networks:
      - pipeline-network

  # ================================
  # Query API (조회 서버)
  # ================================
  query-api:
    build:
      context: ./api
      dockerfile: Dockerfile
    container_name: query-api
    ports:
      - "8000:8000"
    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=logs
      - POSTGRES_USER=admin
      - POSTGRES_PASSWORD=admin123
      - HDFS_URL=hdfs://namenode:9000
      - HDFS_PATH=hdfs://namenode:9000/data/logs/raw
      - SPARK_MASTER=local[*]
    depends_on:
      - postgres
      - namenode
    networks:
      - pipeline-network

  # ================================
  # Kafka UI (모니터링용)
  # ================================
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "8080:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092
    depends_on:
      - kafka
    networks:
      - pipeline-network

  # ================================
  # HDFS NameNode (마스터)
  # ================================
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    ports:
      - "9870:9870"
      - "9000:9000"
    environment:
      - CLUSTER_NAME=distributed-log-pipeline
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_replication=2
    volumes:
      - namenode_data:/hadoop/dfs/name
    networks:
      - pipeline-network

  # ================================
  # HDFS DataNode 1
  # ================================
  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    depends_on:
      - namenode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_replication=2
    volumes:
      - datanode1_data:/hadoop/dfs/data
    networks:
      - pipeline-network

  # ================================
  # HDFS DataNode 2
  # ================================
  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    depends_on:
      - namenode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_replication=2
    volumes:
      - datanode2_data:/hadoop/dfs/data
    networks:
      - pipeline-network

  # ================================
  # Spark Master
  # ================================
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    ports:
      - "8082:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
    volumes:
      - ./spark-jobs:/opt/spark-jobs
    networks:
      - pipeline-network

  # ================================
  # Spark Worker
  # ================================
  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker
    depends_on:
      - spark-master
    ports:
      - "8083:8081"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    volumes:
      - ./spark-jobs:/opt/spark-jobs
    networks:
      - pipeline-network

  # ================================
  # Airflow (스케줄러)
  # ================================
  airflow:
    image: apache/airflow:2.7.0-python3.10
    container_name: airflow
    user: root
    ports:
      - "8084:8080"
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__SECRET_KEY=mysecretkey123
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./spark-jobs:/opt/spark-jobs
      - /var/run/docker.sock:/var/run/docker.sock
    command: bash -c "airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true && (airflow webserver &) && airflow scheduler"
    networks:
      - pipeline-network

  # ================================
  # Prometheus (메트릭 수집)
  # ================================
  prometheus:
    image: prom/prometheus:v2.47.0
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    networks:
      - pipeline-network

  # ================================
  # Grafana (시각화)
  # ================================
  grafana:
    image: grafana/grafana:10.1.0
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus
    networks:
      - pipeline-network

volumes:
  kafka_data:
  postgres_data:
  namenode_data:
  datanode1_data:
  datanode2_data:
  prometheus_data:
  grafana_data:

networks:
  pipeline-network:
    driver: bridge
