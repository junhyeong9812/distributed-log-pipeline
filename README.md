# Distributed Log Pipeline

> ë¶„ì‚°ì²˜ë¦¬ ì‹œìŠ¤í…œ(Kubernetes, Hadoop, Spark)ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ì‹¤ìŠµ í”„ë¡œì íŠ¸

---

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

### ëª©ì 
Kubernetes, Hadoop, Sparkë¥¼ í™œìš©í•œ ë¶„ì‚° ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ì§ì ‘ êµ¬ì¶•í•˜ê³  ìš´ì˜í•´ë³´ë©° ë¶„ì‚°ì²˜ë¦¬ ì‹œìŠ¤í…œì˜ í•µì‹¬ ê°œë…ì„ í•™ìŠµí•©ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥
- ë°°ì¹˜ ê¸°ë°˜ ë¡œê·¸/ì´ë²¤íŠ¸ ë°ì´í„° ìë™ ìƒì„±
- ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ë° ë©”ì‹œì§€ í ì²˜ë¦¬
- ë¶„ì‚° ì €ì¥ì†Œ(HDFS)ì— ë°ì´í„° ì ì¬
- Sparkë¥¼ í™œìš©í•œ ì‹¤ì‹œê°„/ë°°ì¹˜ ë°ì´í„° ë¶„ì„
- Grafana ê¸°ë°˜ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ì „ì²´ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         System Architecture                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚       â”‚            ë¦¬ëˆ…ìŠ¤ ìš°ë¶„íˆ¬ - ë…¸íŠ¸ë¶ (Master)               â”‚
â”‚   Data          â”‚       â”‚                                         â”‚
â”‚   Generator     â”‚       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   Server        â”‚ â”€â”€â”€â”€â–¶ â”‚  â”‚   Java Backend (Spring Boot)      â”‚  â”‚
â”‚                 â”‚ HTTP  â”‚  â”‚   â€¢ REST API ìˆ˜ì§‘ ì„œë²„             â”‚  â”‚
â”‚  (Python)       â”‚       â”‚  â”‚   â€¢ Kafka Producer                â”‚  â”‚
â”‚  â€¢ FastAPI      â”‚       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â€¢ ë°°ì¹˜ ìŠ¤ì¼€ì¤„ëŸ¬ â”‚       â”‚                    â”‚                    â”‚
â”‚  â€¢ ë°ì´í„° ìƒì„±ê¸° â”‚       â”‚                    â–¼                    â”‚
â”‚                 â”‚       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚  â”‚   Kubernetes Control Plane        â”‚  â”‚
                          â”‚  â”‚   â€¢ API Server                    â”‚  â”‚
                          â”‚  â”‚   â€¢ Scheduler                     â”‚  â”‚
                          â”‚  â”‚   â€¢ etcd                          â”‚  â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                          â”‚                    â”‚                    â”‚
                          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                          â”‚  â”‚   Kafka (KRaft ëª¨ë“œ)               â”‚  â”‚
                          â”‚  â”‚   â€¢ ë©”ì‹œì§€ í                      â”‚  â”‚
                          â”‚  â”‚   â€¢ ì‹¤ì‹œê°„ ë°ì´í„° ë²„í¼ë§           â”‚  â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                          â”‚                    â”‚                    â”‚
                          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                          â”‚  â”‚   Spark Driver + Airflow          â”‚  â”‚
                          â”‚  â”‚   â€¢ ì‘ì—… ìŠ¤ì¼€ì¤„ë§                  â”‚  â”‚
                          â”‚  â”‚   â€¢ ë¶„ì„ Job ê´€ë¦¬                  â”‚  â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                          â”‚                    â”‚                    â”‚
                          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                          â”‚  â”‚   Monitoring Stack                â”‚  â”‚
                          â”‚  â”‚   â€¢ Grafana (ì‹œê°í™”)              â”‚  â”‚
                          â”‚  â”‚   â€¢ Prometheus (ë©”íŠ¸ë¦­ ìˆ˜ì§‘)       â”‚  â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚                                       â”‚
                          â–¼                                       â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   ë¦¬ëˆ…ìŠ¤ PC - A      â”‚             â”‚   ë¦¬ëˆ…ìŠ¤ PC - B      â”‚
               â”‚   (Worker Node 1)   â”‚             â”‚   (Worker Node 2)   â”‚
               â”‚                     â”‚             â”‚                     â”‚
               â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚             â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
               â”‚ â”‚ K8s Worker      â”‚ â”‚             â”‚ â”‚ K8s Worker      â”‚ â”‚
               â”‚ â”‚ (kubelet)       â”‚ â”‚             â”‚ â”‚ (kubelet)       â”‚ â”‚
               â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
               â”‚                     â”‚             â”‚                     â”‚
               â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚             â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
               â”‚ â”‚ HDFS DataNode   â”‚ â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ â”‚ HDFS DataNode   â”‚ â”‚
               â”‚ â”‚ (ë°ì´í„° ì €ì¥)    â”‚ â”‚  Replicationâ”‚ â”‚ (ë°ì´í„° ì €ì¥)    â”‚ â”‚
               â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
               â”‚                     â”‚             â”‚                     â”‚
               â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚             â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
               â”‚ â”‚ Spark Executor  â”‚ â”‚             â”‚ â”‚ Spark Executor  â”‚ â”‚
               â”‚ â”‚ (ë°ì´í„° ì²˜ë¦¬)    â”‚ â”‚             â”‚ â”‚ (ë°ì´í„° ì²˜ë¦¬)    â”‚ â”‚
               â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ë°ì´í„° í”Œë¡œìš°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Data Flow                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. ë°ì´í„° ìƒì„± (Python Generator)
   â”‚
   â”‚  ë°°ì¹˜ ìŠ¤ì¼€ì¤„ (ë§¤ Nì´ˆ/ë¶„ë§ˆë‹¤)
   â”‚  â€¢ ì‹œìŠ¤í…œ ë¡œê·¸ ìƒì„±
   â”‚  â€¢ ì‚¬ìš©ì í™œë™ ì´ë²¤íŠ¸ ìƒì„±
   â–¼
2. ë°ì´í„° ìˆ˜ì§‘ (Java Spring Boot)
   â”‚
   â”‚  REST APIë¡œ ìˆ˜ì‹ 
   â”‚  â€¢ ë°ì´í„° ê²€ì¦
   â”‚  â€¢ Kafkaë¡œ ë°œí–‰
   â–¼
3. ë©”ì‹œì§€ í (Kafka)
   â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                      â”‚                      â”‚
   â–¼                      â–¼                      â–¼
4a. ì‹¤ì‹œê°„ ì²˜ë¦¬       4b. ë°°ì¹˜ ì €ì¥          4c. HDFS ì ì¬
    (Spark Streaming)     (Kafka â†’ HDFS)         (ì›ë³¸ ë³´ê´€)
    â”‚                      â”‚                      â”‚
    â”‚ â€¢ 5ì´ˆ ìœˆë„ìš° ì§‘ê³„     â”‚ â€¢ ì‹œê°„ë³„ íŒŒí‹°ì…˜      â”‚ â€¢ ì¥ê¸° ë³´ê´€
    â”‚ â€¢ ì´ìƒ íƒì§€          â”‚ â€¢ ì••ì¶• ì €ì¥          â”‚ â€¢ ì¬ì²˜ë¦¬ìš©
    â–¼                      â–¼                      â–¼
5. ê²°ê³¼ ì €ì¥
   â”‚
   â”œâ”€â”€ Prometheus (ë©”íŠ¸ë¦­)
   â”œâ”€â”€ HDFS (ë¶„ì„ ê²°ê³¼)
   â””â”€â”€ PostgreSQL (ì§‘ê³„ ë°ì´í„°)
   â”‚
   â–¼
6. ì‹œê°í™” (Grafana)
   â”‚
   â€¢ ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ
   â€¢ ì•Œë¦¼ ì„¤ì •
```

---

## ğŸ–¥ï¸ PCë³„ ì—­í•  ë° êµ¬ì„±

### ì—­í•  ë¶„ë°°

| PC | ì—­í•  | ì£¼ìš” ì»´í¬ë„ŒíŠ¸ | ê¶Œì¥ ì‚¬ì–‘ |
|-----|------|-------------|----------|
| **ë…¸íŠ¸ë¶** | Master + Generator | K8s Control Plane, Java Backend, Kafka, Spark Driver, Monitoring | 8GB+ RAM, 4+ Core |
| **ë¦¬ëˆ…ìŠ¤ A** | Worker Node 1 | K8s Worker, HDFS DataNode, Spark Executor | 4GB+ RAM, 2+ Core |
| **ë¦¬ëˆ…ìŠ¤ B** | Worker Node 2 | K8s Worker, HDFS DataNode, Spark Executor | 4GB+ RAM, 2+ Core |

### ë„¤íŠ¸ì›Œí¬ êµ¬ì„±

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Network Topology                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         192.168.x.0/24 (ì˜ˆì‹œ)
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚             â”‚
    â–¼             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚Master â”‚    â”‚Worker1â”‚    â”‚Worker2â”‚
â”‚  .10  â”‚    â”‚  .11  â”‚    â”‚  .12  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”˜

í•„ìš” í¬íŠ¸:
â€¢ 6443  : Kubernetes API
â€¢ 9092  : Kafka
â€¢ 9870  : HDFS NameNode Web UI
â€¢ 8080  : Spark Master Web UI
â€¢ 3000  : Grafana
â€¢ 8081  : Java Backend API
```

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
distributed-log-pipeline/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml              # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©
â”œâ”€â”€ .env.example                    # í™˜ê²½ë³€ìˆ˜ í…œí”Œë¦¿
â”‚
â”œâ”€â”€ docs/                           # ë¬¸ì„œ
â”‚   â”œâ”€â”€ SETUP_MASTER.md            # ë§ˆìŠ¤í„° ë…¸ë“œ ì„¤ì • ê°€ì´ë“œ
â”‚   â”œâ”€â”€ SETUP_WORKER.md            # ì›Œì»¤ ë…¸ë“œ ì„¤ì • ê°€ì´ë“œ
â”‚   â”œâ”€â”€ TROUBLESHOOTING.md         # ë¬¸ì œ í•´ê²° ê°€ì´ë“œ
â”‚   â””â”€â”€ ARCHITECTURE.md            # ìƒì„¸ ì•„í‚¤í…ì²˜ ë¬¸ì„œ
â”‚
â”œâ”€â”€ generator/                      # ë°ì´í„° ìƒì„±ê¸° (Python)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                # FastAPI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”‚   â”‚   â”œâ”€â”€ scheduler.py           # ë°°ì¹˜ ìŠ¤ì¼€ì¤„ëŸ¬
â”‚   â”‚   â”œâ”€â”€ generators/
â”‚   â”‚   â”‚   â”œâ”€â”€ log_generator.py   # ë¡œê·¸ ë°ì´í„° ìƒì„±
â”‚   â”‚   â”‚   â””â”€â”€ event_generator.py # ì´ë²¤íŠ¸ ë°ì´í„° ìƒì„±
â”‚   â”‚   â””â”€â”€ config.py              # ì„¤ì •
â”‚   â””â”€â”€ tests/
â”‚
â”œâ”€â”€ backend/                        # ìˆ˜ì§‘ ì„œë²„ (Java Spring Boot)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ pom.xml
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ main/
â”‚   â”‚       â”œâ”€â”€ java/
â”‚   â”‚       â”‚   â””â”€â”€ com/pipeline/
â”‚   â”‚       â”‚       â”œâ”€â”€ PipelineApplication.java
â”‚   â”‚       â”‚       â”œâ”€â”€ controller/
â”‚   â”‚       â”‚       â”‚   â””â”€â”€ CollectorController.java
â”‚   â”‚       â”‚       â”œâ”€â”€ service/
â”‚   â”‚       â”‚       â”‚   â””â”€â”€ KafkaProducerService.java
â”‚   â”‚       â”‚       â”œâ”€â”€ model/
â”‚   â”‚       â”‚       â”‚   â”œâ”€â”€ LogEvent.java
â”‚   â”‚       â”‚       â”‚   â””â”€â”€ ActivityEvent.java
â”‚   â”‚       â”‚       â””â”€â”€ config/
â”‚   â”‚       â”‚           â””â”€â”€ KafkaConfig.java
â”‚   â”‚       â””â”€â”€ resources/
â”‚   â”‚           â””â”€â”€ application.yml
â”‚   â””â”€â”€ tests/
â”‚
â”œâ”€â”€ spark-jobs/                     # Spark ì‘ì—… (PySpark)
â”‚   â”œâ”€â”€ streaming/
â”‚   â”‚   â”œâ”€â”€ realtime_aggregator.py # ì‹¤ì‹œê°„ ì§‘ê³„
â”‚   â”‚   â””â”€â”€ anomaly_detector.py    # ì´ìƒ íƒì§€
â”‚   â”œâ”€â”€ batch/
â”‚   â”‚   â”œâ”€â”€ daily_report.py        # ì¼ë³„ ë¦¬í¬íŠ¸
â”‚   â”‚   â””â”€â”€ weekly_analysis.py     # ì£¼ê°„ ë¶„ì„
â”‚   â””â”€â”€ common/
â”‚       â””â”€â”€ spark_utils.py         # ê³µí†µ ìœ í‹¸
â”‚
â”œâ”€â”€ airflow/                        # ì›Œí¬í”Œë¡œìš° ìŠ¤ì¼€ì¤„ë§
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ daily_pipeline.py
â”‚   â”‚   â””â”€â”€ weekly_pipeline.py
â”‚   â””â”€â”€ plugins/
â”‚
â”œâ”€â”€ kubernetes/                     # K8s ë§¤ë‹ˆí˜ìŠ¤íŠ¸
â”‚   â”œâ”€â”€ namespace.yaml
â”‚   â”œâ”€â”€ master/                    # ë§ˆìŠ¤í„° ë…¸ë“œìš©
â”‚   â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â”‚   â”œâ”€â”€ zookeeper.yaml
â”‚   â”‚   â”‚   â””â”€â”€ kafka.yaml
â”‚   â”‚   â”œâ”€â”€ spark/
â”‚   â”‚   â”‚   â””â”€â”€ spark-master.yaml
â”‚   â”‚   â””â”€â”€ monitoring/
â”‚   â”‚       â”œâ”€â”€ prometheus.yaml
â”‚   â”‚       â””â”€â”€ grafana.yaml
â”‚   â”œâ”€â”€ worker/                    # ì›Œì»¤ ë…¸ë“œìš©
â”‚   â”‚   â”œâ”€â”€ hdfs/
â”‚   â”‚   â”‚   â””â”€â”€ datanode.yaml
â”‚   â”‚   â””â”€â”€ spark/
â”‚   â”‚       â””â”€â”€ spark-worker.yaml
â”‚   â””â”€â”€ common/                    # ê³µí†µ
â”‚       â”œâ”€â”€ configmaps.yaml
â”‚       â””â”€â”€ secrets.yaml
â”‚
â”œâ”€â”€ hadoop/                         # Hadoop ì„¤ì •
â”‚   â”œâ”€â”€ core-site.xml
â”‚   â”œâ”€â”€ hdfs-site.xml
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ start-namenode.sh
â”‚       â””â”€â”€ start-datanode.sh
â”‚
â”œâ”€â”€ monitoring/                     # ëª¨ë‹ˆí„°ë§ ì„¤ì •
â”‚   â”œâ”€â”€ grafana/
â”‚   â”‚   â”œâ”€â”€ provisioning/
â”‚   â”‚   â””â”€â”€ dashboards/
â”‚   â”‚       â”œâ”€â”€ overview.json      # ì „ì²´ í˜„í™© ëŒ€ì‹œë³´ë“œ
â”‚   â”‚       â”œâ”€â”€ kafka.json         # Kafka ëª¨ë‹ˆí„°ë§
â”‚   â”‚       â””â”€â”€ spark.json         # Spark ëª¨ë‹ˆí„°ë§
â”‚   â””â”€â”€ prometheus/
â”‚       â””â”€â”€ prometheus.yml
â”‚
â””â”€â”€ scripts/                        # ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸
    â”œâ”€â”€ setup-master.sh            # ë§ˆìŠ¤í„° ì´ˆê¸° ì„¤ì •
    â”œâ”€â”€ setup-worker.sh            # ì›Œì»¤ ì´ˆê¸° ì„¤ì •
    â”œâ”€â”€ deploy-all.sh              # ì „ì²´ ë°°í¬
    â””â”€â”€ cleanup.sh                 # ì •ë¦¬
```

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ì‚¬ì „ ìš”êµ¬ì‚¬í•­

**ëª¨ë“  PC ê³µí†µ:**
```bash
# Docker ì„¤ì¹˜
curl -fsSL https://get.docker.com | sh
sudo usermod -aG docker $USER

# kubectl ì„¤ì¹˜
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
```

### Step 1: í”„ë¡œì íŠ¸ í´ë¡  (ëª¨ë“  PC)

```bash
git clone https://github.com/your-repo/distributed-log-pipeline.git
cd distributed-log-pipeline
cp .env.example .env
# .env íŒŒì¼ì—ì„œ ê° PCì˜ IP ì£¼ì†Œ ì„¤ì •
```

### Step 2: Master ë…¸ë“œ ì„¤ì • (ë…¸íŠ¸ë¶)

```bash
# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export NODE_ROLE=master
export MASTER_IP=192.168.x.10  # ì‹¤ì œ IPë¡œ ë³€ê²½

# Kubernetes í´ëŸ¬ìŠ¤í„° ì´ˆê¸°í™”
./scripts/setup-master.sh

# ë§ˆìŠ¤í„° ì»´í¬ë„ŒíŠ¸ ë°°í¬
kubectl apply -f kubernetes/master/

# Java Backend ì‹¤í–‰
cd backend
./mvnw spring-boot:run

# ë°ì´í„° ìƒì„±ê¸° ì‹¤í–‰ (ë³„ë„ í„°ë¯¸ë„)
cd generator
pip install -r requirements.txt
python -m app.main
```

### Step 3: Worker ë…¸ë“œ ì„¤ì • (ë¦¬ëˆ…ìŠ¤ A, B)

```bash
# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export NODE_ROLE=worker
export MASTER_IP=192.168.x.10  # ë§ˆìŠ¤í„° IP

# í´ëŸ¬ìŠ¤í„° ì¡°ì¸
./scripts/setup-worker.sh

# ì›Œì»¤ ì»´í¬ë„ŒíŠ¸ ë°°í¬
kubectl apply -f kubernetes/worker/
```

### Step 4: ë™ì‘ í™•ì¸

```bash
# í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
kubectl get nodes
kubectl get pods -A

# HDFS ìƒíƒœ í™•ì¸
hdfs dfsadmin -report

# Kafka í† í”½ í™•ì¸
kafka-topics.sh --list --bootstrap-server localhost:9092

# Grafana ì ‘ì†
# ë¸Œë¼ìš°ì €ì—ì„œ http://<MASTER_IP>:3000 ì ‘ì†
# ê¸°ë³¸ ê³„ì •: admin / admin
```

---

## ğŸ“Š ìƒì„± ë°ì´í„° í˜•ì‹

### ì‹œìŠ¤í…œ ë¡œê·¸

```json
{
  "timestamp": "2024-01-15T10:30:00.123Z",
  "level": "INFO",
  "service": "api-gateway",
  "host": "worker-1",
  "message": "Request processed successfully",
  "metadata": {
    "request_id": "uuid-1234",
    "response_time_ms": 45,
    "status_code": 200,
    "endpoint": "/api/users",
    "method": "GET"
  }
}
```

### ì‚¬ìš©ì í™œë™ ì´ë²¤íŠ¸

```json
{
  "event_id": "evt-uuid-5678",
  "timestamp": "2024-01-15T10:30:05.456Z",
  "user_id": "user_12345",
  "session_id": "sess_abcde",
  "event_type": "CLICK",
  "event_data": {
    "page": "/products/123",
    "element": "add_to_cart_button",
    "position": {"x": 450, "y": 320}
  },
  "device": {
    "type": "mobile",
    "os": "iOS",
    "browser": "Safari"
  }
}
```

---

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

### ì œê³µë˜ëŠ” ëŒ€ì‹œë³´ë“œ

| ëŒ€ì‹œë³´ë“œ | ì„¤ëª… | ì£¼ìš” ë©”íŠ¸ë¦­ |
|---------|------|------------|
| **Overview** | ì‹œìŠ¤í…œ ì „ì²´ í˜„í™© | ë…¸ë“œ ìƒíƒœ, ì²˜ë¦¬ëŸ‰, ì—ëŸ¬ìœ¨ |
| **Kafka** | ë©”ì‹œì§€ í ëª¨ë‹ˆí„°ë§ | í† í”½ë³„ ì²˜ë¦¬ëŸ‰, ì§€ì—°ì‹œê°„, ì»¨ìŠˆë¨¸ ë™ |
| **Spark** | ì²˜ë¦¬ ì‘ì—… ëª¨ë‹ˆí„°ë§ | ì‘ì—… ìƒíƒœ, ì²˜ë¦¬ ì‹œê°„, ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ |
| **HDFS** | ì €ì¥ì†Œ ëª¨ë‹ˆí„°ë§ | ìš©ëŸ‰, ë³µì œ ìƒíƒœ, ë…¸ë“œ ìƒíƒœ |

### ì•Œë¦¼ ì„¤ì •

```yaml
# ê¸°ë³¸ ì œê³µ ì•Œë¦¼ ê·œì¹™
alerts:
  - name: HighErrorRate
    condition: error_rate > 5%
    duration: 5m
    
  - name: KafkaLag
    condition: consumer_lag > 10000
    duration: 10m
    
  - name: SparkJobFailed
    condition: job_status == "FAILED"
```

---

## ğŸ”§ ì£¼ìš” ì„¤ì •

### ë°°ì¹˜ ìŠ¤ì¼€ì¤„ ì„¤ì • (generator/app/config.py)

```python
BATCH_CONFIG = {
    "log_interval_seconds": 5,      # ë¡œê·¸ ìƒì„± ì£¼ê¸°
    "event_interval_seconds": 10,   # ì´ë²¤íŠ¸ ìƒì„± ì£¼ê¸°
    "batch_size": 100,              # ë°°ì¹˜ë‹¹ ë°ì´í„° ìˆ˜
    "target_backend_url": "http://master:8081/api/collect"
}
```

### Kafka í† í”½ ì„¤ì •

| í† í”½ | íŒŒí‹°ì…˜ | ë³µì œ ê³„ìˆ˜ | ë³´ì¡´ ê¸°ê°„ |
|------|--------|----------|----------|
| logs.raw | 3 | 2 | 7ì¼ |
| events.activity | 3 | 2 | 7ì¼ |
| alerts.realtime | 1 | 2 | 1ì¼ |

---

## ğŸ› ï¸ ê°œë°œ ê°€ì´ë“œ

### ë¡œì»¬ í…ŒìŠ¤íŠ¸ (Docker Compose)

```bash
# ì „ì²´ ìŠ¤íƒ ë¡œì»¬ ì‹¤í–‰ (ë‹¨ì¼ PC í…ŒìŠ¤íŠ¸ìš©)
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f

# ì¢…ë£Œ
docker-compose down -v
```

### ìƒˆë¡œìš´ Spark Job ì¶”ê°€

```python
# spark-jobs/batch/my_new_job.py

from pyspark.sql import SparkSession
from common.spark_utils import get_spark_session, read_from_hdfs

def main():
    spark = get_spark_session("MyNewJob")
    
    # HDFSì—ì„œ ë°ì´í„° ì½ê¸°
    df = read_from_hdfs(spark, "/data/logs/2024/01/15")
    
    # ì²˜ë¦¬ ë¡œì§
    result = df.groupBy("level").count()
    
    # ê²°ê³¼ ì €ì¥
    result.write.mode("overwrite").parquet("/data/results/my_job")

if __name__ == "__main__":
    main()
```

---

## ğŸ“š í•™ìŠµ ë¡œë“œë§µ

### Week 1-2: í™˜ê²½ êµ¬ì¶•
- [ ] 3ëŒ€ PC ë„¤íŠ¸ì›Œí¬ êµ¬ì„±
- [ ] Docker, kubectl ì„¤ì¹˜
- [ ] Kubernetes í´ëŸ¬ìŠ¤í„° êµ¬ì¶•
- [ ] ê¸°ë³¸ í†µì‹  í…ŒìŠ¤íŠ¸

### Week 3: ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê¸°ì´ˆ
- [ ] Kafka ì„¤ì¹˜ ë° í† í”½ ìƒì„±
- [ ] Java Backend ê°œë°œ
- [ ] Python Generator ê°œë°œ
- [ ] ê¸°ë³¸ ë°ì´í„° í”Œë¡œìš° í™•ì¸

### Week 4: HDFS ì—°ë™
- [ ] HDFS í´ëŸ¬ìŠ¤í„° êµ¬ì„±
- [ ] Kafka â†’ HDFS ì—°ë™
- [ ] ë°ì´í„° ì ì¬ í™•ì¸

### Week 5: Spark ì²˜ë¦¬
- [ ] Spark Streaming ì‘ì—… ê°œë°œ
- [ ] Batch ì‘ì—… ê°œë°œ
- [ ] Airflow ìŠ¤ì¼€ì¤„ë§

### Week 6: ëª¨ë‹ˆí„°ë§
- [ ] Prometheus + Grafana êµ¬ì„±
- [ ] ëŒ€ì‹œë³´ë“œ êµ¬ì„±
- [ ] ì•Œë¦¼ ì„¤ì •
- [ ] ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸

---

## â“ FAQ

### Q: ë‹¨ì¼ PCì—ì„œë„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆë‚˜ìš”?
A: ë„¤, `docker-compose up`ìœ¼ë¡œ ë¡œì»¬ì—ì„œ ì „ì²´ ìŠ¤íƒì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Q: ìµœì†Œ ì‚¬ì–‘ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?
A: Master 8GB RAM, Worker ê° 4GB RAMì„ ê¶Œì¥í•©ë‹ˆë‹¤. ë” ë‚®ì€ ì‚¬ì–‘ì—ì„œë„ ë™ì‘í•˜ì§€ë§Œ ì„±ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Q: Windowsì—ì„œë„ ê°€ëŠ¥í•œê°€ìš”?
A: WSL2 + Docker Desktop í™˜ê²½ì—ì„œ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë‹¨, ë„¤íŠ¸ì›Œí¬ ì„¤ì •ì´ ë³µì¡í•´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
---

## ğŸ”§ ê¸°ìˆ  ìƒì„¸ ì„¤ëª…

### PySparkëŠ” ì–´ë–»ê²Œ ë™ì‘í•˜ë‚˜ìš”?

SparkëŠ” Java/Scalaë¡œ ë§Œë“¤ì–´ì¡Œì§€ë§Œ, Pythonìœ¼ë¡œ ì‘ì„±í•œ ì½”ë“œë„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### ë™ì‘ êµ¬ì¡°
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PySpark êµ¬ì¡°                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚   Python ì½”ë“œ (ìš°ë¦¬ê°€ ì‘ì„±)                               â”‚
â”‚        â†“                                                 â”‚
â”‚   Py4J (Python â†” Java ë¸Œë¦¿ì§€)                            â”‚
â”‚        â†“                                                 â”‚
â”‚   Spark Core (Java/Scala - ì‹¤ì œ ì‹¤í–‰)                    â”‚
â”‚        â†“                                                 â”‚
â”‚   JVMì—ì„œ ë¶„ì‚° ì²˜ë¦¬                                       â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ì‹¤ì œ ì‹¤í–‰ íë¦„
```python
# ìš°ë¦¬ê°€ ì‘ì„±í•œ Python ì½”ë“œ
df.filter(col("level") == "ERROR").count()
```

ìœ„ ì½”ë“œê°€ ì‹¤í–‰ë˜ë©´:

1. **Python â†’ Py4J**: Python ì½”ë“œê°€ Py4J ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ Javaë¡œ ì „ë‹¬
2. **ì‹¤í–‰ ê³„íš ìƒì„±**: Spark(Java)ê°€ ìµœì í™”ëœ ì‹¤í–‰ ê³„íš ìƒì„±
3. **JVM ë¶„ì‚° ì²˜ë¦¬**: ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ëŠ” JVMì—ì„œ ë¶„ì‚° ì‹¤í–‰
4. **ê²°ê³¼ ë°˜í™˜**: ì²˜ë¦¬ ê²°ê³¼ë§Œ Pythonìœ¼ë¡œ ë°˜í™˜

#### ì„±ëŠ¥ ë¹„êµ

| êµ¬ë¶„ | PySpark | Scala/Java Spark |
|------|---------|------------------|
| **DataFrame API** | ê±°ì˜ ë™ì¼ | ê¸°ì¤€ |
| **RDD ì—°ì‚°** | ì•½ê°„ ëŠë¦¼ | ë¹ ë¦„ |
| **UDF (ì‚¬ìš©ì í•¨ìˆ˜)** | ëŠë¦¼ (ì§ë ¬í™” ì˜¤ë²„í—¤ë“œ) | ë¹ ë¦„ |
| **ê°œë°œ ì†ë„** | ë¹ ë¦„ | ëŠë¦¼ |

#### ì–¸ì œ ë¬´ì—‡ì„ ì„ íƒí• ê¹Œ?
```
DataFrame/SQL ìœ„ì£¼ ì‘ì—…     â†’ PySpark ì¶”ì²œ (ì¶©ë¶„í•œ ì„±ëŠ¥)
ë³µì¡í•œ ì»¤ìŠ¤í…€ ë¡œì§/UDF ë§ìŒ  â†’ Scala ê³ ë ¤
ML íŒŒì´í”„ë¼ì¸               â†’ PySpark (MLlib + Python ìƒíƒœê³„)
```

ë³¸ í”„ë¡œì íŠ¸ëŠ” DataFrame API ìœ„ì£¼ë¡œ ì‘ì—…í•˜ë¯€ë¡œ **PySparkë¡œ ì¶©ë¶„**í•©ë‹ˆë‹¤.

---

### Kafka í† í”½ íŒŒí‹°ì…˜ ì„¤ì • ì£¼ì˜ì‚¬í•­

#### ë¬¸ì œ ìƒí™©

Kafka í† í”½ì´ ì˜ë„í•œ íŒŒí‹°ì…˜ ìˆ˜ì™€ ë‹¤ë¥´ê²Œ ìƒì„±ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```
ì˜ˆìƒ: íŒŒí‹°ì…˜ 3ê°œë¡œ ë¶„ì‚° ì €ì¥
ì‹¤ì œ: íŒŒí‹°ì…˜ 1ê°œì— ëª¨ë“  ë°ì´í„° ì €ì¥
```

#### ì›ì¸
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ì„œë¹„ìŠ¤ ì‹œì‘ ìˆœì„œ ë¬¸ì œ                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Case 1: Generatorê°€ ë¨¼ì € ë°ì´í„° ì „ì†¡                    â”‚
â”‚  â†’ Kafkaê°€ í† í”½ ìë™ ìƒì„± (AUTO_CREATE_TOPICS=true)      â”‚
â”‚  â†’ ê¸°ë³¸ê°’ íŒŒí‹°ì…˜ 1ê°œë¡œ ìƒì„±                              â”‚
â”‚  â†’ Java Backendì˜ KafkaConfig ì„¤ì • ë¬´ì‹œë¨               â”‚
â”‚                                                          â”‚
â”‚  Case 2: Java Backendê°€ ë¨¼ì € ì—°ê²°                        â”‚
â”‚  â†’ KafkaConfig.javaì˜ NewTopic ì„¤ì • ì ìš©                â”‚
â”‚  â†’ íŒŒí‹°ì…˜ 3ê°œë¡œ í† í”½ ìƒì„±                                â”‚
â”‚  â†’ ë°ì´í„°ê°€ ì •ìƒì ìœ¼ë¡œ ë¶„ì‚° ì €ì¥                         â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### í•´ê²° ë°©ë²•

**ë°©ë²• 1: ì„œë¹„ìŠ¤ ì‹œì‘ ìˆœì„œ ë³´ì¥**
```yaml
# docker-compose.yml
generator:
  depends_on:
    backend:
      condition: service_healthy  # Backendê°€ healthy ìƒíƒœì¼ ë•Œë§Œ ì‹œì‘
```

**ë°©ë²• 2: í† í”½ ì‚¬ì „ ìƒì„±**
```bash
# Kafka ì»¨í…Œì´ë„ˆì—ì„œ ì§ì ‘ í† í”½ ìƒì„±
docker exec kafka /opt/kafka/bin/kafka-topics.sh \
  --create \
  --topic logs.raw \
  --partitions 3 \
  --replication-factor 1 \
  --bootstrap-server localhost:9092
```

**ë°©ë²• 3: ë³¼ë¥¨ ì´ˆê¸°í™” í›„ ì¬ì‹œì‘**
```bash
docker compose down -v  # ë³¼ë¥¨ê¹Œì§€ ì‚­ì œ
docker compose up -d    # ìƒˆë¡œ ì‹œì‘
```

#### íŒŒí‹°ì…˜ ë¶„ë°° ì›ë¦¬
```java
// KafkaProducerService.java
String key = logEvent.getService();  // key = service ì´ë¦„
kafkaTemplate.send(logsTopic, key, logEvent);
```

| Key (Service) | íŒŒí‹°ì…˜ í• ë‹¹ (hash % íŒŒí‹°ì…˜ìˆ˜) |
|---------------|------------------------------|
| api-gateway | 0, 1, ë˜ëŠ” 2 |
| user-service | 0, 1, ë˜ëŠ” 2 |
| order-service | 0, 1, ë˜ëŠ” 2 |

**ê°™ì€ keyëŠ” í•­ìƒ ê°™ì€ íŒŒí‹°ì…˜**ìœ¼ë¡œ ì „ì†¡ë˜ì–´ ë©”ì‹œì§€ ìˆœì„œê°€ ë³´ì¥ë©ë‹ˆë‹¤.

---

### Spark Streaming íŒŒí‹°ì…˜ ì €ì¥ ë¬¸ì œ í•´ê²°

#### ë¬¸ì œ í˜„ìƒ

HDFSì— ë°ì´í„°ê°€ ì €ì¥ë  ë•Œ íŒŒí‹°ì…˜ì´ ì œëŒ€ë¡œ ìƒì„±ë˜ì§€ ì•ŠëŠ” ë¬¸ì œ:
```
# ì˜ˆìƒí•œ ê²°ê³¼
/data/logs/raw/year=2026/month=1/day=11/hour=19/

# ì‹¤ì œ ê²°ê³¼
/data/logs/raw/year=__HIVE_DEFAULT_PARTITION__/month=__HIVE_DEFAULT_PARTITION__/...
```

#### ì›ì¸

Generatorê°€ ë³´ë‚´ëŠ” timestamp í˜•ì‹ê³¼ Sparkì—ì„œ íŒŒì‹±í•˜ëŠ” í˜•ì‹ì´ ë¶ˆì¼ì¹˜:
```python
# Generatorê°€ ë³´ë‚´ëŠ” í˜•ì‹ (Unix timestamp)
{
    "timestamp": 1768123166.291045000,  # epoch seconds
    ...
}

# ì²˜ìŒ Spark ì½”ë“œ (ISO í˜•ì‹ìœ¼ë¡œ íŒŒì‹± ì‹œë„)
.withColumn("event_time", to_timestamp(col("timestamp"), "yyyy-MM-dd'T'HH:mm:ss.SSSXXX"))
# â†’ íŒŒì‹± ì‹¤íŒ¨ â†’ null â†’ __HIVE_DEFAULT_PARTITION__
```

#### í•´ê²°

Unix timestampë¥¼ ì˜¬ë°”ë¥´ê²Œ íŒŒì‹±í•˜ë„ë¡ ìˆ˜ì •:
```python
# ìˆ˜ì • ì „ (ISO í˜•ì‹)
.withColumn("event_time", to_timestamp(col("timestamp"), "yyyy-MM-dd'T'HH:mm:ss.SSSXXX"))

# ìˆ˜ì • í›„ (Unix timestamp)
.withColumn("event_time", from_unixtime(col("timestamp")).cast("timestamp"))
```

ë˜í•œ ìŠ¤í‚¤ë§ˆì—ì„œ timestamp íƒ€ì…ë„ ë³€ê²½:
```python
# ìˆ˜ì • ì „
StructField("timestamp", StringType(), True)

# ìˆ˜ì • í›„
StructField("timestamp", DoubleType(), True)
```

#### ë””ë²„ê¹… ë°©ë²•
```bash
# 1. Generatorê°€ ë³´ë‚´ëŠ” ì‹¤ì œ ë°ì´í„° í˜•ì‹ í™•ì¸
curl -s http://<MASTER_IP>:8000/sample/log

# 2. Kafka UIì—ì„œ ë©”ì‹œì§€ ì§ì ‘ í™•ì¸
# http://<MASTER_IP>:8080 â†’ Topics â†’ logs.raw â†’ Messages
```

---

### Spark Job ë¦¬ì†ŒìŠ¤ ì ìœ  ë¬¸ì œ

#### ë¬¸ì œ í˜„ìƒ

Spark Job ì¬ì‹¤í–‰ ì‹œ ë¦¬ì†ŒìŠ¤ë¥¼ í• ë‹¹ë°›ì§€ ëª»í•˜ëŠ” ê²½ê³ :
```
WARN TaskSchedulerImpl: Initial job has not accepted any resources; 
check your cluster UI to ensure that workers are registered and have sufficient resources
```

#### ì›ì¸

ì´ì „ Spark Jobì´ ë¹„ì •ìƒ ì¢…ë£Œë˜ë©´ì„œ Workerì˜ ë¦¬ì†ŒìŠ¤ë¥¼ ê³„ì† ì ìœ í•˜ê³  ìˆìŒ.
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ë¦¬ì†ŒìŠ¤ ì ìœ  ìƒíƒœ                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  ì´ì „ Job (ì¢…ë£Œë¨)     í˜„ì¬ Job (ëŒ€ê¸°ì¤‘)                  â”‚
â”‚  â””â”€ Worker ë¦¬ì†ŒìŠ¤ ì ìœ   â””â”€ ë¦¬ì†ŒìŠ¤ í• ë‹¹ ëŒ€ê¸°               â”‚
â”‚     (24 cores, 1GB)       (í• ë‹¹ ë¶ˆê°€)                    â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### í•´ê²° ë°©ë²•

**ë°©ë²• 1: Spark ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘**
```bash
docker compose restart spark-master spark-worker
```

**ë°©ë²• 2: Spark Master UIì—ì„œ ì§ì ‘ ì¢…ë£Œ**
```
1. http://<MASTER_IP>:8082 ì ‘ì†
2. Running Applicationsì—ì„œ (kill) í´ë¦­
3. ìƒˆ Job ì‹¤í–‰
```

**ë°©ë²• 3: ì „ì²´ í´ëŸ¬ìŠ¤í„° ì¬ì‹œì‘**
```bash
docker compose down
docker compose up -d
```

#### ì˜ˆë°© ë°©ë²•

Spark Job ì¢…ë£Œ ì‹œ í•­ìƒ `Ctrl+C`ë¡œ ì •ìƒ ì¢…ë£Œí•˜ê³ , Spark UIì—ì„œ Applicationì´ Completedë¡œ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸.

---

### Airflow ì„¤ì • ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

#### Airflow ê¸°ë³¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì–¸ì–´** | Python |
| **ê¸°ë³¸ DB** | SQLite (ê°œë°œìš©) |
| **ìš´ì˜ DB** | PostgreSQL, MySQL (ê¶Œì¥) |

#### ë¬¸ì œ 1: DB ì´ˆê¸°í™” ì‹¤íŒ¨

**ì¦ìƒ:**
```
ERROR: You need to initialize the database. Please run `airflow db init`.
```

**ì›ì¸:**
- ì´ì „ ì‹¤í–‰ì—ì„œ DBê°€ ë¶ˆì™„ì „í•˜ê²Œ ìƒì„±ë¨
- `docker compose down`ì€ ë³¼ë¥¨ì„ ìœ ì§€í•˜ë¯€ë¡œ ë¶ˆì™„ì „í•œ DBê°€ ë‚¨ì•„ìˆìŒ

**í•´ê²°:**
```bash
# ë³¼ë¥¨ê¹Œì§€ ì‚­ì œí•´ì„œ ê¹¨ë—í•˜ê²Œ ì‹œì‘
docker compose down -v
docker compose up -d
```

#### ë¬¸ì œ 2: SQLite + LocalExecutor í˜¸í™˜ ë¶ˆê°€

**ì¦ìƒ:**
```
airflow.exceptions.AirflowConfigException: error: cannot use SQLite with the LocalExecutor
```

**ì›ì¸:**
- SQLiteëŠ” ë™ì‹œ ì“°ê¸°ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
- LocalExecutorëŠ” ë³‘ë ¬ ì‹¤í–‰ì„ ìœ„í•´ ë™ì‹œ DB ì ‘ê·¼ í•„ìš”
- ë”°ë¼ì„œ SQLite + LocalExecutor ì¡°í•©ì€ ë¶ˆê°€

**Executor ì¢…ë¥˜:**

| Executor | DB ìš”êµ¬ì‚¬í•­ | íŠ¹ì§• |
|----------|------------|------|
| **SequentialExecutor** | SQLite ê°€ëŠ¥ | ìˆœì°¨ ì‹¤í–‰, ê°œë°œìš© |
| **LocalExecutor** | PostgreSQL/MySQL í•„ìš” | ë³‘ë ¬ ì‹¤í–‰ |
| **CeleryExecutor** | PostgreSQL/MySQL + Redis | ë¶„ì‚° ì‹¤í–‰ |
| **KubernetesExecutor** | PostgreSQL/MySQL | K8s Podìœ¼ë¡œ ì‹¤í–‰ |

**í•´ê²°:**
```yaml
# docker-compose.yml
environment:
  # ìˆ˜ì • ì „ (ì—ëŸ¬ ë°œìƒ)
  - AIRFLOW__CORE__EXECUTOR=LocalExecutor
  
  # ìˆ˜ì • í›„ (ì •ìƒ ë™ì‘)
  - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
```

#### ë¬¸ì œ 3: DB ëª…ë ¹ì–´ ë²„ì „ ì°¨ì´

**ì¦ìƒ:**
```
DeprecationWarning: `db init` is deprecated. Use `db migrate` instead
```

**ì›ì¸:**
- Airflow 2.7.0ë¶€í„° `airflow db init`ì´ deprecated
- ìƒˆ ë²„ì „ì—ì„œëŠ” `airflow db migrate` ì‚¬ìš© ê¶Œì¥

**í•´ê²°:**
```yaml
# ë‘ ëª…ë ¹ì–´ ëª¨ë‘ ì‹¤í–‰í•˜ì—¬ í˜¸í™˜ì„± í™•ë³´
command: bash -c "airflow db init && airflow users create ... "
```

#### ìµœì¢… Airflow ì„¤ì • (docker-compose.yml)
```yaml
airflow:
  image: apache/airflow:2.7.0-python3.10
  container_name: airflow
  user: root
  ports:
    - "8084:8080"
  environment:
    - AIRFLOW__CORE__EXECUTOR=SequentialExecutor  # SQLiteì™€ í˜¸í™˜
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
    - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
    - AIRFLOW__CORE__LOAD_EXAMPLES=False
    - AIRFLOW__WEBSERVER__SECRET_KEY=mysecretkey123
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./spark-jobs:/opt/spark-jobs
    - /var/run/docker.sock:/var/run/docker.sock  # Docker ëª…ë ¹ ì‹¤í–‰ìš©
  command: bash -c "airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true && (airflow webserver &) && airflow scheduler"
```

#### Airflow ì ‘ì† ì •ë³´
```
URL: http://<MASTER_IP>:8084
ID: admin
PW: admin
```

---

## ğŸ–¥ï¸ ë¶„ì‚° í™˜ê²½ êµ¬ì„±

### ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       ë¶„ì‚° í´ëŸ¬ìŠ¤í„° êµ¬ì„±                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             ë…¸íŠ¸ë¶ - Master (192.168.55.114)             â”‚
    â”‚                                                          â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚  Kafka   â”‚ â”‚ NameNode â”‚ â”‚  Spark   â”‚ â”‚ Airflow  â”‚   â”‚
    â”‚  â”‚          â”‚ â”‚  (HDFS)  â”‚ â”‚  Master  â”‚ â”‚          â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                                                          â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚ Backend  â”‚ â”‚Generator â”‚ â”‚ Grafana  â”‚ â”‚Prometheusâ”‚   â”‚
    â”‚  â”‚  (Java)  â”‚ â”‚ (Python) â”‚ â”‚          â”‚ â”‚          â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                               â”‚
              â–¼                               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ë¦¬ëˆ…ìŠ¤ A - Worker 1  â”‚       â”‚ ë¦¬ëˆ…ìŠ¤ B - Worker 2  â”‚
    â”‚  (192.168.55.158)   â”‚       â”‚   (192.168.55.9)    â”‚
    â”‚                     â”‚       â”‚                     â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚   DataNode    â”‚  â”‚       â”‚  â”‚   DataNode    â”‚  â”‚
    â”‚  â”‚    (HDFS)     â”‚  â”‚       â”‚  â”‚    (HDFS)     â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚                     â”‚       â”‚                     â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚ Spark Worker  â”‚  â”‚       â”‚  â”‚ Spark Worker  â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### PCë³„ ì—­í•  ë° ì„œë¹„ìŠ¤

| PC | IP | ì—­í•  | ì„œë¹„ìŠ¤ | í¬íŠ¸ |
|----|-----|------|--------|------|
| **ë…¸íŠ¸ë¶** | 192.168.55.114 | Master | Kafka | 9092 |
| | | | Kafka UI | 8080 |
| | | | HDFS NameNode | 9870, 9000 |
| | | | Spark Master | 8082, 7077 |
| | | | Airflow | 8084 |
| | | | Grafana | 3000 |
| | | | Prometheus | 9090 |
| | | | Backend | 8081 |
| | | | Generator | 8000 |
| **ë¦¬ëˆ…ìŠ¤ A** | 192.168.55.158 | Worker 1 | HDFS DataNode | 9864 |
| | | | Spark Worker | 8081 |
| **ë¦¬ëˆ…ìŠ¤ B** | 192.168.55.9 | Worker 2 | HDFS DataNode | 9864 |
| | | | Spark Worker | 8081 |

### ë°°í¬ íŒŒì¼ êµ¬ì¡°
```
deploy/
â”œâ”€â”€ docker-compose.master.yml   # Master ë…¸ë“œìš©
â””â”€â”€ docker-compose.worker.yml   # Worker ë…¸ë“œìš©

scripts/
â”œâ”€â”€ setup-master.sh             # Master ì´ˆê¸° ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ setup-worker.sh             # Worker ì´ˆê¸° ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
```

### ë¹ ë¥¸ ì‹œì‘

#### Master ë…¸ë“œ (ë…¸íŠ¸ë¶)
```bash
# 1. ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (ìµœì´ˆ 1íšŒ)
./scripts/setup-master.sh

# 2. Backend ë¹Œë“œ
cd backend && ./gradlew build && cd ..

# 3. ì„œë¹„ìŠ¤ ì‹œì‘
cd deploy
docker compose -f docker-compose.master.yml up -d --build
```

#### Worker ë…¸ë“œ (ë¦¬ëˆ…ìŠ¤ A, B)
```bash
# 1. ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (ìµœì´ˆ 1íšŒ)
./scripts/setup-worker.sh

# 2. ì„œë¹„ìŠ¤ ì‹œì‘
cd deploy
docker compose -f docker-compose.worker.yml up -d
```

### ì—°ê²° í™•ì¸

#### HDFS í´ëŸ¬ìŠ¤í„° ìƒíƒœ
```bash
# Masterì—ì„œ ì‹¤í–‰
docker exec namenode hdfs dfsadmin -report
```

ì˜ˆìƒ ì¶œë ¥:
```
Live datanodes (2):
  Name: 192.168.55.158:9866
  Name: 192.168.55.9:9866
```

#### Spark í´ëŸ¬ìŠ¤í„° ìƒíƒœ
```bash
# Spark Master UI í™•ì¸
http://192.168.55.114:8082
```

ì˜ˆìƒ: Workers (2) í‘œì‹œ

### ë‹¨ì¼ PC vs ë¶„ì‚° í™˜ê²½

| êµ¬ë¶„ | ë‹¨ì¼ PC (ê°œë°œìš©) | ë¶„ì‚° í™˜ê²½ (ìš´ì˜ìš©) |
|------|-----------------|------------------|
| **docker-compose íŒŒì¼** | docker-compose.yml | docker-compose.master.yml + docker-compose.worker.yml |
| **HDFS ë³µì œ** | DataNode 2ê°œ (ê°™ì€ PC) | DataNode 2ê°œ (ë‹¤ë¥¸ PC) |
| **Spark** | Master + Worker 1ê°œ | Master + Worker 2ê°œ |
| **ì¥ì•  ëŒ€ì‘** | ë¶ˆê°€ | 1ëŒ€ ì¥ì•  ì‹œ ê³„ì† ìš´ì˜ |
| **ì„±ëŠ¥** | ë¦¬ì†ŒìŠ¤ ê²½ìŸ | ë¦¬ì†ŒìŠ¤ ë¶„ì‚° |

### ì£¼ì˜ì‚¬í•­

1. **ë„¤íŠ¸ì›Œí¬**: ëª¨ë“  PCê°€ ê°™ì€ ë„¤íŠ¸ì›Œí¬(192.168.55.x)ì— ìˆì–´ì•¼ í•¨
2. **ë°©í™”ë²½**: í•„ìš”í•œ í¬íŠ¸ê°€ ì—´ë ¤ìˆì–´ì•¼ í•¨
3. **ì‹œê°„ ë™ê¸°í™”**: ëª¨ë“  PCì˜ ì‹œê°„ì´ ë™ê¸°í™”ë˜ì–´ì•¼ í•¨ (NTP ì‚¬ìš© ê¶Œì¥)
4. **Master ë¨¼ì € ì‹œì‘**: WorkerëŠ” Masterê°€ ì‹¤í–‰ëœ í›„ ì‹œì‘í•´ì•¼ ì—°ê²°ë¨

### ì„œë¹„ìŠ¤ URL ìš”ì•½

| ì„œë¹„ìŠ¤ | URL | ìš©ë„ |
|--------|-----|------|
| Kafka UI | http://192.168.55.114:8080 | Kafka í† í”½/ë©”ì‹œì§€ ëª¨ë‹ˆí„°ë§ |
| HDFS | http://192.168.55.114:9870 | ë¶„ì‚° íŒŒì¼ ì‹œìŠ¤í…œ ê´€ë¦¬ |
| Spark | http://192.168.55.114:8082 | Spark ì‘ì—… ëª¨ë‹ˆí„°ë§ |
| Airflow | http://192.168.55.114:8084 | ë°°ì¹˜ ì‘ì—… ìŠ¤ì¼€ì¤„ë§ |
| Grafana | http://192.168.55.114:3000 | ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ |
| Prometheus | http://192.168.55.114:9090 | ë©”íŠ¸ë¦­ ìˆ˜ì§‘/ì¿¼ë¦¬ |

---

### ë¶„ì‚° í™˜ê²½ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

#### ë¬¸ì œ: DataNodeê°€ NameNodeì— ì—°ê²° ì•ˆ ë¨

**ì¦ìƒ:**
```bash
docker exec namenode hdfs dfsadmin -report
# Live datanodes (0) - DataNodeê°€ ì—†ìŒ
```

**Worker ë¡œê·¸ ì—ëŸ¬:**
```
ERROR datanode.DataNode: Initialization failed for Block pool...
Datanode denied communication with namenode because hostname cannot be resolved
(ip=192.168.55.158, hostname=192.168.55.158)
```

**ì›ì¸:**
- NameNodeê°€ DataNodeì˜ IPë¥¼ í˜¸ìŠ¤íŠ¸ëª…ìœ¼ë¡œ ì—­ë°©í–¥ DNS ì¡°íšŒ ì‹œë„
- í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨ë¡œ ì—°ê²° ê±°ë¶€

**í•´ê²°:**
docker-compose.master.ymlì˜ namenode ì„¤ì •ì— ì¶”ê°€:
```yaml
namenode:
  environment:
    # ê¸°ì¡´ ì„¤ì •...
    - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
```

> ì°¸ê³ : í™˜ê²½ë³€ìˆ˜ì—ì„œ `.`ì€ `_`ë¡œ, `-`ëŠ” `___`ë¡œ ë³€í™˜ë¨
> `dfs.namenode.datanode.registration.ip-hostname-check` â†’ `HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check`

**ì ìš©:**
```bash
# Master ì¬ì‹œì‘
docker compose -f docker-compose.master.yml down
docker compose -f docker-compose.master.yml up -d

# Worker ì¬ì‹œì‘
docker compose -f docker-compose.worker.yml down
docker compose -f docker-compose.worker.yml up -d
```

**í™•ì¸:**
```bash
docker exec namenode hdfs dfsadmin -report
# Live datanodes (2) í™•ì¸
```

---

#### ë¬¸ì œ: Worker í¬íŠ¸ ì¶©ëŒ

**ì¦ìƒ:**
```
Error response from daemon: Bind for 0.0.0.0:8081 failed: port is already allocated
```

**ì›ì¸:**
- Worker PCì—ì„œ 8081 í¬íŠ¸ê°€ ì´ë¯¸ ì‚¬ìš© ì¤‘

**í•´ê²°:**
docker-compose.worker.ymlì—ì„œ í¬íŠ¸ ë³€ê²½:
```yaml
spark-worker:
  ports:
    - "10000:8081"   # ì™¸ë¶€ í¬íŠ¸ë¥¼ 10000ìœ¼ë¡œ ë³€ê²½
```

ë˜ëŠ” ëª…ë ¹ì–´ë¡œ:
```bash
sed -i 's/8081:8081/10000:8081/' docker-compose.worker.yml
```

---

#### ë¬¸ì œ: Sparkì—ì„œ HDFS DataNode ì—°ê²° ì‹¤íŒ¨

**ì¦ìƒ:**
```
WARN DataStreamer: Exception in createBlockOutputStream
java.net.ConnectException: Connection refused
WARN DataStreamer: Excluding datanode DatanodeInfoWithStorage[192.168.55.9:9866...]
ERROR: File could only be written to 0 of the 1 minReplication nodes.
There are 2 datanode(s) running and 2 node(s) are excluded in this operation.
```

**ì›ì¸:**
- NameNode â†” DataNode í†µì‹ ì€ ì •ìƒ (ë©”íƒ€ë°ì´í„°)
- Spark â†’ DataNode ì§ì ‘ ë°ì´í„° ì“°ê¸° ì‹œ 9866 í¬íŠ¸ í•„ìš”
- Workerì˜ docker-composeì—ì„œ 9866 í¬íŠ¸ê°€ ë…¸ì¶œë˜ì§€ ì•ŠìŒ
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ë°ì´í„° íë¦„                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  [NameNode] â†â”€â”€â”€â”€ ë©”íƒ€ë°ì´í„° â”€â”€â”€â”€â†’ [DataNode]               â”‚
â”‚      â†‘              (ì •ìƒ)            :9866                  â”‚
â”‚      â”‚                                  â†‘                    â”‚
â”‚  ë©”íƒ€ë°ì´í„°                              â”‚                    â”‚
â”‚   ì¡°íšŒ                            Connection refused         â”‚
â”‚      â”‚                                  â”‚                    â”‚
â”‚  [Spark] â”€â”€â”€â”€â”€â”€â”€ ë°ì´í„° ì“°ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                  (í¬íŠ¸ ë¯¸ë…¸ì¶œ)                               â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•´ê²°:**
docker-compose.worker.ymlì—ì„œ DataNode í¬íŠ¸ ì¶”ê°€ ë…¸ì¶œ:
```yaml
datanode:
  image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
  container_name: datanode
  ports:
    - "9864:9864"   # HTTP (Web UI)
    - "9866:9866"   # ë°ì´í„° ì „ì†¡ â† í•„ìˆ˜!
    - "9867:9867"   # IPC
  environment:
    - CORE_CONF_fs_defaultFS=hdfs://192.168.55.114:9000
    - HDFS_CONF_dfs_replication=2
    - HDFS_CONF_dfs_datanode_address=0.0.0.0:9866
    - HDFS_CONF_dfs_datanode_http_address=0.0.0.0:9864
    - HDFS_CONF_dfs_datanode_ipc_address=0.0.0.0:9867
```

**DataNode í¬íŠ¸ ì„¤ëª…:**

| í¬íŠ¸ | ìš©ë„ | í•„ìˆ˜ ì—¬ë¶€ |
|------|------|----------|
| 9864 | Web UI (HTTP) | ì„ íƒ |
| 9866 | ë°ì´í„° ì „ì†¡ | **í•„ìˆ˜** |
| 9867 | IPC í†µì‹  | ê¶Œì¥ |

**ì ìš©:**
```bash
# Workerì—ì„œ ì‹¤í–‰ (ë¦¬ëˆ…ìŠ¤ A, B ë‘˜ ë‹¤)
docker compose -f docker-compose.worker.yml down
docker compose -f docker-compose.worker.yml up -d
```

**í™•ì¸:**
```bash
# í¬íŠ¸ ë…¸ì¶œ í™•ì¸
docker compose -f docker-compose.worker.yml ps
# 9866 í¬íŠ¸ê°€ ë³´ì—¬ì•¼ í•¨
```

---

---

## â˜¸ï¸ Kubernetes í™˜ê²½ êµ¬ì„±

### ê°œìš”

Docker Compose ë¶„ì‚° í™˜ê²½ì„ Kubernetes(k3s)ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•©ë‹ˆë‹¤.
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Kubernetes í´ëŸ¬ìŠ¤í„° êµ¬ì„±                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚        ë…¸íŠ¸ë¶ - Master (192.168.55.114)                  â”‚
    â”‚        k3s control-plane                                 â”‚
    â”‚                                                          â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚              log-pipeline namespace               â”‚   â”‚
    â”‚  â”‚                                                   â”‚   â”‚
    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚   â”‚
    â”‚  â”‚  â”‚  Kafka  â”‚ â”‚NameNode â”‚ â”‚ Spark   â”‚            â”‚   â”‚
    â”‚  â”‚  â”‚   Pod   â”‚ â”‚   Pod   â”‚ â”‚ Master  â”‚            â”‚   â”‚
    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚   â”‚
    â”‚  â”‚                                                   â”‚   â”‚
    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚   â”‚
    â”‚  â”‚  â”‚ Airflow â”‚ â”‚ Grafana â”‚ â”‚Promethe â”‚            â”‚   â”‚
    â”‚  â”‚  â”‚   Pod   â”‚ â”‚   Pod   â”‚ â”‚us Pod   â”‚            â”‚   â”‚
    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚   â”‚
    â”‚  â”‚                                                   â”‚   â”‚
    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚   â”‚
    â”‚  â”‚  â”‚ Backend â”‚ â”‚Generatorâ”‚                         â”‚   â”‚
    â”‚  â”‚  â”‚   Pod   â”‚ â”‚   Pod   â”‚                         â”‚   â”‚
    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                               â”‚
              â–¼                               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ë¦¬ëˆ…ìŠ¤ A - Worker 1  â”‚       â”‚ ë¦¬ëˆ…ìŠ¤ B - Worker 2  â”‚
    â”‚   k3s agent         â”‚       â”‚   k3s agent         â”‚
    â”‚  (192.168.55.158)   â”‚       â”‚   (192.168.55.9)    â”‚
    â”‚                     â”‚       â”‚                     â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚   DataNode    â”‚  â”‚       â”‚  â”‚   DataNode    â”‚  â”‚
    â”‚  â”‚  (DaemonSet)  â”‚  â”‚       â”‚  â”‚  (DaemonSet)  â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚                     â”‚       â”‚                     â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚ Spark Worker  â”‚  â”‚       â”‚  â”‚ Spark Worker  â”‚  â”‚
    â”‚  â”‚  (DaemonSet)  â”‚  â”‚       â”‚  â”‚  (DaemonSet)  â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Docker Compose vs Kubernetes ë¹„êµ

| í•­ëª© | Docker Compose | Kubernetes |
|------|---------------|------------|
| **ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜** | ë‹¨ì¼/ìˆ˜ë™ | ìë™í™” |
| **ìŠ¤ì¼€ì¼ë§** | ìˆ˜ë™ | ìë™ (HPA) |
| **ìê°€ ì¹˜ìœ ** | ì—†ìŒ | Pod ìë™ ì¬ì‹œì‘ |
| **ì„œë¹„ìŠ¤ ë””ìŠ¤ì»¤ë²„ë¦¬** | Docker DNS | K8s DNS |
| **ë¡œë“œ ë°¸ëŸ°ì‹±** | ìˆ˜ë™ | Service ìë™ |
| **ë¡¤ë§ ì—…ë°ì´íŠ¸** | ìˆ˜ë™ | ìë™ |
| **ì„¤ì • ê´€ë¦¬** | .env íŒŒì¼ | ConfigMap/Secret |

---

### k3s í´ëŸ¬ìŠ¤í„° ì„¤ì¹˜

#### 1. Master ë…¸ë“œ (ë…¸íŠ¸ë¶)
```bash
# k3s ì„¤ì¹˜
curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644

# ì„¤ì¹˜ í™•ì¸
kubectl get nodes

# Worker ì¡°ì¸ìš© í† í° í™•ì¸
sudo cat /var/lib/rancher/k3s/server/node-token
```

#### 2. Worker ë…¸ë“œ (ë¦¬ëˆ…ìŠ¤ A, B)
```bash
# k3s agent ì„¤ì¹˜ (í† í°ì€ Masterì—ì„œ í™•ì¸í•œ ê°’)
curl -sfL https://get.k3s.io | K3S_URL=https://192.168.55.114:6443 K3S_TOKEN=<í† í°ê°’> sh -
```

#### 3. í´ëŸ¬ìŠ¤í„° í™•ì¸ (Masterì—ì„œ)
```bash
kubectl get nodes
```

ì˜ˆìƒ ì¶œë ¥:
```
NAME         STATUS   ROLES                  AGE
jun-victus   Ready    control-plane,master   5m
jun          Ready    <none>                 2m
jun-mini1    Ready    <none>                 2m
```

---

### Namespace ìƒì„±
```bash
kubectl apply -f kubernetes/namespace/namespace.yaml
```
```yaml
# kubernetes/namespace/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: log-pipeline
```

---

### ì„œë¹„ìŠ¤ ë°°í¬

#### ë°°í¬ ìˆœì„œ
```
1. Kafka
2. HDFS (NameNode â†’ DataNode)
3. Spark (Master â†’ Worker)
4. Airflow
5. Monitoring (Prometheus, Grafana)
6. Apps (Backend, Generator)
```

#### Kubernetes ë””ë ‰í† ë¦¬ êµ¬ì¡°
```
kubernetes/
â”œâ”€â”€ namespace/
â”‚   â””â”€â”€ namespace.yaml
â”œâ”€â”€ kafka/
â”‚   â””â”€â”€ kafka.yaml
â”œâ”€â”€ hdfs/
â”‚   â”œâ”€â”€ namenode.yaml
â”‚   â””â”€â”€ datanode.yaml
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ spark-master.yaml
â”‚   â”œâ”€â”€ spark-worker.yaml
â”‚   â””â”€â”€ spark-jobs-configmap.yaml
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ airflow.yaml
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ prometheus.yaml
â”‚   â””â”€â”€ grafana.yaml
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ backend.yaml
â”‚   â””â”€â”€ generator.yaml
â””â”€â”€ nodeport.yaml
```

#### ì „ì²´ ë°°í¬ ëª…ë ¹ì–´
```bash
# Namespace
kubectl apply -f kubernetes/namespace/

# ì¸í”„ë¼ ì„œë¹„ìŠ¤
kubectl apply -f kubernetes/kafka/
kubectl apply -f kubernetes/hdfs/
kubectl apply -f kubernetes/spark/

# ì• í”Œë¦¬ì¼€ì´ì…˜
kubectl apply -f kubernetes/airflow/
kubectl apply -f kubernetes/monitoring/
kubectl apply -f kubernetes/apps/

# ì™¸ë¶€ ì ‘ì† (NodePort)
kubectl apply -f kubernetes/nodeport.yaml
```

---

### ì»¤ìŠ¤í…€ ì´ë¯¸ì§€ ë°°í¬

Backend, GeneratorëŠ” ë¡œì»¬ ë¹Œë“œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
```bash
# 1. ì´ë¯¸ì§€ ë¹Œë“œ
cd backend
./gradlew build -x test
docker build -t log-pipeline-backend:latest .

cd ../generator
docker build -t log-pipeline-generator:latest .

# 2. k3së¡œ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
docker save log-pipeline-backend:latest | sudo k3s ctr images import -
docker save log-pipeline-generator:latest | sudo k3s ctr images import -

# 3. ë°°í¬ (imagePullPolicy: Never í•„ìˆ˜)
kubectl apply -f kubernetes/apps/
```

---

### ì™¸ë¶€ ì ‘ì† URL

| ì„œë¹„ìŠ¤ | NodePort | URL |
|--------|----------|-----|
| Grafana | 30000 | http://192.168.55.114:30000 |
| Airflow | 30084 | http://192.168.55.114:30084 |
| Spark UI | 30082 | http://192.168.55.114:30082 |
| HDFS UI | 30870 | http://192.168.55.114:30870 |
| Generator | 30800 | http://192.168.55.114:30800 |

---

### Spark Streaming Job ì‹¤í–‰
```bash
# HDFS ë””ë ‰í† ë¦¬ ìƒì„± (ì„ íƒì‚¬í•­ - ìë™ ìƒì„±ë¨)
kubectl exec -n log-pipeline deployment/namenode -- hdfs dfs -mkdir -p /data/logs/raw
kubectl exec -n log-pipeline deployment/namenode -- hdfs dfs -mkdir -p /checkpoints/raw_logs

# Spark Streaming Job ì‹¤í–‰
kubectl exec -n log-pipeline deployment/spark-master -- /spark/bin/spark-submit \
  --master spark://spark-master-svc.log-pipeline.svc.cluster.local:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 \
  /opt/spark-jobs/raw_to_hdfs.py
```

---

### ìƒíƒœ í™•ì¸ ëª…ë ¹ì–´
```bash
# ì „ì²´ Pod ìƒíƒœ
kubectl get pods -n log-pipeline

# íŠ¹ì • Pod ë¡œê·¸
kubectl logs -n log-pipeline deployment/<name> --tail=50

# HDFS í´ëŸ¬ìŠ¤í„° ìƒíƒœ
kubectl exec -n log-pipeline deployment/namenode -- hdfs dfsadmin -report

# HDFS ë°ì´í„° í™•ì¸
kubectl exec -n log-pipeline deployment/namenode -- hdfs dfs -ls -R /data/logs/raw

# Kafka í† í”½ í™•ì¸
kubectl exec -n log-pipeline deployment/kafka -- /opt/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092
```

---

### í´ëŸ¬ìŠ¤í„° ì •ë¦¬
```bash
# ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì‚­ì œ
kubectl delete namespace log-pipeline

# k3s ì œê±° (Master)
/usr/local/bin/k3s-uninstall.sh

# k3s ì œê±° (Worker)
/usr/local/bin/k3s-agent-uninstall.sh
```

---

## ğŸ”§ Kubernetes í™˜ê²½ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 14: Spark Master Service ì´ë¦„ ì¶©ëŒ

**ì¦ìƒ:**
```
java.lang.NumberFormatException: For input string: "tcp://10.43.220.131:8080"
```

**ì›ì¸:**
- K8sê°€ Service ì´ë¦„ìœ¼ë¡œ í™˜ê²½ë³€ìˆ˜ ìë™ ìƒì„±
- `spark-master` Service â†’ `SPARK_MASTER_PORT=tcp://...`
- Sparkê°€ ì´ ê°’ì„ ìˆ«ìë¡œ íŒŒì‹± ì‹œë„ â†’ ì‹¤íŒ¨

**í•´ê²°:**
Service ì´ë¦„ì„ `spark-master-svc`ë¡œ ë³€ê²½í•˜ê³  í™˜ê²½ë³€ìˆ˜ ëª…ì‹œì  ì„¤ì •:
```yaml
env:
  - name: SPARK_MASTER_PORT
    value: "7077"
```

---

### ë¬¸ì œ 15: Airflow DAG ë””ë ‰í† ë¦¬ ì¬ê·€ ë£¨í”„

**ì¦ìƒ:**
```
RuntimeError: Detected recursive loop when walking DAG directory
```

**ì›ì¸:**
- ConfigMapì„ DAG ë””ë ‰í† ë¦¬ì— ì§ì ‘ ë§ˆìš´íŠ¸
- ConfigMapì˜ ì‹¬ë³¼ë¦­ ë§í¬ êµ¬ì¡°ê°€ ë¬´í•œ ë£¨í”„ ìœ ë°œ

**í•´ê²°:**
initContainerì—ì„œ DAG íŒŒì¼ì„ PVCë¡œ ë³µì‚¬:
```yaml
initContainers:
  - name: init-dags
    command: ['sh', '-c', 'mkdir -p /opt/airflow/dags && cat > /opt/airflow/dags/pipeline.py << EOF...']
```

---

### ë¬¸ì œ 16: Airflow initContainer ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨

**ì¦ìƒ:**
```
sh: can't create /opt/airflow/dags/manual_pipeline.py: nonexistent directory
```

**ì›ì¸:**
- PVC ë§ˆìš´íŠ¸ ì‹œ ë¹ˆ ë””ë ‰í† ë¦¬ë¡œ ì‹œì‘
- dags ë””ë ‰í† ë¦¬ ë¯¸ì¡´ì¬

**í•´ê²°:**
initContainerì—ì„œ `mkdir -p` ë¨¼ì € ì‹¤í–‰:
```yaml
command: ['sh', '-c', 'mkdir -p /opt/airflow/dags && cat > ...']
```

---

### ë¬¸ì œ 17: Kafka Replication Factor ì˜¤ë¥˜

**ì¦ìƒ:**
```
InvalidReplicationFactorException: Unable to replicate the partition 2 time(s): 
only 1 broker(s) are registered
```

**ì›ì¸:**
- KafkaConfigì—ì„œ replicas(2) ì„¤ì •
- K8sì—ì„œ Kafka broker 1ê°œë§Œ ì‹¤í–‰

**í•´ê²°:**
KafkaConfig.javaì—ì„œ replicasë¥¼ 1ë¡œ ë³€ê²½ í›„ ì´ë¯¸ì§€ ì¬ë¹Œë“œ

---

### ë¬¸ì œ 18: Generatorì—ì„œ Backend ì—°ê²° ì‹¤íŒ¨

**ì¦ìƒ:**
```
ERROR: Failed to send logs: All connection attempts failed
```

**ì›ì¸:**
- backend_urlì´ localhostë¡œ ì„¤ì •ë¨
- K8sì—ì„œëŠ” Service DNS ì‚¬ìš© í•„ìš”

**í•´ê²°:**
í™˜ê²½ë³€ìˆ˜ë¡œ URL ì£¼ì…:
```yaml
env:
  - name: BACKEND_URL
    value: "http://backend-svc.log-pipeline.svc.cluster.local:8081"
```

---

### ë¬¸ì œ 19: Generator Settings ì†ì„± ëˆ„ë½

**ì¦ìƒ:**
```
AttributeError: 'Settings' object has no attribute 'services'
```

**ì›ì¸:**
- config.py ìˆ˜ì • ì‹œ ê¸°ì¡´ ì†ì„± ëˆ„ë½

**í•´ê²°:**
ê¸°ì¡´ ì„¤ì • ìœ ì§€í•˜ë©´ì„œ K8s í™˜ê²½ë³€ìˆ˜ë§Œ ì¶”ê°€

---

### ë¬¸ì œ 20: HDFS Cluster ID ë¶ˆì¼ì¹˜

**ì¦ìƒ:**
```
Incompatible clusterIDs in /hadoop/dfs/data: 
namenode clusterID = CID-xxx; datanode clusterID = CID-yyy
```

**ì›ì¸:**
- NameNode ì¬ìƒì„± ì‹œ ìƒˆ Cluster ID ë°œê¸‰
- DataNodeëŠ” ê¸°ì¡´ ID ë³´ìœ 

**í•´ê²°:**
Worker ë…¸ë“œì—ì„œ DataNode ë°ì´í„° ì‚­ì œ:
```bash
sudo rm -rf /data/hdfs/datanode/*
kubectl rollout restart daemonset/datanode -n log-pipeline
```

---

### ë¬¸ì œ 21: Spark Executor í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨

**ì¦ìƒ:**
```
java.net.UnknownHostException: spark-master-5885b7bc-6lck4
Failed to connect to spark-master-5885b7bc-6lck4:33405
```

**ì›ì¸:**
- Spark Masterê°€ Pod ì´ë¦„ì„ Driver URLë¡œ ì‚¬ìš©
- Workerì—ì„œ Pod ì´ë¦„ DNS í•´ì„ ë¶ˆê°€

**í•´ê²°:**
Headless Service + hostname/subdomainìœ¼ë¡œ DNS ì´ë¦„ ë¶€ì—¬:
```yaml
spec:
  hostname: spark-master
  subdomain: spark-headless
  containers:
    - env:
        - name: SPARK_PUBLIC_DNS
          value: "spark-master.spark-headless.log-pipeline.svc.cluster.local"
---
apiVersion: v1
kind: Service
metadata:
  name: spark-headless
spec:
  clusterIP: None  # Headless Service
```

---

### K8s í™˜ê²½ ì²´í¬ë¦¬ìŠ¤íŠ¸

| í•­ëª© | í™•ì¸ ëª…ë ¹ì–´ |
|------|------------|
| ë…¸ë“œ ìƒíƒœ | `kubectl get nodes` |
| Pod ìƒíƒœ | `kubectl get pods -n log-pipeline` |
| Service ìƒíƒœ | `kubectl get svc -n log-pipeline` |
| Pod ë¡œê·¸ | `kubectl logs -n log-pipeline <pod>` |
| Pod ìƒì„¸ | `kubectl describe pod -n log-pipeline <pod>` |
| HDFS ìƒíƒœ | `kubectl exec -n log-pipeline deployment/namenode -- hdfs dfsadmin -report` |
| Kafka í† í”½ | `kubectl exec -n log-pipeline deployment/kafka -- kafka-topics.sh --list --bootstrap-server localhost:9092` |
```

---

ğŸ‰ **K8s íŒŒì´í”„ë¼ì¸ ì™„ì„±!**

ì „ì²´ ë°ì´í„° íë¦„:
```
Generator â†’ Backend â†’ Kafka â†’ Spark Streaming â†’ HDFS
âœ…         âœ…        âœ…          âœ…            âœ…

## ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼

### ê°œìš”

PostgreSQLê³¼ HDFS/Sparkì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ìš”ì•½                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  í…ŒìŠ¤íŠ¸ í™˜ê²½:                                                        â”‚
â”‚  - Master: ë…¸íŠ¸ë¶ (8GB RAM, 4 Core)                                 â”‚
â”‚  - Worker: ë¦¬ëˆ…ìŠ¤ PC 2ëŒ€ (ê° 4GB RAM, 2 Core)                       â”‚
â”‚  - ìŠ¤í† ë¦¬ì§€: Master SSD, Worker HDD                                 â”‚
â”‚                                                                      â”‚
â”‚  í…ŒìŠ¤íŠ¸ ë°ì´í„°:                                                      â”‚
â”‚  - Write í…ŒìŠ¤íŠ¸: ìµœëŒ€ 500ë§Œê±´                                       â”‚
â”‚  - Read í…ŒìŠ¤íŠ¸: 500ë§Œê±´ ëŒ€ìƒ                                        â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Write Performance ê²°ê³¼

#### Phase 1: JPA saveAll() ë°©ì‹

| ë¶€í•˜ | ëª©í‘œ | ì‹¤ì œ ì²˜ë¦¬ëŸ‰ | ë‹¬ì„±ë¥  | ìƒíƒœ |
|------|------|------------|--------|------|
| 9ë§Œê±´/ë¶„ | 9ë§Œ | 9ë§Œê±´/ë¶„ | 100% | âœ… |
| 90ë§Œê±´/ë¶„ | 90ë§Œ | 90ë§Œê±´/ë¶„ | 100% | âœ… |
| 900ë§Œê±´/ë¶„ | 90ë§Œ | **20ë§Œê±´/ë¶„** | 22% | âŒ |

**ë³‘ëª© ì›ì¸**: JPA IDENTITY ì „ëµìœ¼ë¡œ ì¸í•œ ë°°ì¹˜ ë¹„í™œì„±í™”
- ìƒì„¸: [BENCHMARK_WRITE_RESULT.md](docs/BENCHMARK_WRITE_RESULT.md)

#### Phase 2: JDBC Batch ë°©ì‹

| ë¶€í•˜ | PostgreSQL | HDFS/Spark | ìŠ¹ì |
|------|------------|------------|------|
| 90ë§Œê±´/ë¶„ | âœ… ì•ˆì • | âœ… ì•ˆì • | ë¬´ìŠ¹ë¶€ |
| 180ë§Œê±´/ë¶„ | âœ… ì•ˆì • | âœ… ì•ˆì • | ë¬´ìŠ¹ë¶€ |
| 360ë§Œê±´/ë¶„ | âœ… **ë™ì‘** | âŒ **ì‚¬ë§** | PostgreSQL |

**ê°œì„  íš¨ê³¼**: JPA â†’ JDBC Batchë¡œ **9ë°° ì„±ëŠ¥ í–¥ìƒ** (20ë§Œ â†’ 180ë§Œê±´/ë¶„)
- ìƒì„¸: [BENCHMARK_WRITE_PHASE2.md](docs/BENCHMARK_WRITE_PHASE2.md)

#### HDFS ì‚¬ë§ ì›ì¸

```
Error: "2 datanode(s) running and 2 node(s) are excluded in this operation"
ì›ì¸: DataNodeê°€ ì“°ê¸° ì†ë„ë¥¼ ëª» ë”°ë¼ê°€ì„œ excluded ì²˜ë¦¬ë¨
```

- íŠ¸ëŸ¬ë¸”ìŠˆíŒ…: [TROUBLESHOOTING_SPARK_STREAMING.md](docs/TROUBLESHOOTING_SPARK_STREAMING.md)

---

### Read Performance ê²°ê³¼ (500ë§Œê±´)

| í…ŒìŠ¤íŠ¸ ìœ í˜• | PostgreSQL | HDFS | ë°°ìˆ˜ | ìŠ¹ì |
|-------------|------------|------|------|------|
| COUNT(*) | 376ms | 4,396ms | **11.7x** | PostgreSQL |
| GROUP BY | 584ms | 8,410ms | **14.4x** | PostgreSQL |
| ORDER BY | 229ms | 20,969ms | **91.5x** | PostgreSQL |
| ë³µì¡í•œ ì§‘ê³„ | 584ms | 8,058ms | **13.8x** | PostgreSQL |

**ì•ˆì •ì„± ë¹„êµ**:
| ì§€í‘œ | PostgreSQL | HDFS |
|------|------------|------|
| ì‘ë‹µ ì‹œê°„ | ë¹ ë¦„ (200~600ms) | ëŠë¦¼ (4~21ì´ˆ) |
| timeout ë°œìƒ | âš ï¸ ìˆìŒ | âœ… ì—†ìŒ |
| ë™ì‹œ ë¶€í•˜ ì²˜ë¦¬ | âš ï¸ ë¶ˆì•ˆì • | âœ… ì•ˆì •ì  |

- ìƒì„¸: [BENCHMARK_readPerformance.md](docs/BENCHMARK_readPerformance.md)

---

### í•µì‹¬ ê²°ë¡ 

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    í•µì‹¬ ë°œê²¬                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  1. 500ë§Œê±´ ê·œëª¨ì—ì„œëŠ” PostgreSQLì´ ì••ë„ì ìœ¼ë¡œ ë¹ ë¦„                 â”‚
â”‚     - Write: 9ë°° ë¹ ë¦„ (JDBC Batch ì ìš© ì‹œ)                          â”‚
â”‚     - Read: 10~90ë°° ë¹ ë¦„                                            â”‚
â”‚                                                                      â”‚
â”‚  2. HDFS/Sparkì˜ ì¥ì                                                â”‚
â”‚     - ì•ˆì •ì„±: timeout ì—†ì´ ì¼ê´€ëœ ì‘ë‹µ                              â”‚
â”‚     - í™•ì¥ì„±: ë…¸ë“œ ì¶”ê°€ë¡œ ë¬´í•œ í™•ì¥ ê°€ëŠ¥                            â”‚
â”‚     - ë¹„ìš©: ì½œë“œ ë°ì´í„° ì €ì¥ ë¹„ìš© ì ˆê° (HDD ì‚¬ìš©)                   â”‚
â”‚                                                                      â”‚
â”‚  3. ì‹œìŠ¤í…œ ì„ íƒ ê¸°ì¤€                                                â”‚
â”‚     - < 1000ë§Œê±´: PostgreSQL ê¶Œì¥                                   â”‚
â”‚     - > 1ì–µê±´: HDFS/Spark ê³ ë ¤                                      â”‚
â”‚     - í•˜ì´ë¸Œë¦¬ë“œ: Hot(PostgreSQL) + Cold(HDFS) ì¡°í•©                â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- ì‹œìŠ¤í…œ ì„ íƒ ê°€ì´ë“œ: [WHY_HDFS_SPARK.md](docs/WHY_HDFS_SPARK.md)

---

### ë²¤ì¹˜ë§ˆí¬ ë¬¸ì„œ ëª©ë¡

| ë¬¸ì„œ | ì„¤ëª… |
|------|------|
| [BENCHMARK.md](docs/BENCHMARK.md) | ë²¤ì¹˜ë§ˆí¬ ê°œìš” |
| [BENCHMARK_WRITE_RESULT.md](docs/BENCHMARK_WRITE_RESULT.md) | Write Phase 1 ê²°ê³¼ (JPA) |
| [BENCHMARK_WRITE_PHASE2.md](docs/BENCHMARK_WRITE_PHASE2.md) | Write Phase 2 ê²°ê³¼ (JDBC Batch) |
| [BENCHMARK_WRITE_PHASE3.md](docs/BENCHMARK_WRITE_PHASE3.md) | Write Phase 3 ê³„íš (ë¦¬ì†ŒìŠ¤ í™•ì¥) |
| [BENCHMARK_readPerformance.md](docs/BENCHMARK_readPerformance.md) | Read ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ |
| [WHY_HDFS_SPARK.md](docs/WHY_HDFS_SPARK.md) | ì‹œìŠ¤í…œ ì„ íƒ ê°€ì´ë“œ |
| [TROUBLESHOOTING_SPARK_STREAMING.md](docs/TROUBLESHOOTING_SPARK_STREAMING.md) | Spark Streaming íŠ¸ëŸ¬ë¸”ìŠˆíŒ… |

---


### ë¶„ì‚° í™˜ê²½ HDFS íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ìš”ì•½

ë¶„ì‚° í™˜ê²½ì—ì„œ HDFS êµ¬ì„± ì‹œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œë“¤ì…ë‹ˆë‹¤.

#### ë¬¸ì œ 1: DataNodeê°€ NameNodeì— ë“±ë¡ ì‹¤íŒ¨

**ì¦ìƒ:**
```bash
docker exec namenode hdfs dfsadmin -report
# Live datanodes (0) - DataNodeê°€ ì—†ìŒ
```

**Worker ë¡œê·¸:**
```
ERROR datanode.DataNode: Initialization failed for Block pool...
Datanode denied communication with namenode because hostname cannot be resolved
(ip=192.168.55.158, hostname=192.168.55.158)
```

**ì›ì¸:**
- NameNodeê°€ DataNodeì˜ IPë¥¼ í˜¸ìŠ¤íŠ¸ëª…ìœ¼ë¡œ ì—­ë°©í–¥ DNS ì¡°íšŒ ì‹œë„
- ë¶„ì‚° í™˜ê²½ì—ì„œ DNS ì„œë²„ê°€ ì—†ìœ¼ë©´ í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨

**í•´ê²°:**
docker-compose.master.ymlì˜ namenodeì— ì„¤ì • ì¶”ê°€:
```yaml
namenode:
  environment:
    # ê¸°ì¡´ ì„¤ì •...
    - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
```

> í™˜ê²½ë³€ìˆ˜ ë³€í™˜ ê·œì¹™:
> - `.` â†’ `_`
> - `-` â†’ `___`
> - `dfs.namenode.datanode.registration.ip-hostname-check`
> - â†’ `HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check`

---

#### ë¬¸ì œ 2: Sparkì—ì„œ DataNode ì—°ê²° ì‹¤íŒ¨

**ì¦ìƒ:**
```
WARN DataStreamer: Exception in createBlockOutputStream
java.net.ConnectException: Connection refused
WARN DataStreamer: Excluding datanode DatanodeInfoWithStorage[192.168.55.9:9866...]
ERROR: File could only be written to 0 of the 1 minReplication nodes.
There are 2 datanode(s) running and 2 node(s) are excluded in this operation.
```

**ì›ì¸:**
- NameNode â†” DataNode ë©”íƒ€ë°ì´í„° í†µì‹ ì€ ì •ìƒ
- Spark â†’ DataNode ì§ì ‘ ë°ì´í„° ì“°ê¸° ì‹œ 9866 í¬íŠ¸ í•„ìš”
- Workerì˜ docker-composeì—ì„œ 9866 í¬íŠ¸ê°€ ë…¸ì¶œë˜ì§€ ì•ŠìŒ
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HDFS ë°ì´í„° íë¦„                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  [Client/Spark]                                              â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”‚ 1. íŒŒì¼ ìƒì„± ìš”ì²­                                    â”‚
â”‚       â–¼                                                      â”‚
â”‚  [NameNode]                                                  â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”‚ 2. DataNode ìœ„ì¹˜ ë°˜í™˜                                â”‚
â”‚       â–¼                                                      â”‚
â”‚  [Client/Spark]                                              â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”‚ 3. DataNodeì— ì§ì ‘ ë°ì´í„° ì“°ê¸° (:9866)              â”‚
â”‚       â–¼                                                      â”‚
â”‚  [DataNode] â† ì—¬ê¸°ì„œ Connection refused ë°œìƒ                â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•´ê²°:**
docker-compose.worker.ymlì—ì„œ DataNode í¬íŠ¸ ì¶”ê°€:
```yaml
datanode:
  ports:
    - "9864:9864"   # HTTP (Web UI)
    - "9866:9866"   # ë°ì´í„° ì „ì†¡ â† í•„ìˆ˜!
    - "9867:9867"   # IPC
  environment:
    - HDFS_CONF_dfs_datanode_address=0.0.0.0:9866
    - HDFS_CONF_dfs_datanode_http_address=0.0.0.0:9864
    - HDFS_CONF_dfs_datanode_ipc_address=0.0.0.0:9867
```

---

#### ë¬¸ì œ 3: Worker í¬íŠ¸ ì¶©ëŒ

**ì¦ìƒ:**
```
Error response from daemon: Bind for 0.0.0.0:8081 failed: port is already allocated
```

**ì›ì¸:**
- Worker PCì—ì„œ 8081 í¬íŠ¸ê°€ ì´ë¯¸ ì‚¬ìš© ì¤‘

**í•´ê²°:**
docker-compose.worker.ymlì—ì„œ í¬íŠ¸ ë³€ê²½:
```yaml
spark-worker:
  ports:
    - "10000:8081"   # ì™¸ë¶€ í¬íŠ¸ë¥¼ 10000ìœ¼ë¡œ ë³€ê²½
```

---

#### ë¶„ì‚° í™˜ê²½ ì²´í¬ë¦¬ìŠ¤íŠ¸

| í•­ëª© | Master ì„¤ì • | Worker ì„¤ì • |
|------|------------|-------------|
| **hostname ì²´í¬ ë¹„í™œì„±í™”** | `HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false` | - |
| **DataNode ë°ì´í„° í¬íŠ¸** | - | `ports: 9866:9866` |
| **DataNode ì£¼ì†Œ ë°”ì¸ë”©** | - | `HDFS_CONF_dfs_datanode_address=0.0.0.0:9866` |
| **NameNode ì£¼ì†Œ** | `CORE_CONF_fs_defaultFS=hdfs://192.168.55.114:9000` | `CORE_CONF_fs_defaultFS=hdfs://192.168.55.114:9000` |

#### ì—°ê²° í™•ì¸ ëª…ë ¹ì–´
```bash
# 1. DataNode ë“±ë¡ í™•ì¸ (Masterì—ì„œ)
docker exec namenode hdfs dfsadmin -report

# 2. HDFS íŒŒì¼ ì“°ê¸° í…ŒìŠ¤íŠ¸ (Masterì—ì„œ)
docker exec namenode hdfs dfs -mkdir -p /test
docker exec namenode hdfs dfs -touchz /test/hello.txt
docker exec namenode hdfs dfs -ls /test

# 3. Spark Worker í™•ì¸
http://192.168.55.114:8082
```

---

#### ë¬¸ì œ 4: Docker ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬ë¡œ ì¸í•œ ì—°ê²° ë¬¸ì œ

**ì¦ìƒ:**
```
WARN DataStreamer: Exception in createBlockOutputStream
java.io.EOFException: Unexpected EOF while trying to read response from server
WARN TaskSchedulerImpl: Initial job has not accepted any resources; 
check your cluster UI to ensure that workers are registered and have sufficient resources
```

**ì›ì¸:**
- Docker ê¸°ë³¸ bridge ë„¤íŠ¸ì›Œí¬ëŠ” ì»¨í…Œì´ë„ˆë¥¼ ê²©ë¦¬í•¨
- Masterì™€ Workerê°€ ì„œë¡œ ë‹¤ë¥¸ PCì— ìˆì„ ë•Œ Docker ë‚´ë¶€ IPë¡œ í†µì‹  ì‹œë„
- DataNode/Spark Workerê°€ ë‚´ë¶€ IP(172.x.x.x)ë¥¼ Masterì— ë³´ê³ 
- Masterê°€ í•´ë‹¹ ë‚´ë¶€ IPë¡œ ì ‘ê·¼ ì‹œë„ â†’ ì‹¤íŒ¨
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ìƒí™©                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Master (192.168.55.114)                                     â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”‚ DataNode ìœ„ì¹˜ ì¡°íšŒ                                   â”‚
â”‚       â–¼                                                      â”‚
â”‚  NameNode: "DataNodeëŠ” 172.19.0.3:9866ì— ìˆì–´"              â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”‚ 172.19.0.3:9866 ì ‘ê·¼ ì‹œë„                           â”‚
â”‚       â–¼                                                      â”‚
â”‚  âŒ ì‹¤íŒ¨ (172.x.x.xëŠ” Worker PC ë‚´ë¶€ Docker IP)             â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•´ê²°:**
Workerì˜ docker-compose.worker.ymlì—ì„œ `network_mode: host` ì‚¬ìš©:
```yaml
services:
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://192.168.55.114:9000
      - HDFS_CONF_dfs_replication=2
      - HDFS_CONF_dfs_datanode_address=0.0.0.0:9866
      - HDFS_CONF_dfs_datanode_http_address=0.0.0.0:9864
      - HDFS_CONF_dfs_datanode_ipc_address=0.0.0.0:9867
      - HDFS_CONF_dfs_datanode_use_datanode_hostname=false
      - HDFS_CONF_dfs_client_use_datanode_hostname=false
    volumes:
      - datanode_data:/hadoop/dfs/data
    network_mode: host    # â† í•µì‹¬!

  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker
    environment:
      - SPARK_MASTER=spark://192.168.55.114:7077
    volumes:
      - ./spark-jobs:/opt/spark-jobs
    network_mode: host    # â† í•µì‹¬!

volumes:
  datanode_data:
```

**network_mode: host ì„¤ëª…:**

| ëª¨ë“œ | ì„¤ëª… | IP ì˜ˆì‹œ |
|------|------|---------|
| bridge (ê¸°ë³¸) | Docker ê°€ìƒ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš© | 172.19.0.3 |
| host | í˜¸ìŠ¤íŠ¸ ë„¤íŠ¸ì›Œí¬ ì§ì ‘ ì‚¬ìš© | 192.168.55.158 |

**ì£¼ì˜ì‚¬í•­:**
- `network_mode: host` ì‚¬ìš© ì‹œ `ports` ì„¤ì • ë¶ˆí•„ìš” (ì¶©ëŒ ë°œìƒ)
- ì»¨í…Œì´ë„ˆê°€ í˜¸ìŠ¤íŠ¸ì˜ ëª¨ë“  í¬íŠ¸ì— ì§ì ‘ ë°”ì¸ë”©ë¨
- Linuxì—ì„œë§Œ ì§€ì› (Mac/Windows ë¯¸ì§€ì›)

**ì ìš©:**
```bash
# Workerì—ì„œ ì‹¤í–‰ (ë¦¬ëˆ…ìŠ¤ A, B ë‘˜ ë‹¤)
docker compose -f docker-compose.worker.yml down
docker compose -f docker-compose.worker.yml up -d
```

**í™•ì¸:**
```bash
# Masterì—ì„œ DataNode í™•ì¸
docker exec namenode hdfs dfsadmin -report
# Live datanodesì— ì‹¤ì œ IP(192.168.55.x)ê°€ ë³´ì—¬ì•¼ í•¨

# Spark UIì—ì„œ Worker í™•ì¸
http://192.168.55.114:8082
# Workersì— ì‹¤ì œ IPê°€ ë³´ì—¬ì•¼ í•¨
```

---

#### ë¬¸ì œ 4: Docker ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬ë¡œ ì¸í•œ ì—°ê²° ë¬¸ì œ

**ì¦ìƒ:**
```
WARN DataStreamer: Exception in createBlockOutputStream
java.io.EOFException: Unexpected EOF while trying to read response from server
WARN TaskSchedulerImpl: Initial job has not accepted any resources; 
check your cluster UI to ensure that workers are registered and have sufficient resources
```

**ì›ì¸:**
- Docker ê¸°ë³¸ bridge ë„¤íŠ¸ì›Œí¬ëŠ” ì»¨í…Œì´ë„ˆë¥¼ ê²©ë¦¬í•¨
- Masterì™€ Workerê°€ ì„œë¡œ ë‹¤ë¥¸ PCì— ìˆì„ ë•Œ Docker ë‚´ë¶€ IPë¡œ í†µì‹  ì‹œë„
- DataNode/Spark Workerê°€ ë‚´ë¶€ IP(172.x.x.x)ë¥¼ Masterì— ë³´ê³ 
- Masterê°€ í•´ë‹¹ ë‚´ë¶€ IPë¡œ ì ‘ê·¼ ì‹œë„ â†’ ì‹¤íŒ¨
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ìƒí™©                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Master (192.168.55.114)                                     â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”‚ DataNode ìœ„ì¹˜ ì¡°íšŒ                                   â”‚
â”‚       â–¼                                                      â”‚
â”‚  NameNode: "DataNodeëŠ” 172.19.0.3:9866ì— ìˆì–´"              â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”‚ 172.19.0.3:9866 ì ‘ê·¼ ì‹œë„                           â”‚
â”‚       â–¼                                                      â”‚
â”‚  âŒ ì‹¤íŒ¨ (172.x.x.xëŠ” Worker PC ë‚´ë¶€ Docker IP)             â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•´ê²°:**
Workerì˜ docker-compose.worker.ymlì—ì„œ `network_mode: host` ì‚¬ìš©:
```yaml
services:
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://192.168.55.114:9000
      - HDFS_CONF_dfs_replication=2
      - HDFS_CONF_dfs_datanode_address=0.0.0.0:9866
      - HDFS_CONF_dfs_datanode_http_address=0.0.0.0:9864
      - HDFS_CONF_dfs_datanode_ipc_address=0.0.0.0:9867
      - HDFS_CONF_dfs_datanode_use_datanode_hostname=false
      - HDFS_CONF_dfs_client_use_datanode_hostname=false
    volumes:
      - datanode_data:/hadoop/dfs/data
    network_mode: host    # â† í•µì‹¬!

  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker
    environment:
      - SPARK_MASTER=spark://192.168.55.114:7077
    volumes:
      - ./spark-jobs:/opt/spark-jobs
    network_mode: host    # â† í•µì‹¬!

volumes:
  datanode_data:
```

**network_mode: host ì„¤ëª…:**

| ëª¨ë“œ | ì„¤ëª… | IP ì˜ˆì‹œ |
|------|------|---------|
| bridge (ê¸°ë³¸) | Docker ê°€ìƒ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš© | 172.19.0.3 |
| host | í˜¸ìŠ¤íŠ¸ ë„¤íŠ¸ì›Œí¬ ì§ì ‘ ì‚¬ìš© | 192.168.55.158 |

**ì£¼ì˜ì‚¬í•­:**
- `network_mode: host` ì‚¬ìš© ì‹œ `ports` ì„¤ì • ë¶ˆí•„ìš” (ì¶©ëŒ ë°œìƒ)
- ì»¨í…Œì´ë„ˆê°€ í˜¸ìŠ¤íŠ¸ì˜ ëª¨ë“  í¬íŠ¸ì— ì§ì ‘ ë°”ì¸ë”©ë¨
- Linuxì—ì„œë§Œ ì§€ì› (Mac/Windows ë¯¸ì§€ì›)

**ì ìš©:**
```bash
# Workerì—ì„œ ì‹¤í–‰ (ë¦¬ëˆ…ìŠ¤ A, B ë‘˜ ë‹¤)
docker compose -f docker-compose.worker.yml down
docker compose -f docker-compose.worker.yml up -d
```

**í™•ì¸:**
```bash
# Masterì—ì„œ DataNode í™•ì¸
docker exec namenode hdfs dfsadmin -report
# Live datanodesì— ì‹¤ì œ IP(192.168.55.x)ê°€ ë³´ì—¬ì•¼ í•¨

# Spark UIì—ì„œ Worker í™•ì¸
http://192.168.55.114:8082
# Workersì— ì‹¤ì œ IPê°€ ë³´ì—¬ì•¼ í•¨
```

---

### ë¶„ì‚° í™˜ê²½ ë„¤íŠ¸ì›Œí¬ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì¢…í•©

ë¶„ì‚° í™˜ê²½ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë„¤íŠ¸ì›Œí¬ ê´€ë ¨ ë¬¸ì œë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

#### ë¬¸ì œ 5: Spark Driver í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨

**ì¦ìƒ:**
```
WARN TaskSchedulerImpl: Initial job has not accepted any resources;
check your cluster UI to ensure that workers are registered and have sufficient resources
```

Worker ë¡œê·¸:
```
--driver-url "spark://CoarseGrainedScheduler@jun-Victus-by-HP-Gaming-Laptop-16-r0xxx.local:44691"
```

**ì›ì¸:**
- Spark Masterê°€ `network_mode: host`ë¡œ ì‹¤í–‰ë˜ë©´ì„œ í˜¸ìŠ¤íŠ¸ëª… ì‚¬ìš©
- Workerì—ì„œ Masterì˜ í˜¸ìŠ¤íŠ¸ëª…ì„ í•´ì„ ëª»í•¨

**í•´ê²°:**
Workerì˜ docker-composeì— `extra_hosts` ì¶”ê°€:
```yaml
services:
  spark-worker:
    extra_hosts:
      - "jun-Victus-by-HP-Gaming-Laptop-16-r0xxx.local:192.168.55.114"
```

ë˜ëŠ” Worker PCì˜ /etc/hostsì— ì¶”ê°€:
```bash
echo "192.168.55.114 jun-Victus-by-HP-Gaming-Laptop-16-r0xxx.local" | sudo tee -a /etc/hosts
```

---

#### ë¬¸ì œ 6: DataNode ê°„ ë³µì œ ì‹¤íŒ¨

**ì¦ìƒ:**
```
Starting thread to transfer blk_xxx to 192.168.55.158:9866
java.nio.channels.UnresolvedAddressException
```

**ì›ì¸:**
- HDFS ë³µì œ íŒ©í„°ê°€ 2ë¡œ ì„¤ì •ë¨
- DataNodeê°€ ë‹¤ë¥¸ DataNodeë¡œ ë¸”ë¡ ë³µì œ ì‹œë„
- Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ ë‹¤ë¥¸ Worker IPë¥¼ í•´ì„ ëª»í•¨

**í•´ê²°:**
ê° Workerì˜ docker-composeì— ë‹¤ë¥¸ Worker IP ì¶”ê°€:
```yaml
# Worker 1 (192.168.55.158)
services:
  datanode:
    extra_hosts:
      - "worker2:192.168.55.9"

# Worker 2 (192.168.55.9)
services:
  datanode:
    extra_hosts:
      - "worker1:192.168.55.158"
```

---

#### ë¬¸ì œ 7: Kafka ì˜¤í”„ì…‹ ë¶ˆì¼ì¹˜

**ì¦ìƒ:**
```
ERROR: Partition logs.raw-0's offset was changed from 8400 to 5100,
some data may have been missed.
```

**ì›ì¸:**
- Spark Streaming Jobì´ ë¹„ì •ìƒ ì¢…ë£Œ
- ì²´í¬í¬ì¸íŠ¸ì˜ ì˜¤í”„ì…‹ê³¼ Kafkaì˜ ì‹¤ì œ ì˜¤í”„ì…‹ ë¶ˆì¼ì¹˜
- Kafka ë°ì´í„°ê°€ retention ì •ì±…ìœ¼ë¡œ ì‚­ì œë¨

**í•´ê²°:**

1. ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ:
```bash
docker exec namenode hdfs dfs -rm -r /checkpoints/raw_logs
docker exec namenode hdfs dfs -mkdir -p /checkpoints/raw_logs
```

2. Spark Jobì— failOnDataLoss ì˜µì…˜ ì¶”ê°€:
```python
kafka_df = spark.readStream \
    .format("kafka") \
    .option("failOnDataLoss", "false") \  # ë°ì´í„° ì†ì‹¤ í—ˆìš©
    .load()
```

---

#### ë¬¸ì œ 8: Spark Job IP vs í˜¸ìŠ¤íŠ¸ëª… ë¬¸ì œ

**ì¦ìƒ:**
```
java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode
```

**ì›ì¸:**
- Spark Masterê°€ `network_mode: host`ë¡œ ë³€ê²½ë˜ë©´ì„œ Docker ë‚´ë¶€ DNS ì‚¬ìš© ë¶ˆê°€
- Spark Jobì—ì„œ `namenode`, `kafka` ë“± Docker ì„œë¹„ìŠ¤ëª… ì‚¬ìš©

**í•´ê²°:**
Spark Job íŒŒì¼ì—ì„œ ì„œë¹„ìŠ¤ëª… ëŒ€ì‹  ì‹¤ì œ IP ì‚¬ìš©:
```python
# ìˆ˜ì • ì „
.config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000")
.option("kafka.bootstrap.servers", "kafka:9092")

# ìˆ˜ì • í›„
.config("spark.hadoop.fs.defaultFS", "hdfs://192.168.55.114:9000")
.option("kafka.bootstrap.servers", "192.168.55.114:9092")
```

---

### ë¶„ì‚° í™˜ê²½ íŒŒì¼ êµ¬ì¡°
```
deploy/
â”œâ”€â”€ docker-compose.master.yml    # Master ë…¸ë“œìš©
â”œâ”€â”€ docker-compose.worker1.yml   # Worker 1 (192.168.55.158)ìš©
â””â”€â”€ docker-compose.worker2.yml   # Worker 2 (192.168.55.9)ìš©
```

### ë¶„ì‚° í™˜ê²½ ì‹¤í–‰ ìˆœì„œ
```bash
# 1. Master ë¨¼ì € ì‹¤í–‰
cd deploy
docker compose -f docker-compose.master.yml up -d

# 2. Worker 1 ì‹¤í–‰ (192.168.55.158ì—ì„œ)
docker compose -f docker-compose.worker1.yml up -d

# 3. Worker 2 ì‹¤í–‰ (192.168.55.9ì—ì„œ)
docker compose -f docker-compose.worker2.yml up -d

# 4. ìƒíƒœ í™•ì¸
docker exec namenode hdfs dfsadmin -report
# Spark UI: http://192.168.55.114:8082
```

### ë„¤íŠ¸ì›Œí¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

| í•­ëª© | Master | Worker 1 | Worker 2 |
|------|--------|----------|----------|
| **Master í˜¸ìŠ¤íŠ¸ëª… ë“±ë¡** | - | extra_hosts ë˜ëŠ” /etc/hosts | extra_hosts ë˜ëŠ” /etc/hosts |
| **ë‹¤ë¥¸ Worker IP ë“±ë¡** | - | worker2 IP | worker1 IP |
| **network_mode** | host (spark-masterë§Œ) | host | host |
| **Spark Job IP ì‚¬ìš©** | 192.168.55.114 | - | - |

---

---

#### ë¬¸ì œ 9: network_mode: hostì—ì„œ extra_hosts ë¬´ì‹œë¨

**ì¦ìƒ:**
docker-compose.ymlì— `extra_hosts` ì„¤ì •í–ˆëŠ”ë°ë„ í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨:
```
java.nio.channels.UnresolvedAddressException
Starting thread to transfer blk_xxx to 192.168.55.9:9866
```

**ì›ì¸:**
- `network_mode: host` ì‚¬ìš© ì‹œ ì»¨í…Œì´ë„ˆê°€ í˜¸ìŠ¤íŠ¸ ë„¤íŠ¸ì›Œí¬ ì§ì ‘ ì‚¬ìš©
- Dockerì˜ `extra_hosts` ì„¤ì •ì´ ë¬´ì‹œë¨
- ì»¨í…Œì´ë„ˆê°€ í˜¸ìŠ¤íŠ¸ PCì˜ `/etc/hosts` íŒŒì¼ì„ ì§ì ‘ ì°¸ì¡°

**í•´ê²°:**
Worker PCì˜ `/etc/hosts`ì— ì§ì ‘ ì¶”ê°€:
```bash
# ë¦¬ëˆ…ìŠ¤ A (192.168.55.158)ì—ì„œ
echo "192.168.55.9 worker2" | sudo tee -a /etc/hosts
echo "192.168.55.114 jun-Victus-by-HP-Gaming-Laptop-16-r0xxx.local" | sudo tee -a /etc/hosts

# ë¦¬ëˆ…ìŠ¤ B (192.168.55.9)ì—ì„œ
echo "192.168.55.158 worker1" | sudo tee -a /etc/hosts
echo "192.168.55.114 jun-Victus-by-HP-Gaming-Laptop-16-r0xxx.local" | sudo tee -a /etc/hosts
```

---

#### ë¬¸ì œ 10: Spark Worker ìê¸° í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨

**ì¦ìƒ:**
```
ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[main,5,main]
java.net.UnknownHostException: jun: jun: Try again
    at java.net.InetAddress.getLocalHost(InetAddress.java:1507)
    at org.apache.spark.util.Utils$.findLocalInetAddress(Utils.scala:1047)
```

**ì›ì¸:**
- Spark Workerê°€ ì‹œì‘ ì‹œ ìì‹ ì˜ í˜¸ìŠ¤íŠ¸ëª…ì„ IPë¡œ í•´ì„ ì‹œë„
- `network_mode: host` ì‚¬ìš© ì‹œ í˜¸ìŠ¤íŠ¸ PCì˜ `/etc/hosts` ì°¸ì¡°
- `/etc/hosts`ì— ìê¸° ìì‹ ì˜ í˜¸ìŠ¤íŠ¸ëª…ì´ ì—†ìœ¼ë©´ ì‹¤íŒ¨

**í•´ê²°:**
ê° Worker PCì˜ `/etc/hosts`ì— ìê¸° ìì‹ ì˜ í˜¸ìŠ¤íŠ¸ëª… ì¶”ê°€:
```bash
# ë¨¼ì € í˜¸ìŠ¤íŠ¸ëª… í™•ì¸
hostname

# /etc/hostsì— ì¶”ê°€
echo "127.0.0.1 $(hostname)" | sudo tee -a /etc/hosts
```

ì˜ˆì‹œ:
```bash
# ë¦¬ëˆ…ìŠ¤ A (hostname: jun)
echo "127.0.0.1 jun" | sudo tee -a /etc/hosts

# ë¦¬ëˆ…ìŠ¤ B (hostname: jun-mini1)
echo "127.0.0.1 jun-mini1" | sudo tee -a /etc/hosts
```

---

### ë¶„ì‚° í™˜ê²½ /etc/hosts ìµœì¢… ì„¤ì •

ê° PCë³„ë¡œ í•„ìš”í•œ `/etc/hosts` ì„¤ì •:

**Master (192.168.55.114) - ë…¸íŠ¸ë¶:**
```
# ê¸°ë³¸ ì„¤ì •ë§Œìœ¼ë¡œ ì¶©ë¶„
127.0.0.1 localhost
```

**Worker 1 (192.168.55.158) - ë¦¬ëˆ…ìŠ¤ A:**
```
127.0.0.1 localhost
127.0.0.1 jun                                              # ìê¸° ìì‹ 
192.168.55.114 jun-Victus-by-HP-Gaming-Laptop-16-r0xxx.local  # Master
192.168.55.9 worker2                                        # ë‹¤ë¥¸ Worker
```

**Worker 2 (192.168.55.9) - ë¦¬ëˆ…ìŠ¤ B:**
```
127.0.0.1 localhost
127.0.0.1 jun-mini1                                        # ìê¸° ìì‹ 
192.168.55.114 jun-Victus-by-HP-Gaming-Laptop-16-r0xxx.local  # Master
192.168.55.158 worker1                                      # ë‹¤ë¥¸ Worker
```

#### ë¬¸ì œ 11: Spark Worker IPv6 í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨

**ì¦ìƒ:**
```
ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[main,5,main]
java.net.UnknownHostException: jun: jun: Try again
    at java.net.InetAddress.getLocalHost(InetAddress.java:1507)
    at org.apache.spark.util.Utils$.findLocalInetAddress(Utils.scala:1047)
Caused by: java.net.UnknownHostException: jun: Try again
    at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
```

**ì›ì¸:**
- Spark Workerê°€ ì‹œì‘ ì‹œ ìì‹ ì˜ í˜¸ìŠ¤íŠ¸ëª…ì„ IPë¡œ í•´ì„ ì‹œë„
- Javaê°€ ê¸°ë³¸ì ìœ¼ë¡œ IPv6(`Inet6AddressImpl`)ë¡œ ë¨¼ì € ì¡°íšŒ
- `/etc/hosts`ì— í˜¸ìŠ¤íŠ¸ëª…ì´ ìˆì–´ë„ IPv6 ì¡°íšŒ ì‹¤íŒ¨ë¡œ ì—ëŸ¬ ë°œìƒ
- `network_mode: host` í™˜ê²½ì—ì„œ ë°œìƒ

**í•´ê²°:**
docker-composeì—ì„œ IPv4 ê°•ì œ ì‚¬ìš© ë° ë¡œì»¬ IP ëª…ì‹œì  ì§€ì •:
```yaml
spark-worker:
  image: bde2020/spark-worker:3.3.0-hadoop3.3
  container_name: spark-worker
  environment:
    - SPARK_MASTER=spark://192.168.55.114:7077
    - SPARK_LOCAL_IP=192.168.55.158           # ìì‹ ì˜ IP ëª…ì‹œ
    - SPARK_WORKER_OPTS=-Djava.net.preferIPv4Stack=true  # IPv4 ê°•ì œ
  network_mode: host
```

**í•µì‹¬ ì„¤ì •:**

| í™˜ê²½ë³€ìˆ˜ | ì„¤ëª… |
|----------|------|
| `SPARK_LOCAL_IP` | Workerì˜ IPë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§€ì • |
| `SPARK_WORKER_OPTS=-Djava.net.preferIPv4Stack=true` | Javaê°€ IPv4ë§Œ ì‚¬ìš©í•˜ë„ë¡ ê°•ì œ |

**Workerë³„ ì„¤ì • ì˜ˆì‹œ:**

Worker 1 (192.168.55.158):
```yaml
environment:
  - SPARK_LOCAL_IP=192.168.55.158
  - SPARK_WORKER_OPTS=-Djava.net.preferIPv4Stack=true
```

Worker 2 (192.168.55.9):
```yaml
environment:
  - SPARK_LOCAL_IP=192.168.55.9
  - SPARK_WORKER_OPTS=-Djava.net.preferIPv4Stack=true
```

**ì„±ê³µ ë¡œê·¸:**
```
INFO Worker: Starting Spark worker 192.168.55.158:37273 with 6 cores, 14.5 GiB RAM
INFO Worker: Connecting to master 192.168.55.114:7077...
INFO Worker: Successfully registered with master spark://192.168.55.114:7077
```

---

#### ë¬¸ì œ 12: DataNode ê°„ ë¸”ë¡ ë³µì œ ì‹¤íŒ¨

**ì¦ìƒ:**
```
WARN DataStreamer: Exception in createBlockOutputStream blk_xxx
java.io.IOException: Got error, status=ERROR, status message , ack with firstBadLink as 192.168.55.9:9866
WARN DataStreamer: Abandoning BP-xxx:blk_xxx
WARN DataStreamer: Excluding datanode DatanodeInfoWithStorage[192.168.55.9:9866...]
```

ë˜ëŠ”:
```
java.io.EOFException: Unexpected EOF while trying to read response from server
    at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed
    at org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream
```

**ì›ì¸:**
- HDFS ë³µì œ íŒ©í„°ê°€ 2ë¡œ ì„¤ì •ë¨
- ë¸”ë¡ì„ ì“¸ ë•Œ ì²« ë²ˆì§¸ DataNodeì—ì„œ ë‘ ë²ˆì§¸ DataNodeë¡œ ë³µì œ ì‹œë„
- DataNode ê°„ ë„¤íŠ¸ì›Œí¬ í†µì‹  ë¬¸ì œë¡œ ë³µì œ ì‹¤íŒ¨
- ë¶„ì‚° í™˜ê²½ì—ì„œ Docker ì»¨í…Œì´ë„ˆ ê°„ ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬ ë¬¸ì œ
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ë¸”ë¡ ë³µì œ íë¦„                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Client (Spark)                                              â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”‚ 1. ë¸”ë¡ ì“°ê¸° ìš”ì²­                                    â”‚
â”‚       â–¼                                                      â”‚
â”‚  DataNode 1 (192.168.55.158)                                â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”‚ 2. ë³µì œ íŒ©í„°=2ì´ë¯€ë¡œ ë‹¤ë¥¸ DataNodeë¡œ ë³µì œ ì‹œë„       â”‚
â”‚       â–¼                                                      â”‚
â”‚  DataNode 2 (192.168.55.9)                                  â”‚
â”‚       â”‚                                                      â”‚
â”‚       âŒ ì—°ê²° ì‹¤íŒ¨ (ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬)                           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•´ê²°:**
ë³µì œ íŒ©í„°ë¥¼ 1ë¡œ ë³€ê²½í•˜ì—¬ DataNode ê°„ ë³µì œë¥¼ ë¹„í™œì„±í™”:

Master docker-compose.master.yml:
```yaml
namenode:
  environment:
    - HDFS_CONF_dfs_replication=1   # 2 â†’ 1ë¡œ ë³€ê²½
```

Worker docker-compose.worker1.yml, docker-compose.worker2.yml:
```yaml
datanode:
  environment:
    - HDFS_CONF_dfs_replication=1   # 2 â†’ 1ë¡œ ë³€ê²½
```

**ë³µì œ íŒ©í„° ì„¤ëª…:**

| ë³µì œ íŒ©í„° | ì„¤ëª… | ì¥ì  | ë‹¨ì  |
|-----------|------|------|------|
| 1 | ë¸”ë¡ì„ 1ê°œ DataNodeì—ë§Œ ì €ì¥ | ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ì—†ìŒ | ì¥ì•  ì‹œ ë°ì´í„° ì†ì‹¤ |
| 2 | ë¸”ë¡ì„ 2ê°œ DataNodeì— ë³µì œ | 1ëŒ€ ì¥ì•  í—ˆìš© | DataNode ê°„ í†µì‹  í•„ìš” |
| 3 (ê¸°ë³¸) | ë¸”ë¡ì„ 3ê°œ DataNodeì— ë³µì œ | 2ëŒ€ ì¥ì•  í—ˆìš© | ë” ë§ì€ í†µì‹  í•„ìš” |

**ì°¸ê³ :**
- ê°œë°œ/í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œëŠ” ë³µì œ íŒ©í„° 1 ê¶Œì¥
- í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ë³µì œ íŒ©í„° 2 ì´ìƒ + ë„¤íŠ¸ì›Œí¬ ì„¤ì • í•„ìš”
- Kubernetes í™˜ê²½ì—ì„œëŠ” Pod ê°„ ë„¤íŠ¸ì›Œí¬ê°€ ìë™ êµ¬ì„±ë˜ì–´ ë³µì œ íŒ©í„° 2 ì´ìƒ ì‚¬ìš© ê°€ëŠ¥

**ì ìš© í›„ ì¬ì‹œì‘:**
```bash
# Master
docker compose -f docker-compose.master.yml down
docker compose -f docker-compose.master.yml up -d

# Worker 1
docker compose -f docker-compose.worker1.yml down
docker compose -f docker-compose.worker1.yml up -d

# Worker 2
docker compose -f docker-compose.worker2.yml down
docker compose -f docker-compose.worker2.yml up -d
```

---

#### ë¬¸ì œ 13: Airflowì—ì„œ Spark Master í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨

**ì¦ìƒ:**
```
WARN TransportClientFactory: DNS resolution failed for spark-master:7077 took 5003 ms
WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master spark-master:7077
Caused by: java.net.UnknownHostException: spark-master
ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive!
```

**ì›ì¸:**
- Spark Masterê°€ `network_mode: host`ë¡œ ë³€ê²½ë¨
- Airflow DAGì—ì„œ `spark://spark-master:7077`ë¡œ ì—°ê²° ì‹œë„
- Docker ì„œë¹„ìŠ¤ëª… `spark-master`ë¥¼ DNSë¡œ í•´ì„ ë¶ˆê°€
- Airflow ì»¨í…Œì´ë„ˆëŠ” `pipeline-network`ì— ìˆê³ , Spark MasterëŠ” host ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Airflow (pipeline-network)                                  â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”‚ spark://spark-master:7077                            â”‚
â”‚       â–¼                                                      â”‚
â”‚  âŒ DNS í•´ì„ ì‹¤íŒ¨                                            â”‚
â”‚                                                              â”‚
â”‚  Spark Master (network_mode: host)                           â”‚
â”‚       â”‚                                                      â”‚
â”‚       â””â”€ ì‹¤ì œ ì£¼ì†Œ: 192.168.55.114:7077                      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•´ê²°:**
Airflow DAG íŒŒì¼ì—ì„œ Docker ì„œë¹„ìŠ¤ëª… ëŒ€ì‹  ì‹¤ì œ IP ì‚¬ìš©:
```python
# ìˆ˜ì • ì „
daily_report = BashOperator(
    task_id='daily_report',
    bash_command='''
        docker exec spark-master /spark/bin/spark-submit \
            --master spark://spark-master:7077 \
            /opt/spark-jobs/batch/daily_report.py {{ ds }}
    ''',
    dag=dag,
)

# ìˆ˜ì • í›„
daily_report = BashOperator(
    task_id='daily_report',
    bash_command='''
        docker exec spark-master /spark/bin/spark-submit \
            --master spark://192.168.55.114:7077 \
            /opt/spark-jobs/batch/daily_report.py {{ ds }}
    ''',
    dag=dag,
)
```

**ìˆ˜ì • íŒŒì¼:**
- `airflow/dags/manual_pipeline.py`
- `airflow/dags/daily_pipeline.py`

**ì ìš©:**
DAG íŒŒì¼ ìˆ˜ì • í›„ Airflowê°€ ìë™ìœ¼ë¡œ ê°ì§€ (ì¬ì‹œì‘ ë¶ˆí•„ìš”)

**í™•ì¸:**
```bash
# Airflow UIì—ì„œ
1. ì‹¤íŒ¨í•œ DAG run Clear ë˜ëŠ” ì‚­ì œ
2. Trigger DAG í´ë¦­
3. Task ë¡œê·¸ì—ì„œ "Successfully registered with master" í™•ì¸
```

---

---

## ğŸ”§ Kubernetes í™˜ê²½ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 14: Spark Master Service ì´ë¦„ ì¶©ëŒ

**ì¦ìƒ:**
```
ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[main,5,main]
java.lang.NumberFormatException: For input string: "tcp://10.43.220.131:8080"
    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
    at java.lang.Integer.parseInt(Integer.java:580)
    at org.apache.spark.deploy.master.MasterArguments.<init>(MasterArguments.scala:46)
```

**ì›ì¸:**
- KubernetesëŠ” Service ìƒì„± ì‹œ ìë™ìœ¼ë¡œ í™˜ê²½ë³€ìˆ˜ë¥¼ ì£¼ì…í•¨
- Service ì´ë¦„ì´ `spark-master`ì¼ ê²½ìš°:
    - `SPARK_MASTER_SERVICE_HOST=10.43.220.131`
    - `SPARK_MASTER_SERVICE_PORT=tcp://10.43.220.131:8080`
    - `SPARK_MASTER_PORT=tcp://10.43.220.131:7077`
- SparkëŠ” `SPARK_MASTER_PORT`ë¥¼ ìˆ«ì(í¬íŠ¸ ë²ˆí˜¸)ë¡œ íŒŒì‹±í•˜ë ¤ê³  ì‹œë„
- `tcp://...` ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜ ì‹¤íŒ¨
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   í™˜ê²½ë³€ìˆ˜ ì¶©ëŒ ìƒí™©                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Kubernetes ìë™ ì£¼ì…:                                       â”‚
â”‚  SPARK_MASTER_PORT=tcp://10.43.220.131:7077                 â”‚
â”‚                                                              â”‚
â”‚  Sparkê°€ ê¸°ëŒ€í•˜ëŠ” ê°’:                                        â”‚
â”‚  SPARK_MASTER_PORT=7077                                      â”‚
â”‚                                                              â”‚
â”‚  â†’ NumberFormatException ë°œìƒ!                              â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•´ê²°:**
Service ì´ë¦„ì„ ë³€ê²½í•˜ì—¬ í™˜ê²½ë³€ìˆ˜ ì¶©ëŒ ë°©ì§€:
```yaml
# ìˆ˜ì • ì „
apiVersion: v1
kind: Service
metadata:
  name: spark-master   # SPARK_MASTER_* í™˜ê²½ë³€ìˆ˜ ìë™ ìƒì„±ë¨

# ìˆ˜ì • í›„
apiVersion: v1
kind: Service
metadata:
  name: spark-master-svc   # SPARK_MASTER_SVC_* í™˜ê²½ë³€ìˆ˜ ìƒì„± (ì¶©ëŒ ì—†ìŒ)
```

ë˜í•œ ëª…ì‹œì ìœ¼ë¡œ í¬íŠ¸ í™˜ê²½ë³€ìˆ˜ ì„¤ì •:
```yaml
containers:
  - name: spark-master
    env:
      - name: SPARK_MASTER_PORT
        value: "7077"
      - name: SPARK_MASTER_WEBUI_PORT
        value: "8080"
```

**Spark Workerë„ Service ì£¼ì†Œ ë³€ê²½:**
```yaml
env:
  - name: SPARK_MASTER
    value: "spark://spark-master-svc.log-pipeline.svc.cluster.local:7077"
```

**Kubernetes Service í™˜ê²½ë³€ìˆ˜ ê·œì¹™:**
- Service ì´ë¦„: `my-service`
- ìë™ ìƒì„± í™˜ê²½ë³€ìˆ˜:
    - `MY_SERVICE_SERVICE_HOST`
    - `MY_SERVICE_SERVICE_PORT`
    - `MY_SERVICE_PORT`

ì• í”Œë¦¬ì¼€ì´ì…˜ì´ íŠ¹ì • í™˜ê²½ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, Service ì´ë¦„ì´ ì¶©ëŒí•˜ì§€ ì•Šë„ë¡ ì£¼ì˜!

---

### ë¬¸ì œ 15: Airflow DAG ë””ë ‰í† ë¦¬ ì¬ê·€ ë£¨í”„ ì˜¤ë¥˜

**ì¦ìƒ:**
```
RuntimeError: Detected recursive loop when walking DAG directory /opt/airflow/dags: 
/opt/airflow/dags/..2026_01_12_01_22_40.195680184 has appeared more than once.
ConnectionResetError: [Errno 104] Connection reset by peer
```

**ì›ì¸:**
- Kubernetes ConfigMapì„ DAG ë””ë ‰í† ë¦¬ì— ì§ì ‘ ë§ˆìš´íŠ¸
- ConfigMapì€ ì‹¬ë³¼ë¦­ ë§í¬ êµ¬ì¡°ë¡œ íŒŒì¼ ìƒì„± (`..data` â†’ `..2026_01_12_...`)
- Airflow DAG íŒŒì„œê°€ ì‹¬ë³¼ë¦­ ë§í¬ë¥¼ ë”°ë¼ê°€ë©´ì„œ ë¬´í•œ ë£¨í”„ ë°œìƒ
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ConfigMap ë§ˆìš´íŠ¸ êµ¬ì¡°                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  /opt/airflow/dags/                                          â”‚
â”‚  â”œâ”€â”€ ..2026_01_12_01_22_40.195680184/  (ì‹¤ì œ ë°ì´í„°)        â”‚
â”‚  â”‚   â””â”€â”€ manual_pipeline.py                                  â”‚
â”‚  â”œâ”€â”€ ..data -> ..2026_01_12_01_22_40.195680184 (ì‹¬ë³¼ë¦­ë§í¬) â”‚
â”‚  â””â”€â”€ manual_pipeline.py -> ..data/manual_pipeline.py        â”‚
â”‚                                                              â”‚
â”‚  Airflowê°€ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì‹œ ì‹¬ë³¼ë¦­ ë§í¬ ìˆœí™˜ ê°ì§€ â†’ ì—ëŸ¬    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•´ê²°:**
ConfigMap ì§ì ‘ ë§ˆìš´íŠ¸ ëŒ€ì‹  initContainerë¡œ DAG íŒŒì¼ ë³µì‚¬:
```yaml
# ìˆ˜ì • ì „: ConfigMap ì§ì ‘ ë§ˆìš´íŠ¸ (ë¬¸ì œ ë°œìƒ)
volumes:
  - name: dags
    configMap:
      name: airflow-dags
containers:
  - volumeMounts:
      - name: dags
        mountPath: /opt/airflow/dags

# ìˆ˜ì • í›„: initContainerë¡œ íŒŒì¼ ë³µì‚¬
initContainers:
  - name: init-dags
    image: busybox
    command: ['sh', '-c', 'cat > /opt/airflow/dags/manual_pipeline.py << DAGEOF
from airflow import DAG
...
DAGEOF']
    volumeMounts:
      - name: airflow-data
        mountPath: /opt/airflow
containers:
  - name: airflow
    volumeMounts:
      - name: airflow-data
        mountPath: /opt/airflow
volumes:
  - name: airflow-data
    persistentVolumeClaim:
      claimName: airflow-pvc
```

**initContainer ë°©ì‹ì˜ ì¥ì :**
- ì‹¬ë³¼ë¦­ ë§í¬ ì—†ì´ ì‹¤ì œ íŒŒì¼ë¡œ ë³µì‚¬
- PVCì— ì €ì¥ë˜ì–´ Pod ì¬ì‹œì‘ ì‹œì—ë„ ìœ ì§€
- Airflow DAG íŒŒì„œê°€ ì •ìƒì ìœ¼ë¡œ íŒŒì¼ ì¸ì‹

---

### ë¬¸ì œ 16: Airflow initContainerì—ì„œ DAG ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨

**ì¦ìƒ:**
```
sh: can't create /opt/airflow/dags/manual_pipeline.py: nonexistent directory
```

**ì›ì¸:**
- PVCê°€ ë§ˆìš´íŠ¸ë˜ë©´ ë¹ˆ ë””ë ‰í† ë¦¬ë¡œ ì‹œì‘
- `/opt/airflow/dags` ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ
- ConfigMapì˜ init.sh ìŠ¤í¬ë¦½íŠ¸ì—ì„œ `mkdir -p` ëª…ë ¹ì´ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PVC ë§ˆìš´íŠ¸ ìƒíƒœ                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  /opt/airflow/  (PVC ë§ˆìš´íŠ¸ - ë¹ˆ ë””ë ‰í† ë¦¬)                  â”‚
â”‚  â””â”€â”€ (ì—†ìŒ)                                                  â”‚
â”‚                                                              â”‚
â”‚  cat > /opt/airflow/dags/manual_pipeline.py                 â”‚
â”‚       â†“                                                      â”‚
â”‚  âŒ /opt/airflow/dags/ ë””ë ‰í† ë¦¬ ì—†ìŒ â†’ ì‹¤íŒ¨                 â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•´ê²°:**
initContainer commandì—ì„œ ì§ì ‘ mkdir ì‹¤í–‰:
```yaml
initContainers:
  - name: init-dags
    image: busybox
    command:
      - sh
      - -c
      - |
        mkdir -p /opt/airflow/dags
        cat > /opt/airflow/dags/manual_pipeline.py << 'PYEND'
        from airflow import DAG
        ...
        PYEND
    volumeMounts:
      - name: airflow-data
        mountPath: /opt/airflow
```

**ì£¼ì˜ì‚¬í•­:**
- ConfigMapì„ ë³„ë„ ë§ˆìš´íŠ¸í•˜ë©´ ì‹¬ë³¼ë¦­ ë§í¬ ë¬¸ì œ ë°œìƒ ê°€ëŠ¥ (ë¬¸ì œ 15 ì°¸ì¡°)
- initContainerì—ì„œ ì§ì ‘ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ì´ ë” ì•ˆì •ì 
- heredoc ì‚¬ìš© ì‹œ ë“¤ì—¬ì“°ê¸° ì£¼ì˜ (YAMLê³¼ shell ë¬¸ë²• ì¶©ëŒ)

---

### ë¬¸ì œ 17: Kafka Replication Factor ì˜¤ë¥˜ (K8s í™˜ê²½)

**ì¦ìƒ:**
```
Caused by: org.apache.kafka.common.errors.InvalidReplicationFactorException: 
Unable to replicate the partition 2 time(s): The target replication factor of 2 
cannot be reached because only 1 broker(s) are registered.
```

**ì›ì¸:**
- KafkaConfig.javaì—ì„œ replicas(2)ë¡œ ì„¤ì •
- K8s í™˜ê²½ì—ì„œëŠ” Kafka brokerê°€ 1ê°œë§Œ ì‹¤í–‰
- ë³µì œ íŒ©í„°ê°€ broker ìˆ˜ë³´ë‹¤ í¬ë©´ í† í”½ ìƒì„± ì‹¤íŒ¨

**í•´ê²°:**
KafkaConfig.javaì—ì„œ replicasë¥¼ 1ë¡œ ë³€ê²½:
```java
// ìˆ˜ì • ì „
return TopicBuilder.name("logs.raw")
        .partitions(3)
        .replicas(2)
        .build();

// ìˆ˜ì • í›„
return TopicBuilder.name("logs.raw")
        .partitions(3)
        .replicas(1)
        .build();
```

ì´ë¯¸ì§€ ì¬ë¹Œë“œ ë° ë°°í¬:
```bash
cd backend
./gradlew build -x test
docker build -t log-pipeline-backend:latest .
docker save log-pipeline-backend:latest | sudo k3s ctr images import -
kubectl rollout restart deployment/backend -n log-pipeline
```

---

### ë¬¸ì œ 18: Generatorì—ì„œ Backend ì—°ê²° ì‹¤íŒ¨ (K8s í™˜ê²½)

**ì¦ìƒ:**
```
ERROR:app.scheduler:Failed to send logs: All connection attempts failed
```

**ì›ì¸:**
- Generatorì˜ config.pyì—ì„œ backend_urlì´ localhostë¡œ ì„¤ì •
- K8s í™˜ê²½ì—ì„œëŠ” Service DNS ì´ë¦„ ì‚¬ìš© í•„ìš”

**í•´ê²°:**
config.pyì—ì„œ í™˜ê²½ë³€ìˆ˜ë¡œ URL ì£¼ì… ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì •:
```python
# ìˆ˜ì • ì „
backend_url: str = "http://localhost:8081"

# ìˆ˜ì • í›„
backend_url: str = os.getenv("BACKEND_URL", "http://localhost:8081")
```

K8s Deploymentì—ì„œ í™˜ê²½ë³€ìˆ˜ ì„¤ì •:
```yaml
env:
  - name: BACKEND_URL
    value: "http://backend-svc.log-pipeline.svc.cluster.local:8081"
```

---

### ë¬¸ì œ 19: Generator Settings ì†ì„± ëˆ„ë½

**ì¦ìƒ:**
```
AttributeError: 'Settings' object has no attribute 'services'
```

**ì›ì¸:**
- config.py ìˆ˜ì • ì‹œ ê¸°ì¡´ ì„¤ì • ì†ì„±ë“¤ì´ ëˆ„ë½ë¨
- log_generator.pyì—ì„œ settings.services ì°¸ì¡°

**í•´ê²°:**
ê¸°ì¡´ ì„¤ì •ì„ ìœ ì§€í•˜ë©´ì„œ K8s í™˜ê²½ë³€ìˆ˜ë§Œ ì¶”ê°€:
```python
from pydantic_settings import BaseSettings
import os

class Settings(BaseSettings):
    # Backend API ì„¤ì • - K8s í™˜ê²½ë³€ìˆ˜ ìš°ì„ 
    backend_url: str = os.getenv("BACKEND_URL", "http://localhost:8081")
    backend_timeout: int = 30

    # ë°°ì¹˜ ìŠ¤ì¼€ì¤„ ì„¤ì •
    log_interval_seconds: int = 5
    event_interval_seconds: int = 10
    batch_size: int = 100

    # ìƒì„± ë°ì´í„° ì„¤ì • (ëˆ„ë½ë˜ë©´ ì•ˆë¨!)
    services: list = ["api-gateway", "user-service", "order-service", "payment-service"]
    log_levels: list = ["INFO", "DEBUG", "WARN", "ERROR"]
    event_types: list = ["CLICK", "VIEW", "PURCHASE", "LOGIN", "LOGOUT", "SEARCH"]

    error_rate: float = 0.05

    class Config:
        env_file = ".env"
        env_prefix = "GENERATOR_"

settings = Settings()
```

**êµí›ˆ:**
ì„¤ì • íŒŒì¼ ìˆ˜ì • ì‹œ ê¸°ì¡´ ì†ì„±ì„ ëª¨ë‘ ìœ ì§€í•˜ë©´ì„œ í•„ìš”í•œ ë¶€ë¶„ë§Œ ìˆ˜ì •í•  ê²ƒ!

---

### ë¬¸ì œ 20: HDFS DataNode Cluster ID ë¶ˆì¼ì¹˜

**ì¦ìƒ:**
```
WARN common.Storage: Failed to add storage directory [DISK]file:/hadoop/dfs/data
java.io.IOException: Incompatible clusterIDs in /hadoop/dfs/data: 
namenode clusterID = CID-35773007-7d1d-4da8-ac6a-8e6c75978793; 
datanode clusterID = CID-8054f9f7-1c5d-4c00-8049-964f104d0e96

ERROR datanode.DataNode: Initialization failed for Block pool <registering>
java.io.IOException: All specified directories have failed to load.
```

**ì›ì¸:**
- NameNodeê°€ ì¬ìƒì„±ë˜ë©´ì„œ ìƒˆë¡œìš´ Cluster ID ë°œê¸‰
- DataNodeëŠ” ê¸°ì¡´ Cluster IDë¥¼ ê°€ì§„ ë°ì´í„° ë³´ìœ 
- Cluster ID ë¶ˆì¼ì¹˜ë¡œ DataNodeê°€ NameNodeì— ë“±ë¡ ì‹¤íŒ¨
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Cluster ID ë¶ˆì¼ì¹˜ ìƒí™©                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  NameNode (ìƒˆë¡œ ìƒì„±)                                        â”‚
â”‚  â””â”€ Cluster ID: CID-35773007-...  (ì‹ ê·œ)                    â”‚
â”‚                                                              â”‚
â”‚  DataNode (ê¸°ì¡´ ë°ì´í„° ë³´ìœ )                                 â”‚
â”‚  â””â”€ Cluster ID: CID-8054f9f7-...  (ê¸°ì¡´)                    â”‚
â”‚                                                              â”‚
â”‚  â†’ ID ë¶ˆì¼ì¹˜ â†’ DataNode ë“±ë¡ ì‹¤íŒ¨ â†’ CrashLoopBackOff        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•´ê²°:**
DataNodeì˜ ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚­ì œí•˜ì—¬ ìƒˆ Cluster IDë¡œ ì´ˆê¸°í™”:
```bash
# Worker 1ì—ì„œ (SSH ì ‘ì† í›„)
sudo rm -rf /data/hdfs/datanode/*

# Worker 2ì—ì„œ (SSH ì ‘ì† í›„)
sudo rm -rf /data/hdfs/datanode/*

# DataNode ì¬ì‹œì‘
kubectl rollout restart daemonset/datanode -n log-pipeline
```

**K8s í™˜ê²½ì—ì„œ ë°ì´í„° ê²½ë¡œ:**
- hostPathë¡œ ë§ˆìš´íŠ¸ëœ ê²½ë¡œ: `/data/hdfs/datanode`
- ì´ ê²½ë¡œëŠ” Worker ë…¸ë“œì˜ ë¡œì»¬ íŒŒì¼ì‹œìŠ¤í…œ

**í™•ì¸:**
```bash
# DataNode ìƒíƒœ í™•ì¸
kubectl get pods -n log-pipeline | grep datanode

# HDFS í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
kubectl exec -n log-pipeline deployment/namenode -- hdfs dfsadmin -report
```

**ì˜ˆë°©:**
- NameNode PVCë¥¼ ì‚­ì œí•˜ì§€ ì•Šìœ¼ë©´ Cluster ID ìœ ì§€
- NameNode ì¬ìƒì„± ì‹œì—ëŠ” í•­ìƒ DataNode ë°ì´í„°ë„ í•¨ê»˜ ì‚­ì œ
- í”„ë¡œë•ì…˜ì—ì„œëŠ” NameNode ë©”íƒ€ë°ì´í„° ë°±ì—… í•„ìˆ˜

---

### ë¬¸ì œ 21: Spark Executorì—ì„œ Driver í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨ (K8s í™˜ê²½)

**ì¦ìƒ:**
```
Executor finished with state EXITED message Command exited with code 1

Caused by: java.io.IOException: Failed to connect to spark-master-5885b7bc-6lck4:33405
Caused by: java.net.UnknownHostException: spark-master-5885b7bc-6lck4
```

**ì›ì¸:**
- Spark Masterê°€ Driver URLì— Pod ì´ë¦„ ì‚¬ìš© (`spark-master-5885b7bc-6lck4`)
- Workerì—ì„œ Pod ì´ë¦„ì„ DNSë¡œ í•´ì„ ë¶ˆê°€
- K8sì—ì„œ Pod ì´ë¦„ì€ ìë™ìœ¼ë¡œ DNSì— ë“±ë¡ë˜ì§€ ì•ŠìŒ
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Spark Master (Pod: spark-master-5885b7bc-6lck4)            â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”‚ Driver URL: spark://...@spark-master-5885b7bc-6lck4 â”‚
â”‚       â–¼                                                      â”‚
â”‚  Spark Worker                                                â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”‚ DNS ì¡°íšŒ: spark-master-5885b7bc-6lck4               â”‚
â”‚       â–¼                                                      â”‚
â”‚  âŒ UnknownHostException                                    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•´ê²°:**
Headless Service + hostname/subdomainìœ¼ë¡œ Podì— DNS ì´ë¦„ ë¶€ì—¬:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-master
  namespace: log-pipeline
spec:
  template:
    spec:
      hostname: spark-master           # Pod hostname ì§€ì •
      subdomain: spark-headless        # Headless Serviceì™€ ì—°ê²°
      containers:
        - name: spark-master
          env:
            - name: SPARK_PUBLIC_DNS
              value: "spark-master.spark-headless.log-pipeline.svc.cluster.local"
---
apiVersion: v1
kind: Service
metadata:
  name: spark-headless
  namespace: log-pipeline
spec:
  selector:
    app: spark-master
  clusterIP: None      # Headless Service
  ports:
    - name: spark
      port: 7077
```

**Headless Service ì„¤ëª…:**
- `clusterIP: None`ìœ¼ë¡œ ì„¤ì •
- Podì— ì§ì ‘ ì ‘ê·¼ ê°€ëŠ¥í•œ DNS ìƒì„±
- í˜•ì‹: `{hostname}.{subdomain}.{namespace}.svc.cluster.local`
- ì˜ˆ: `spark-master.spark-headless.log-pipeline.svc.cluster.local`

**ê²°ê³¼:**
```
INFO Master: Starting Spark master at spark://spark-master:7077
INFO MasterWebUI: Bound MasterWebUI to spark-master.spark-headless.log-pipeline.svc.cluster.local:8080
INFO Master: Registering worker 10.42.2.11:42481 with 8 cores, 14.1 GiB RAM
```

---

## ğŸ—‘ï¸ Kubernetes ë¦¬ì†ŒìŠ¤ ê´€ë¦¬

### Namespace ì‚­ì œ ì‹œ ì˜í–¥ ë²”ìœ„
```
ì‚­ì œë˜ëŠ” ê²ƒ (namespace ì‚­ì œ):
â”œâ”€â”€ Podë“¤ (Kafka, HDFS, Spark, Backend ë“±)
â”œâ”€â”€ Serviceë“¤
â”œâ”€â”€ Deploymentë“¤
â”œâ”€â”€ ConfigMapë“¤
â””â”€â”€ PVCë“¤ (ë°ì´í„°)

ìœ ì§€ë˜ëŠ” ê²ƒ:
â”œâ”€â”€ k3s í´ëŸ¬ìŠ¤í„° ìì²´
â”œâ”€â”€ Master ë…¸ë“œ (k3s server)
â”œâ”€â”€ Worker ë…¸ë“œë“¤ (k3s agent) âœ…
â””â”€â”€ Workerì˜ ë¡œì»¬ ë°ì´í„° (/data/hdfs/datanode)
```

### ì „ì²´ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
```bash
# Namespace ì‚­ì œ (ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì •ë¦¬)
kubectl delete namespace log-pipeline

# ë…¸ë“œ ìƒíƒœ í™•ì¸ (Worker ì—°ê²° ìœ ì§€ë¨)
kubectl get nodes
```

### Worker ë¡œì»¬ ë°ì´í„° ì •ë¦¬ (í•„ìš”ì‹œ)

HDFS Cluster ID ë¶ˆì¼ì¹˜ ë“± ë¬¸ì œ ë°œìƒ ì‹œ Worker ë…¸ë“œì—ì„œ ì‹¤í–‰:
```bash
# Worker 1, 2ì—ì„œ ê°ê° ì‹¤í–‰
sudo rm -rf /data/hdfs/datanode/*
```

### ë¶„ì‚° í™˜ê²½ IP ì‚¬ìš© ìš”ì•½

`network_mode: host` ì‚¬ìš© ì‹œ Docker ì„œë¹„ìŠ¤ëª… ëŒ€ì‹  ì‹¤ì œ IPë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ëŠ” ê³³:

| íŒŒì¼ | ë³€ê²½ í•­ëª© |
|------|-----------|
| `spark-jobs/streaming/raw_to_hdfs.py` | `hdfs://192.168.55.114:9000`, `192.168.55.114:9092` |
| `spark-jobs/batch/daily_report.py` | `hdfs://192.168.55.114:9000` |
| `spark-jobs/batch/service_analysis.py` | `hdfs://192.168.55.114:9000` |
| `airflow/dags/manual_pipeline.py` | `spark://192.168.55.114:7077` |
| `airflow/dags/daily_pipeline.py` | `spark://192.168.55.114:7077` |
| `docker-compose.worker1.yml` | `SPARK_MASTER=spark://192.168.55.114:7077` |
| `docker-compose.worker2.yml` | `SPARK_MASTER=spark://192.168.55.114:7077` |
