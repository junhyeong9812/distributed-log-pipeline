# Distributed Log Pipeline

> ë¶„ì‚°ì²˜ë¦¬ ì‹œìŠ¤í…œ(Kubernetes, Hadoop, Spark)ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ì‹¤ìŠµ í”„ë¡œì íŠ¸

---

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

### ëª©ì 
Kubernetes, Hadoop, Sparkë¥¼ í™œìš©í•œ ë¶„ì‚° ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ì§ì ‘ êµ¬ì¶•í•˜ê³  ìš´ì˜í•´ë³´ë©° ë¶„ì‚°ì²˜ë¦¬ ì‹œìŠ¤í…œì˜ í•µì‹¬ ê°œë…ì„ í•™ìŠµí•©ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥
- ë°°ì¹˜ ê¸°ë°˜ ë¡œê·¸/ì´ë²¤íŠ¸ ë°ì´í„° ìë™ ìƒì„±
- ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ë° ë©”ì‹œì§€ í ì²˜ë¦¬
- ë¶„ì‚° ì €ì¥ì†Œ(HDFS)ì— ë°ì´í„° ì ì¬
- Sparkë¥¼ í™œìš©í•œ ì‹¤ì‹œê°„/ë°°ì¹˜ ë°ì´í„° ë¶„ì„
- Grafana ê¸°ë°˜ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ì „ì²´ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         System Architecture                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚       â”‚            ë¦¬ëˆ…ìŠ¤ ìš°ë¶„íˆ¬ - ë…¸íŠ¸ë¶ (Master)               â”‚
â”‚   Data          â”‚       â”‚                                         â”‚
â”‚   Generator     â”‚       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   Server        â”‚ â”€â”€â”€â”€â–¶ â”‚  â”‚   Java Backend (Spring Boot)      â”‚  â”‚
â”‚                 â”‚ HTTP  â”‚  â”‚   â€¢ REST API ìˆ˜ì§‘ ì„œë²„             â”‚  â”‚
â”‚  (Python)       â”‚       â”‚  â”‚   â€¢ Kafka Producer                â”‚  â”‚
â”‚  â€¢ FastAPI      â”‚       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â€¢ ë°°ì¹˜ ìŠ¤ì¼€ì¤„ëŸ¬ â”‚       â”‚                    â”‚                    â”‚
â”‚  â€¢ ë°ì´í„° ìƒì„±ê¸° â”‚       â”‚                    â–¼                    â”‚
â”‚                 â”‚       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚  â”‚   Kubernetes Control Plane        â”‚  â”‚
                          â”‚  â”‚   â€¢ API Server                    â”‚  â”‚
                          â”‚  â”‚   â€¢ Scheduler                     â”‚  â”‚
                          â”‚  â”‚   â€¢ etcd                          â”‚  â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                          â”‚                    â”‚                    â”‚
                          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                          â”‚  â”‚   Kafka (KRaft ëª¨ë“œ)               â”‚  â”‚
                          â”‚  â”‚   â€¢ ë©”ì‹œì§€ í                      â”‚  â”‚
                          â”‚  â”‚   â€¢ ì‹¤ì‹œê°„ ë°ì´í„° ë²„í¼ë§           â”‚  â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                          â”‚                    â”‚                    â”‚
                          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                          â”‚  â”‚   Spark Driver + Airflow          â”‚  â”‚
                          â”‚  â”‚   â€¢ ì‘ì—… ìŠ¤ì¼€ì¤„ë§                  â”‚  â”‚
                          â”‚  â”‚   â€¢ ë¶„ì„ Job ê´€ë¦¬                  â”‚  â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                          â”‚                    â”‚                    â”‚
                          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                          â”‚  â”‚   Monitoring Stack                â”‚  â”‚
                          â”‚  â”‚   â€¢ Grafana (ì‹œê°í™”)              â”‚  â”‚
                          â”‚  â”‚   â€¢ Prometheus (ë©”íŠ¸ë¦­ ìˆ˜ì§‘)       â”‚  â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚                                       â”‚
                          â–¼                                       â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   ë¦¬ëˆ…ìŠ¤ PC - A      â”‚             â”‚   ë¦¬ëˆ…ìŠ¤ PC - B      â”‚
               â”‚   (Worker Node 1)   â”‚             â”‚   (Worker Node 2)   â”‚
               â”‚                     â”‚             â”‚                     â”‚
               â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚             â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
               â”‚ â”‚ K8s Worker      â”‚ â”‚             â”‚ â”‚ K8s Worker      â”‚ â”‚
               â”‚ â”‚ (kubelet)       â”‚ â”‚             â”‚ â”‚ (kubelet)       â”‚ â”‚
               â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
               â”‚                     â”‚             â”‚                     â”‚
               â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚             â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
               â”‚ â”‚ HDFS DataNode   â”‚ â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ â”‚ HDFS DataNode   â”‚ â”‚
               â”‚ â”‚ (ë°ì´í„° ì €ì¥)    â”‚ â”‚  Replicationâ”‚ â”‚ (ë°ì´í„° ì €ì¥)    â”‚ â”‚
               â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
               â”‚                     â”‚             â”‚                     â”‚
               â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚             â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
               â”‚ â”‚ Spark Executor  â”‚ â”‚             â”‚ â”‚ Spark Executor  â”‚ â”‚
               â”‚ â”‚ (ë°ì´í„° ì²˜ë¦¬)    â”‚ â”‚             â”‚ â”‚ (ë°ì´í„° ì²˜ë¦¬)    â”‚ â”‚
               â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ë°ì´í„° í”Œë¡œìš°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Data Flow                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. ë°ì´í„° ìƒì„± (Python Generator)
   â”‚
   â”‚  ë°°ì¹˜ ìŠ¤ì¼€ì¤„ (ë§¤ Nì´ˆ/ë¶„ë§ˆë‹¤)
   â”‚  â€¢ ì‹œìŠ¤í…œ ë¡œê·¸ ìƒì„±
   â”‚  â€¢ ì‚¬ìš©ì í™œë™ ì´ë²¤íŠ¸ ìƒì„±
   â–¼
2. ë°ì´í„° ìˆ˜ì§‘ (Java Spring Boot)
   â”‚
   â”‚  REST APIë¡œ ìˆ˜ì‹ 
   â”‚  â€¢ ë°ì´í„° ê²€ì¦
   â”‚  â€¢ Kafkaë¡œ ë°œí–‰
   â–¼
3. ë©”ì‹œì§€ í (Kafka)
   â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                      â”‚                      â”‚
   â–¼                      â–¼                      â–¼
4a. ì‹¤ì‹œê°„ ì²˜ë¦¬       4b. ë°°ì¹˜ ì €ì¥          4c. HDFS ì ì¬
    (Spark Streaming)     (Kafka â†’ HDFS)         (ì›ë³¸ ë³´ê´€)
    â”‚                      â”‚                      â”‚
    â”‚ â€¢ 5ì´ˆ ìœˆë„ìš° ì§‘ê³„     â”‚ â€¢ ì‹œê°„ë³„ íŒŒí‹°ì…˜      â”‚ â€¢ ì¥ê¸° ë³´ê´€
    â”‚ â€¢ ì´ìƒ íƒì§€          â”‚ â€¢ ì••ì¶• ì €ì¥          â”‚ â€¢ ì¬ì²˜ë¦¬ìš©
    â–¼                      â–¼                      â–¼
5. ê²°ê³¼ ì €ì¥
   â”‚
   â”œâ”€â”€ Prometheus (ë©”íŠ¸ë¦­)
   â”œâ”€â”€ HDFS (ë¶„ì„ ê²°ê³¼)
   â””â”€â”€ PostgreSQL (ì§‘ê³„ ë°ì´í„°)
   â”‚
   â–¼
6. ì‹œê°í™” (Grafana)
   â”‚
   â€¢ ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ
   â€¢ ì•Œë¦¼ ì„¤ì •
```

---

## ğŸ–¥ï¸ PCë³„ ì—­í•  ë° êµ¬ì„±

### ì—­í•  ë¶„ë°°

| PC | ì—­í•  | ì£¼ìš” ì»´í¬ë„ŒíŠ¸ | ê¶Œì¥ ì‚¬ì–‘ |
|-----|------|-------------|----------|
| **ë…¸íŠ¸ë¶** | Master + Generator | K8s Control Plane, Java Backend, Kafka, Spark Driver, Monitoring | 8GB+ RAM, 4+ Core |
| **ë¦¬ëˆ…ìŠ¤ A** | Worker Node 1 | K8s Worker, HDFS DataNode, Spark Executor | 4GB+ RAM, 2+ Core |
| **ë¦¬ëˆ…ìŠ¤ B** | Worker Node 2 | K8s Worker, HDFS DataNode, Spark Executor | 4GB+ RAM, 2+ Core |

### ë„¤íŠ¸ì›Œí¬ êµ¬ì„±

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Network Topology                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         192.168.x.0/24 (ì˜ˆì‹œ)
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚             â”‚
    â–¼             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚Master â”‚    â”‚Worker1â”‚    â”‚Worker2â”‚
â”‚  .10  â”‚    â”‚  .11  â”‚    â”‚  .12  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”˜

í•„ìš” í¬íŠ¸:
â€¢ 6443  : Kubernetes API
â€¢ 9092  : Kafka
â€¢ 9870  : HDFS NameNode Web UI
â€¢ 8080  : Spark Master Web UI
â€¢ 3000  : Grafana
â€¢ 8081  : Java Backend API
```

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
distributed-log-pipeline/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml              # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©
â”œâ”€â”€ .env.example                    # í™˜ê²½ë³€ìˆ˜ í…œí”Œë¦¿
â”‚
â”œâ”€â”€ docs/                           # ë¬¸ì„œ
â”‚   â”œâ”€â”€ SETUP_MASTER.md            # ë§ˆìŠ¤í„° ë…¸ë“œ ì„¤ì • ê°€ì´ë“œ
â”‚   â”œâ”€â”€ SETUP_WORKER.md            # ì›Œì»¤ ë…¸ë“œ ì„¤ì • ê°€ì´ë“œ
â”‚   â”œâ”€â”€ TROUBLESHOOTING.md         # ë¬¸ì œ í•´ê²° ê°€ì´ë“œ
â”‚   â””â”€â”€ ARCHITECTURE.md            # ìƒì„¸ ì•„í‚¤í…ì²˜ ë¬¸ì„œ
â”‚
â”œâ”€â”€ generator/                      # ë°ì´í„° ìƒì„±ê¸° (Python)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                # FastAPI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”‚   â”‚   â”œâ”€â”€ scheduler.py           # ë°°ì¹˜ ìŠ¤ì¼€ì¤„ëŸ¬
â”‚   â”‚   â”œâ”€â”€ generators/
â”‚   â”‚   â”‚   â”œâ”€â”€ log_generator.py   # ë¡œê·¸ ë°ì´í„° ìƒì„±
â”‚   â”‚   â”‚   â””â”€â”€ event_generator.py # ì´ë²¤íŠ¸ ë°ì´í„° ìƒì„±
â”‚   â”‚   â””â”€â”€ config.py              # ì„¤ì •
â”‚   â””â”€â”€ tests/
â”‚
â”œâ”€â”€ backend/                        # ìˆ˜ì§‘ ì„œë²„ (Java Spring Boot)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ pom.xml
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ main/
â”‚   â”‚       â”œâ”€â”€ java/
â”‚   â”‚       â”‚   â””â”€â”€ com/pipeline/
â”‚   â”‚       â”‚       â”œâ”€â”€ PipelineApplication.java
â”‚   â”‚       â”‚       â”œâ”€â”€ controller/
â”‚   â”‚       â”‚       â”‚   â””â”€â”€ CollectorController.java
â”‚   â”‚       â”‚       â”œâ”€â”€ service/
â”‚   â”‚       â”‚       â”‚   â””â”€â”€ KafkaProducerService.java
â”‚   â”‚       â”‚       â”œâ”€â”€ model/
â”‚   â”‚       â”‚       â”‚   â”œâ”€â”€ LogEvent.java
â”‚   â”‚       â”‚       â”‚   â””â”€â”€ ActivityEvent.java
â”‚   â”‚       â”‚       â””â”€â”€ config/
â”‚   â”‚       â”‚           â””â”€â”€ KafkaConfig.java
â”‚   â”‚       â””â”€â”€ resources/
â”‚   â”‚           â””â”€â”€ application.yml
â”‚   â””â”€â”€ tests/
â”‚
â”œâ”€â”€ spark-jobs/                     # Spark ì‘ì—… (PySpark)
â”‚   â”œâ”€â”€ streaming/
â”‚   â”‚   â”œâ”€â”€ realtime_aggregator.py # ì‹¤ì‹œê°„ ì§‘ê³„
â”‚   â”‚   â””â”€â”€ anomaly_detector.py    # ì´ìƒ íƒì§€
â”‚   â”œâ”€â”€ batch/
â”‚   â”‚   â”œâ”€â”€ daily_report.py        # ì¼ë³„ ë¦¬í¬íŠ¸
â”‚   â”‚   â””â”€â”€ weekly_analysis.py     # ì£¼ê°„ ë¶„ì„
â”‚   â””â”€â”€ common/
â”‚       â””â”€â”€ spark_utils.py         # ê³µí†µ ìœ í‹¸
â”‚
â”œâ”€â”€ airflow/                        # ì›Œí¬í”Œë¡œìš° ìŠ¤ì¼€ì¤„ë§
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ daily_pipeline.py
â”‚   â”‚   â””â”€â”€ weekly_pipeline.py
â”‚   â””â”€â”€ plugins/
â”‚
â”œâ”€â”€ kubernetes/                     # K8s ë§¤ë‹ˆí˜ìŠ¤íŠ¸
â”‚   â”œâ”€â”€ namespace.yaml
â”‚   â”œâ”€â”€ master/                    # ë§ˆìŠ¤í„° ë…¸ë“œìš©
â”‚   â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â”‚   â”œâ”€â”€ zookeeper.yaml
â”‚   â”‚   â”‚   â””â”€â”€ kafka.yaml
â”‚   â”‚   â”œâ”€â”€ spark/
â”‚   â”‚   â”‚   â””â”€â”€ spark-master.yaml
â”‚   â”‚   â””â”€â”€ monitoring/
â”‚   â”‚       â”œâ”€â”€ prometheus.yaml
â”‚   â”‚       â””â”€â”€ grafana.yaml
â”‚   â”œâ”€â”€ worker/                    # ì›Œì»¤ ë…¸ë“œìš©
â”‚   â”‚   â”œâ”€â”€ hdfs/
â”‚   â”‚   â”‚   â””â”€â”€ datanode.yaml
â”‚   â”‚   â””â”€â”€ spark/
â”‚   â”‚       â””â”€â”€ spark-worker.yaml
â”‚   â””â”€â”€ common/                    # ê³µí†µ
â”‚       â”œâ”€â”€ configmaps.yaml
â”‚       â””â”€â”€ secrets.yaml
â”‚
â”œâ”€â”€ hadoop/                         # Hadoop ì„¤ì •
â”‚   â”œâ”€â”€ core-site.xml
â”‚   â”œâ”€â”€ hdfs-site.xml
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ start-namenode.sh
â”‚       â””â”€â”€ start-datanode.sh
â”‚
â”œâ”€â”€ monitoring/                     # ëª¨ë‹ˆí„°ë§ ì„¤ì •
â”‚   â”œâ”€â”€ grafana/
â”‚   â”‚   â”œâ”€â”€ provisioning/
â”‚   â”‚   â””â”€â”€ dashboards/
â”‚   â”‚       â”œâ”€â”€ overview.json      # ì „ì²´ í˜„í™© ëŒ€ì‹œë³´ë“œ
â”‚   â”‚       â”œâ”€â”€ kafka.json         # Kafka ëª¨ë‹ˆí„°ë§
â”‚   â”‚       â””â”€â”€ spark.json         # Spark ëª¨ë‹ˆí„°ë§
â”‚   â””â”€â”€ prometheus/
â”‚       â””â”€â”€ prometheus.yml
â”‚
â””â”€â”€ scripts/                        # ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸
    â”œâ”€â”€ setup-master.sh            # ë§ˆìŠ¤í„° ì´ˆê¸° ì„¤ì •
    â”œâ”€â”€ setup-worker.sh            # ì›Œì»¤ ì´ˆê¸° ì„¤ì •
    â”œâ”€â”€ deploy-all.sh              # ì „ì²´ ë°°í¬
    â””â”€â”€ cleanup.sh                 # ì •ë¦¬
```

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ì‚¬ì „ ìš”êµ¬ì‚¬í•­

**ëª¨ë“  PC ê³µí†µ:**
```bash
# Docker ì„¤ì¹˜
curl -fsSL https://get.docker.com | sh
sudo usermod -aG docker $USER

# kubectl ì„¤ì¹˜
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
```

### Step 1: í”„ë¡œì íŠ¸ í´ë¡  (ëª¨ë“  PC)

```bash
git clone https://github.com/your-repo/distributed-log-pipeline.git
cd distributed-log-pipeline
cp .env.example .env
# .env íŒŒì¼ì—ì„œ ê° PCì˜ IP ì£¼ì†Œ ì„¤ì •
```

### Step 2: Master ë…¸ë“œ ì„¤ì • (ë…¸íŠ¸ë¶)

```bash
# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export NODE_ROLE=master
export MASTER_IP=192.168.x.10  # ì‹¤ì œ IPë¡œ ë³€ê²½

# Kubernetes í´ëŸ¬ìŠ¤í„° ì´ˆê¸°í™”
./scripts/setup-master.sh

# ë§ˆìŠ¤í„° ì»´í¬ë„ŒíŠ¸ ë°°í¬
kubectl apply -f kubernetes/master/

# Java Backend ì‹¤í–‰
cd backend
./mvnw spring-boot:run

# ë°ì´í„° ìƒì„±ê¸° ì‹¤í–‰ (ë³„ë„ í„°ë¯¸ë„)
cd generator
pip install -r requirements.txt
python -m app.main
```

### Step 3: Worker ë…¸ë“œ ì„¤ì • (ë¦¬ëˆ…ìŠ¤ A, B)

```bash
# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export NODE_ROLE=worker
export MASTER_IP=192.168.x.10  # ë§ˆìŠ¤í„° IP

# í´ëŸ¬ìŠ¤í„° ì¡°ì¸
./scripts/setup-worker.sh

# ì›Œì»¤ ì»´í¬ë„ŒíŠ¸ ë°°í¬
kubectl apply -f kubernetes/worker/
```

### Step 4: ë™ì‘ í™•ì¸

```bash
# í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
kubectl get nodes
kubectl get pods -A

# HDFS ìƒíƒœ í™•ì¸
hdfs dfsadmin -report

# Kafka í† í”½ í™•ì¸
kafka-topics.sh --list --bootstrap-server localhost:9092

# Grafana ì ‘ì†
# ë¸Œë¼ìš°ì €ì—ì„œ http://<MASTER_IP>:3000 ì ‘ì†
# ê¸°ë³¸ ê³„ì •: admin / admin
```

---

## ğŸ“Š ìƒì„± ë°ì´í„° í˜•ì‹

### ì‹œìŠ¤í…œ ë¡œê·¸

```json
{
  "timestamp": "2024-01-15T10:30:00.123Z",
  "level": "INFO",
  "service": "api-gateway",
  "host": "worker-1",
  "message": "Request processed successfully",
  "metadata": {
    "request_id": "uuid-1234",
    "response_time_ms": 45,
    "status_code": 200,
    "endpoint": "/api/users",
    "method": "GET"
  }
}
```

### ì‚¬ìš©ì í™œë™ ì´ë²¤íŠ¸

```json
{
  "event_id": "evt-uuid-5678",
  "timestamp": "2024-01-15T10:30:05.456Z",
  "user_id": "user_12345",
  "session_id": "sess_abcde",
  "event_type": "CLICK",
  "event_data": {
    "page": "/products/123",
    "element": "add_to_cart_button",
    "position": {"x": 450, "y": 320}
  },
  "device": {
    "type": "mobile",
    "os": "iOS",
    "browser": "Safari"
  }
}
```

---

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

### ì œê³µë˜ëŠ” ëŒ€ì‹œë³´ë“œ

| ëŒ€ì‹œë³´ë“œ | ì„¤ëª… | ì£¼ìš” ë©”íŠ¸ë¦­ |
|---------|------|------------|
| **Overview** | ì‹œìŠ¤í…œ ì „ì²´ í˜„í™© | ë…¸ë“œ ìƒíƒœ, ì²˜ë¦¬ëŸ‰, ì—ëŸ¬ìœ¨ |
| **Kafka** | ë©”ì‹œì§€ í ëª¨ë‹ˆí„°ë§ | í† í”½ë³„ ì²˜ë¦¬ëŸ‰, ì§€ì—°ì‹œê°„, ì»¨ìŠˆë¨¸ ë™ |
| **Spark** | ì²˜ë¦¬ ì‘ì—… ëª¨ë‹ˆí„°ë§ | ì‘ì—… ìƒíƒœ, ì²˜ë¦¬ ì‹œê°„, ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ |
| **HDFS** | ì €ì¥ì†Œ ëª¨ë‹ˆí„°ë§ | ìš©ëŸ‰, ë³µì œ ìƒíƒœ, ë…¸ë“œ ìƒíƒœ |

### ì•Œë¦¼ ì„¤ì •

```yaml
# ê¸°ë³¸ ì œê³µ ì•Œë¦¼ ê·œì¹™
alerts:
  - name: HighErrorRate
    condition: error_rate > 5%
    duration: 5m
    
  - name: KafkaLag
    condition: consumer_lag > 10000
    duration: 10m
    
  - name: SparkJobFailed
    condition: job_status == "FAILED"
```

---

## ğŸ”§ ì£¼ìš” ì„¤ì •

### ë°°ì¹˜ ìŠ¤ì¼€ì¤„ ì„¤ì • (generator/app/config.py)

```python
BATCH_CONFIG = {
    "log_interval_seconds": 5,      # ë¡œê·¸ ìƒì„± ì£¼ê¸°
    "event_interval_seconds": 10,   # ì´ë²¤íŠ¸ ìƒì„± ì£¼ê¸°
    "batch_size": 100,              # ë°°ì¹˜ë‹¹ ë°ì´í„° ìˆ˜
    "target_backend_url": "http://master:8081/api/collect"
}
```

### Kafka í† í”½ ì„¤ì •

| í† í”½ | íŒŒí‹°ì…˜ | ë³µì œ ê³„ìˆ˜ | ë³´ì¡´ ê¸°ê°„ |
|------|--------|----------|----------|
| logs.raw | 3 | 2 | 7ì¼ |
| events.activity | 3 | 2 | 7ì¼ |
| alerts.realtime | 1 | 2 | 1ì¼ |

---

## ğŸ› ï¸ ê°œë°œ ê°€ì´ë“œ

### ë¡œì»¬ í…ŒìŠ¤íŠ¸ (Docker Compose)

```bash
# ì „ì²´ ìŠ¤íƒ ë¡œì»¬ ì‹¤í–‰ (ë‹¨ì¼ PC í…ŒìŠ¤íŠ¸ìš©)
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f

# ì¢…ë£Œ
docker-compose down -v
```

### ìƒˆë¡œìš´ Spark Job ì¶”ê°€

```python
# spark-jobs/batch/my_new_job.py

from pyspark.sql import SparkSession
from common.spark_utils import get_spark_session, read_from_hdfs

def main():
    spark = get_spark_session("MyNewJob")
    
    # HDFSì—ì„œ ë°ì´í„° ì½ê¸°
    df = read_from_hdfs(spark, "/data/logs/2024/01/15")
    
    # ì²˜ë¦¬ ë¡œì§
    result = df.groupBy("level").count()
    
    # ê²°ê³¼ ì €ì¥
    result.write.mode("overwrite").parquet("/data/results/my_job")

if __name__ == "__main__":
    main()
```

---

## ğŸ“š í•™ìŠµ ë¡œë“œë§µ

### Week 1-2: í™˜ê²½ êµ¬ì¶•
- [ ] 3ëŒ€ PC ë„¤íŠ¸ì›Œí¬ êµ¬ì„±
- [ ] Docker, kubectl ì„¤ì¹˜
- [ ] Kubernetes í´ëŸ¬ìŠ¤í„° êµ¬ì¶•
- [ ] ê¸°ë³¸ í†µì‹  í…ŒìŠ¤íŠ¸

### Week 3: ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê¸°ì´ˆ
- [ ] Kafka ì„¤ì¹˜ ë° í† í”½ ìƒì„±
- [ ] Java Backend ê°œë°œ
- [ ] Python Generator ê°œë°œ
- [ ] ê¸°ë³¸ ë°ì´í„° í”Œë¡œìš° í™•ì¸

### Week 4: HDFS ì—°ë™
- [ ] HDFS í´ëŸ¬ìŠ¤í„° êµ¬ì„±
- [ ] Kafka â†’ HDFS ì—°ë™
- [ ] ë°ì´í„° ì ì¬ í™•ì¸

### Week 5: Spark ì²˜ë¦¬
- [ ] Spark Streaming ì‘ì—… ê°œë°œ
- [ ] Batch ì‘ì—… ê°œë°œ
- [ ] Airflow ìŠ¤ì¼€ì¤„ë§

### Week 6: ëª¨ë‹ˆí„°ë§
- [ ] Prometheus + Grafana êµ¬ì„±
- [ ] ëŒ€ì‹œë³´ë“œ êµ¬ì„±
- [ ] ì•Œë¦¼ ì„¤ì •
- [ ] ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸

---

## â“ FAQ

### Q: ë‹¨ì¼ PCì—ì„œë„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆë‚˜ìš”?
A: ë„¤, `docker-compose up`ìœ¼ë¡œ ë¡œì»¬ì—ì„œ ì „ì²´ ìŠ¤íƒì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Q: ìµœì†Œ ì‚¬ì–‘ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?
A: Master 8GB RAM, Worker ê° 4GB RAMì„ ê¶Œì¥í•©ë‹ˆë‹¤. ë” ë‚®ì€ ì‚¬ì–‘ì—ì„œë„ ë™ì‘í•˜ì§€ë§Œ ì„±ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Q: Windowsì—ì„œë„ ê°€ëŠ¥í•œê°€ìš”?
A: WSL2 + Docker Desktop í™˜ê²½ì—ì„œ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë‹¨, ë„¤íŠ¸ì›Œí¬ ì„¤ì •ì´ ë³µì¡í•´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
---

## ğŸ”§ ê¸°ìˆ  ìƒì„¸ ì„¤ëª…

### PySparkëŠ” ì–´ë–»ê²Œ ë™ì‘í•˜ë‚˜ìš”?

SparkëŠ” Java/Scalaë¡œ ë§Œë“¤ì–´ì¡Œì§€ë§Œ, Pythonìœ¼ë¡œ ì‘ì„±í•œ ì½”ë“œë„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### ë™ì‘ êµ¬ì¡°
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PySpark êµ¬ì¡°                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚   Python ì½”ë“œ (ìš°ë¦¬ê°€ ì‘ì„±)                               â”‚
â”‚        â†“                                                 â”‚
â”‚   Py4J (Python â†” Java ë¸Œë¦¿ì§€)                            â”‚
â”‚        â†“                                                 â”‚
â”‚   Spark Core (Java/Scala - ì‹¤ì œ ì‹¤í–‰)                    â”‚
â”‚        â†“                                                 â”‚
â”‚   JVMì—ì„œ ë¶„ì‚° ì²˜ë¦¬                                       â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ì‹¤ì œ ì‹¤í–‰ íë¦„
```python
# ìš°ë¦¬ê°€ ì‘ì„±í•œ Python ì½”ë“œ
df.filter(col("level") == "ERROR").count()
```

ìœ„ ì½”ë“œê°€ ì‹¤í–‰ë˜ë©´:

1. **Python â†’ Py4J**: Python ì½”ë“œê°€ Py4J ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ Javaë¡œ ì „ë‹¬
2. **ì‹¤í–‰ ê³„íš ìƒì„±**: Spark(Java)ê°€ ìµœì í™”ëœ ì‹¤í–‰ ê³„íš ìƒì„±
3. **JVM ë¶„ì‚° ì²˜ë¦¬**: ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ëŠ” JVMì—ì„œ ë¶„ì‚° ì‹¤í–‰
4. **ê²°ê³¼ ë°˜í™˜**: ì²˜ë¦¬ ê²°ê³¼ë§Œ Pythonìœ¼ë¡œ ë°˜í™˜

#### ì„±ëŠ¥ ë¹„êµ

| êµ¬ë¶„ | PySpark | Scala/Java Spark |
|------|---------|------------------|
| **DataFrame API** | ê±°ì˜ ë™ì¼ | ê¸°ì¤€ |
| **RDD ì—°ì‚°** | ì•½ê°„ ëŠë¦¼ | ë¹ ë¦„ |
| **UDF (ì‚¬ìš©ì í•¨ìˆ˜)** | ëŠë¦¼ (ì§ë ¬í™” ì˜¤ë²„í—¤ë“œ) | ë¹ ë¦„ |
| **ê°œë°œ ì†ë„** | ë¹ ë¦„ | ëŠë¦¼ |

#### ì–¸ì œ ë¬´ì—‡ì„ ì„ íƒí• ê¹Œ?
```
DataFrame/SQL ìœ„ì£¼ ì‘ì—…     â†’ PySpark ì¶”ì²œ (ì¶©ë¶„í•œ ì„±ëŠ¥)
ë³µì¡í•œ ì»¤ìŠ¤í…€ ë¡œì§/UDF ë§ìŒ  â†’ Scala ê³ ë ¤
ML íŒŒì´í”„ë¼ì¸               â†’ PySpark (MLlib + Python ìƒíƒœê³„)
```

ë³¸ í”„ë¡œì íŠ¸ëŠ” DataFrame API ìœ„ì£¼ë¡œ ì‘ì—…í•˜ë¯€ë¡œ **PySparkë¡œ ì¶©ë¶„**í•©ë‹ˆë‹¤.

---

### Kafka í† í”½ íŒŒí‹°ì…˜ ì„¤ì • ì£¼ì˜ì‚¬í•­

#### ë¬¸ì œ ìƒí™©

Kafka í† í”½ì´ ì˜ë„í•œ íŒŒí‹°ì…˜ ìˆ˜ì™€ ë‹¤ë¥´ê²Œ ìƒì„±ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```
ì˜ˆìƒ: íŒŒí‹°ì…˜ 3ê°œë¡œ ë¶„ì‚° ì €ì¥
ì‹¤ì œ: íŒŒí‹°ì…˜ 1ê°œì— ëª¨ë“  ë°ì´í„° ì €ì¥
```

#### ì›ì¸
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ì„œë¹„ìŠ¤ ì‹œì‘ ìˆœì„œ ë¬¸ì œ                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Case 1: Generatorê°€ ë¨¼ì € ë°ì´í„° ì „ì†¡                    â”‚
â”‚  â†’ Kafkaê°€ í† í”½ ìë™ ìƒì„± (AUTO_CREATE_TOPICS=true)      â”‚
â”‚  â†’ ê¸°ë³¸ê°’ íŒŒí‹°ì…˜ 1ê°œë¡œ ìƒì„±                              â”‚
â”‚  â†’ Java Backendì˜ KafkaConfig ì„¤ì • ë¬´ì‹œë¨               â”‚
â”‚                                                          â”‚
â”‚  Case 2: Java Backendê°€ ë¨¼ì € ì—°ê²°                        â”‚
â”‚  â†’ KafkaConfig.javaì˜ NewTopic ì„¤ì • ì ìš©                â”‚
â”‚  â†’ íŒŒí‹°ì…˜ 3ê°œë¡œ í† í”½ ìƒì„±                                â”‚
â”‚  â†’ ë°ì´í„°ê°€ ì •ìƒì ìœ¼ë¡œ ë¶„ì‚° ì €ì¥                         â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### í•´ê²° ë°©ë²•

**ë°©ë²• 1: ì„œë¹„ìŠ¤ ì‹œì‘ ìˆœì„œ ë³´ì¥**
```yaml
# docker-compose.yml
generator:
  depends_on:
    backend:
      condition: service_healthy  # Backendê°€ healthy ìƒíƒœì¼ ë•Œë§Œ ì‹œì‘
```

**ë°©ë²• 2: í† í”½ ì‚¬ì „ ìƒì„±**
```bash
# Kafka ì»¨í…Œì´ë„ˆì—ì„œ ì§ì ‘ í† í”½ ìƒì„±
docker exec kafka /opt/kafka/bin/kafka-topics.sh \
  --create \
  --topic logs.raw \
  --partitions 3 \
  --replication-factor 1 \
  --bootstrap-server localhost:9092
```

**ë°©ë²• 3: ë³¼ë¥¨ ì´ˆê¸°í™” í›„ ì¬ì‹œì‘**
```bash
docker compose down -v  # ë³¼ë¥¨ê¹Œì§€ ì‚­ì œ
docker compose up -d    # ìƒˆë¡œ ì‹œì‘
```

#### íŒŒí‹°ì…˜ ë¶„ë°° ì›ë¦¬
```java
// KafkaProducerService.java
String key = logEvent.getService();  // key = service ì´ë¦„
kafkaTemplate.send(logsTopic, key, logEvent);
```

| Key (Service) | íŒŒí‹°ì…˜ í• ë‹¹ (hash % íŒŒí‹°ì…˜ìˆ˜) |
|---------------|------------------------------|
| api-gateway | 0, 1, ë˜ëŠ” 2 |
| user-service | 0, 1, ë˜ëŠ” 2 |
| order-service | 0, 1, ë˜ëŠ” 2 |

**ê°™ì€ keyëŠ” í•­ìƒ ê°™ì€ íŒŒí‹°ì…˜**ìœ¼ë¡œ ì „ì†¡ë˜ì–´ ë©”ì‹œì§€ ìˆœì„œê°€ ë³´ì¥ë©ë‹ˆë‹¤.

---

### Spark Streaming íŒŒí‹°ì…˜ ì €ì¥ ë¬¸ì œ í•´ê²°

#### ë¬¸ì œ í˜„ìƒ

HDFSì— ë°ì´í„°ê°€ ì €ì¥ë  ë•Œ íŒŒí‹°ì…˜ì´ ì œëŒ€ë¡œ ìƒì„±ë˜ì§€ ì•ŠëŠ” ë¬¸ì œ:
```
# ì˜ˆìƒí•œ ê²°ê³¼
/data/logs/raw/year=2026/month=1/day=11/hour=19/

# ì‹¤ì œ ê²°ê³¼
/data/logs/raw/year=__HIVE_DEFAULT_PARTITION__/month=__HIVE_DEFAULT_PARTITION__/...
```

#### ì›ì¸

Generatorê°€ ë³´ë‚´ëŠ” timestamp í˜•ì‹ê³¼ Sparkì—ì„œ íŒŒì‹±í•˜ëŠ” í˜•ì‹ì´ ë¶ˆì¼ì¹˜:
```python
# Generatorê°€ ë³´ë‚´ëŠ” í˜•ì‹ (Unix timestamp)
{
    "timestamp": 1768123166.291045000,  # epoch seconds
    ...
}

# ì²˜ìŒ Spark ì½”ë“œ (ISO í˜•ì‹ìœ¼ë¡œ íŒŒì‹± ì‹œë„)
.withColumn("event_time", to_timestamp(col("timestamp"), "yyyy-MM-dd'T'HH:mm:ss.SSSXXX"))
# â†’ íŒŒì‹± ì‹¤íŒ¨ â†’ null â†’ __HIVE_DEFAULT_PARTITION__
```

#### í•´ê²°

Unix timestampë¥¼ ì˜¬ë°”ë¥´ê²Œ íŒŒì‹±í•˜ë„ë¡ ìˆ˜ì •:
```python
# ìˆ˜ì • ì „ (ISO í˜•ì‹)
.withColumn("event_time", to_timestamp(col("timestamp"), "yyyy-MM-dd'T'HH:mm:ss.SSSXXX"))

# ìˆ˜ì • í›„ (Unix timestamp)
.withColumn("event_time", from_unixtime(col("timestamp")).cast("timestamp"))
```

ë˜í•œ ìŠ¤í‚¤ë§ˆì—ì„œ timestamp íƒ€ì…ë„ ë³€ê²½:
```python
# ìˆ˜ì • ì „
StructField("timestamp", StringType(), True)

# ìˆ˜ì • í›„
StructField("timestamp", DoubleType(), True)
```

#### ë””ë²„ê¹… ë°©ë²•
```bash
# 1. Generatorê°€ ë³´ë‚´ëŠ” ì‹¤ì œ ë°ì´í„° í˜•ì‹ í™•ì¸
curl -s http://<MASTER_IP>:8000/sample/log

# 2. Kafka UIì—ì„œ ë©”ì‹œì§€ ì§ì ‘ í™•ì¸
# http://<MASTER_IP>:8080 â†’ Topics â†’ logs.raw â†’ Messages
```

---

### Spark Job ë¦¬ì†ŒìŠ¤ ì ìœ  ë¬¸ì œ

#### ë¬¸ì œ í˜„ìƒ

Spark Job ì¬ì‹¤í–‰ ì‹œ ë¦¬ì†ŒìŠ¤ë¥¼ í• ë‹¹ë°›ì§€ ëª»í•˜ëŠ” ê²½ê³ :
```
WARN TaskSchedulerImpl: Initial job has not accepted any resources; 
check your cluster UI to ensure that workers are registered and have sufficient resources
```

#### ì›ì¸

ì´ì „ Spark Jobì´ ë¹„ì •ìƒ ì¢…ë£Œë˜ë©´ì„œ Workerì˜ ë¦¬ì†ŒìŠ¤ë¥¼ ê³„ì† ì ìœ í•˜ê³  ìˆìŒ.
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ë¦¬ì†ŒìŠ¤ ì ìœ  ìƒíƒœ                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  ì´ì „ Job (ì¢…ë£Œë¨)     í˜„ì¬ Job (ëŒ€ê¸°ì¤‘)                  â”‚
â”‚  â””â”€ Worker ë¦¬ì†ŒìŠ¤ ì ìœ   â””â”€ ë¦¬ì†ŒìŠ¤ í• ë‹¹ ëŒ€ê¸°               â”‚
â”‚     (24 cores, 1GB)       (í• ë‹¹ ë¶ˆê°€)                    â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### í•´ê²° ë°©ë²•

**ë°©ë²• 1: Spark ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘**
```bash
docker compose restart spark-master spark-worker
```

**ë°©ë²• 2: Spark Master UIì—ì„œ ì§ì ‘ ì¢…ë£Œ**
```
1. http://<MASTER_IP>:8082 ì ‘ì†
2. Running Applicationsì—ì„œ (kill) í´ë¦­
3. ìƒˆ Job ì‹¤í–‰
```

**ë°©ë²• 3: ì „ì²´ í´ëŸ¬ìŠ¤í„° ì¬ì‹œì‘**
```bash
docker compose down
docker compose up -d
```

#### ì˜ˆë°© ë°©ë²•

Spark Job ì¢…ë£Œ ì‹œ í•­ìƒ `Ctrl+C`ë¡œ ì •ìƒ ì¢…ë£Œí•˜ê³ , Spark UIì—ì„œ Applicationì´ Completedë¡œ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸.

---

### Airflow ì„¤ì • ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

#### Airflow ê¸°ë³¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì–¸ì–´** | Python |
| **ê¸°ë³¸ DB** | SQLite (ê°œë°œìš©) |
| **ìš´ì˜ DB** | PostgreSQL, MySQL (ê¶Œì¥) |

#### ë¬¸ì œ 1: DB ì´ˆê¸°í™” ì‹¤íŒ¨

**ì¦ìƒ:**
```
ERROR: You need to initialize the database. Please run `airflow db init`.
```

**ì›ì¸:**
- ì´ì „ ì‹¤í–‰ì—ì„œ DBê°€ ë¶ˆì™„ì „í•˜ê²Œ ìƒì„±ë¨
- `docker compose down`ì€ ë³¼ë¥¨ì„ ìœ ì§€í•˜ë¯€ë¡œ ë¶ˆì™„ì „í•œ DBê°€ ë‚¨ì•„ìˆìŒ

**í•´ê²°:**
```bash
# ë³¼ë¥¨ê¹Œì§€ ì‚­ì œí•´ì„œ ê¹¨ë—í•˜ê²Œ ì‹œì‘
docker compose down -v
docker compose up -d
```

#### ë¬¸ì œ 2: SQLite + LocalExecutor í˜¸í™˜ ë¶ˆê°€

**ì¦ìƒ:**
```
airflow.exceptions.AirflowConfigException: error: cannot use SQLite with the LocalExecutor
```

**ì›ì¸:**
- SQLiteëŠ” ë™ì‹œ ì“°ê¸°ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
- LocalExecutorëŠ” ë³‘ë ¬ ì‹¤í–‰ì„ ìœ„í•´ ë™ì‹œ DB ì ‘ê·¼ í•„ìš”
- ë”°ë¼ì„œ SQLite + LocalExecutor ì¡°í•©ì€ ë¶ˆê°€

**Executor ì¢…ë¥˜:**

| Executor | DB ìš”êµ¬ì‚¬í•­ | íŠ¹ì§• |
|----------|------------|------|
| **SequentialExecutor** | SQLite ê°€ëŠ¥ | ìˆœì°¨ ì‹¤í–‰, ê°œë°œìš© |
| **LocalExecutor** | PostgreSQL/MySQL í•„ìš” | ë³‘ë ¬ ì‹¤í–‰ |
| **CeleryExecutor** | PostgreSQL/MySQL + Redis | ë¶„ì‚° ì‹¤í–‰ |
| **KubernetesExecutor** | PostgreSQL/MySQL | K8s Podìœ¼ë¡œ ì‹¤í–‰ |

**í•´ê²°:**
```yaml
# docker-compose.yml
environment:
  # ìˆ˜ì • ì „ (ì—ëŸ¬ ë°œìƒ)
  - AIRFLOW__CORE__EXECUTOR=LocalExecutor
  
  # ìˆ˜ì • í›„ (ì •ìƒ ë™ì‘)
  - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
```

#### ë¬¸ì œ 3: DB ëª…ë ¹ì–´ ë²„ì „ ì°¨ì´

**ì¦ìƒ:**
```
DeprecationWarning: `db init` is deprecated. Use `db migrate` instead
```

**ì›ì¸:**
- Airflow 2.7.0ë¶€í„° `airflow db init`ì´ deprecated
- ìƒˆ ë²„ì „ì—ì„œëŠ” `airflow db migrate` ì‚¬ìš© ê¶Œì¥

**í•´ê²°:**
```yaml
# ë‘ ëª…ë ¹ì–´ ëª¨ë‘ ì‹¤í–‰í•˜ì—¬ í˜¸í™˜ì„± í™•ë³´
command: bash -c "airflow db init && airflow users create ... "
```

#### ìµœì¢… Airflow ì„¤ì • (docker-compose.yml)
```yaml
airflow:
  image: apache/airflow:2.7.0-python3.10
  container_name: airflow
  user: root
  ports:
    - "8084:8080"
  environment:
    - AIRFLOW__CORE__EXECUTOR=SequentialExecutor  # SQLiteì™€ í˜¸í™˜
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
    - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
    - AIRFLOW__CORE__LOAD_EXAMPLES=False
    - AIRFLOW__WEBSERVER__SECRET_KEY=mysecretkey123
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./spark-jobs:/opt/spark-jobs
    - /var/run/docker.sock:/var/run/docker.sock  # Docker ëª…ë ¹ ì‹¤í–‰ìš©
  command: bash -c "airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true && (airflow webserver &) && airflow scheduler"
```

#### Airflow ì ‘ì† ì •ë³´
```
URL: http://<MASTER_IP>:8084
ID: admin
PW: admin
```

---

## ğŸ–¥ï¸ ë¶„ì‚° í™˜ê²½ êµ¬ì„±

### ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       ë¶„ì‚° í´ëŸ¬ìŠ¤í„° êµ¬ì„±                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             ë…¸íŠ¸ë¶ - Master (192.168.55.114)             â”‚
    â”‚                                                          â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚  Kafka   â”‚ â”‚ NameNode â”‚ â”‚  Spark   â”‚ â”‚ Airflow  â”‚   â”‚
    â”‚  â”‚          â”‚ â”‚  (HDFS)  â”‚ â”‚  Master  â”‚ â”‚          â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                                                          â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚ Backend  â”‚ â”‚Generator â”‚ â”‚ Grafana  â”‚ â”‚Prometheusâ”‚   â”‚
    â”‚  â”‚  (Java)  â”‚ â”‚ (Python) â”‚ â”‚          â”‚ â”‚          â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                               â”‚
              â–¼                               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ë¦¬ëˆ…ìŠ¤ A - Worker 1  â”‚       â”‚ ë¦¬ëˆ…ìŠ¤ B - Worker 2  â”‚
    â”‚  (192.168.55.158)   â”‚       â”‚   (192.168.55.9)    â”‚
    â”‚                     â”‚       â”‚                     â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚   DataNode    â”‚  â”‚       â”‚  â”‚   DataNode    â”‚  â”‚
    â”‚  â”‚    (HDFS)     â”‚  â”‚       â”‚  â”‚    (HDFS)     â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚                     â”‚       â”‚                     â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚ Spark Worker  â”‚  â”‚       â”‚  â”‚ Spark Worker  â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### PCë³„ ì—­í•  ë° ì„œë¹„ìŠ¤

| PC | IP | ì—­í•  | ì„œë¹„ìŠ¤ | í¬íŠ¸ |
|----|-----|------|--------|------|
| **ë…¸íŠ¸ë¶** | 192.168.55.114 | Master | Kafka | 9092 |
| | | | Kafka UI | 8080 |
| | | | HDFS NameNode | 9870, 9000 |
| | | | Spark Master | 8082, 7077 |
| | | | Airflow | 8084 |
| | | | Grafana | 3000 |
| | | | Prometheus | 9090 |
| | | | Backend | 8081 |
| | | | Generator | 8000 |
| **ë¦¬ëˆ…ìŠ¤ A** | 192.168.55.158 | Worker 1 | HDFS DataNode | 9864 |
| | | | Spark Worker | 8081 |
| **ë¦¬ëˆ…ìŠ¤ B** | 192.168.55.9 | Worker 2 | HDFS DataNode | 9864 |
| | | | Spark Worker | 8081 |

### ë°°í¬ íŒŒì¼ êµ¬ì¡°
```
deploy/
â”œâ”€â”€ docker-compose.master.yml   # Master ë…¸ë“œìš©
â””â”€â”€ docker-compose.worker.yml   # Worker ë…¸ë“œìš©

scripts/
â”œâ”€â”€ setup-master.sh             # Master ì´ˆê¸° ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ setup-worker.sh             # Worker ì´ˆê¸° ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
```

### ë¹ ë¥¸ ì‹œì‘

#### Master ë…¸ë“œ (ë…¸íŠ¸ë¶)
```bash
# 1. ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (ìµœì´ˆ 1íšŒ)
./scripts/setup-master.sh

# 2. Backend ë¹Œë“œ
cd backend && ./gradlew build && cd ..

# 3. ì„œë¹„ìŠ¤ ì‹œì‘
cd deploy
docker compose -f docker-compose.master.yml up -d --build
```

#### Worker ë…¸ë“œ (ë¦¬ëˆ…ìŠ¤ A, B)
```bash
# 1. ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (ìµœì´ˆ 1íšŒ)
./scripts/setup-worker.sh

# 2. ì„œë¹„ìŠ¤ ì‹œì‘
cd deploy
docker compose -f docker-compose.worker.yml up -d
```

### ì—°ê²° í™•ì¸

#### HDFS í´ëŸ¬ìŠ¤í„° ìƒíƒœ
```bash
# Masterì—ì„œ ì‹¤í–‰
docker exec namenode hdfs dfsadmin -report
```

ì˜ˆìƒ ì¶œë ¥:
```
Live datanodes (2):
  Name: 192.168.55.158:9866
  Name: 192.168.55.9:9866
```

#### Spark í´ëŸ¬ìŠ¤í„° ìƒíƒœ
```bash
# Spark Master UI í™•ì¸
http://192.168.55.114:8082
```

ì˜ˆìƒ: Workers (2) í‘œì‹œ

### ë‹¨ì¼ PC vs ë¶„ì‚° í™˜ê²½

| êµ¬ë¶„ | ë‹¨ì¼ PC (ê°œë°œìš©) | ë¶„ì‚° í™˜ê²½ (ìš´ì˜ìš©) |
|------|-----------------|------------------|
| **docker-compose íŒŒì¼** | docker-compose.yml | docker-compose.master.yml + docker-compose.worker.yml |
| **HDFS ë³µì œ** | DataNode 2ê°œ (ê°™ì€ PC) | DataNode 2ê°œ (ë‹¤ë¥¸ PC) |
| **Spark** | Master + Worker 1ê°œ | Master + Worker 2ê°œ |
| **ì¥ì•  ëŒ€ì‘** | ë¶ˆê°€ | 1ëŒ€ ì¥ì•  ì‹œ ê³„ì† ìš´ì˜ |
| **ì„±ëŠ¥** | ë¦¬ì†ŒìŠ¤ ê²½ìŸ | ë¦¬ì†ŒìŠ¤ ë¶„ì‚° |

### ì£¼ì˜ì‚¬í•­

1. **ë„¤íŠ¸ì›Œí¬**: ëª¨ë“  PCê°€ ê°™ì€ ë„¤íŠ¸ì›Œí¬(192.168.55.x)ì— ìˆì–´ì•¼ í•¨
2. **ë°©í™”ë²½**: í•„ìš”í•œ í¬íŠ¸ê°€ ì—´ë ¤ìˆì–´ì•¼ í•¨
3. **ì‹œê°„ ë™ê¸°í™”**: ëª¨ë“  PCì˜ ì‹œê°„ì´ ë™ê¸°í™”ë˜ì–´ì•¼ í•¨ (NTP ì‚¬ìš© ê¶Œì¥)
4. **Master ë¨¼ì € ì‹œì‘**: WorkerëŠ” Masterê°€ ì‹¤í–‰ëœ í›„ ì‹œì‘í•´ì•¼ ì—°ê²°ë¨

### ì„œë¹„ìŠ¤ URL ìš”ì•½

| ì„œë¹„ìŠ¤ | URL | ìš©ë„ |
|--------|-----|------|
| Kafka UI | http://192.168.55.114:8080 | Kafka í† í”½/ë©”ì‹œì§€ ëª¨ë‹ˆí„°ë§ |
| HDFS | http://192.168.55.114:9870 | ë¶„ì‚° íŒŒì¼ ì‹œìŠ¤í…œ ê´€ë¦¬ |
| Spark | http://192.168.55.114:8082 | Spark ì‘ì—… ëª¨ë‹ˆí„°ë§ |
| Airflow | http://192.168.55.114:8084 | ë°°ì¹˜ ì‘ì—… ìŠ¤ì¼€ì¤„ë§ |
| Grafana | http://192.168.55.114:3000 | ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ |
| Prometheus | http://192.168.55.114:9090 | ë©”íŠ¸ë¦­ ìˆ˜ì§‘/ì¿¼ë¦¬ |

---

### ë¶„ì‚° í™˜ê²½ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

#### ë¬¸ì œ: DataNodeê°€ NameNodeì— ì—°ê²° ì•ˆ ë¨

**ì¦ìƒ:**
```bash
docker exec namenode hdfs dfsadmin -report
# Live datanodes (0) - DataNodeê°€ ì—†ìŒ
```

**Worker ë¡œê·¸ ì—ëŸ¬:**
```
ERROR datanode.DataNode: Initialization failed for Block pool...
Datanode denied communication with namenode because hostname cannot be resolved
(ip=192.168.55.158, hostname=192.168.55.158)
```

**ì›ì¸:**
- NameNodeê°€ DataNodeì˜ IPë¥¼ í˜¸ìŠ¤íŠ¸ëª…ìœ¼ë¡œ ì—­ë°©í–¥ DNS ì¡°íšŒ ì‹œë„
- í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨ë¡œ ì—°ê²° ê±°ë¶€

**í•´ê²°:**
docker-compose.master.ymlì˜ namenode ì„¤ì •ì— ì¶”ê°€:
```yaml
namenode:
  environment:
    # ê¸°ì¡´ ì„¤ì •...
    - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
```

> ì°¸ê³ : í™˜ê²½ë³€ìˆ˜ì—ì„œ `.`ì€ `_`ë¡œ, `-`ëŠ” `___`ë¡œ ë³€í™˜ë¨
> `dfs.namenode.datanode.registration.ip-hostname-check` â†’ `HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check`

**ì ìš©:**
```bash
# Master ì¬ì‹œì‘
docker compose -f docker-compose.master.yml down
docker compose -f docker-compose.master.yml up -d

# Worker ì¬ì‹œì‘
docker compose -f docker-compose.worker.yml down
docker compose -f docker-compose.worker.yml up -d
```

**í™•ì¸:**
```bash
docker exec namenode hdfs dfsadmin -report
# Live datanodes (2) í™•ì¸
```

---

#### ë¬¸ì œ: Worker í¬íŠ¸ ì¶©ëŒ

**ì¦ìƒ:**
```
Error response from daemon: Bind for 0.0.0.0:8081 failed: port is already allocated
```

**ì›ì¸:**
- Worker PCì—ì„œ 8081 í¬íŠ¸ê°€ ì´ë¯¸ ì‚¬ìš© ì¤‘

**í•´ê²°:**
docker-compose.worker.ymlì—ì„œ í¬íŠ¸ ë³€ê²½:
```yaml
spark-worker:
  ports:
    - "10000:8081"   # ì™¸ë¶€ í¬íŠ¸ë¥¼ 10000ìœ¼ë¡œ ë³€ê²½
```

ë˜ëŠ” ëª…ë ¹ì–´ë¡œ:
```bash
sed -i 's/8081:8081/10000:8081/' docker-compose.worker.yml
```

---

#### ë¬¸ì œ: Sparkì—ì„œ HDFS DataNode ì—°ê²° ì‹¤íŒ¨

**ì¦ìƒ:**
```
WARN DataStreamer: Exception in createBlockOutputStream
java.net.ConnectException: Connection refused
WARN DataStreamer: Excluding datanode DatanodeInfoWithStorage[192.168.55.9:9866...]
ERROR: File could only be written to 0 of the 1 minReplication nodes.
There are 2 datanode(s) running and 2 node(s) are excluded in this operation.
```

**ì›ì¸:**
- NameNode â†” DataNode í†µì‹ ì€ ì •ìƒ (ë©”íƒ€ë°ì´í„°)
- Spark â†’ DataNode ì§ì ‘ ë°ì´í„° ì“°ê¸° ì‹œ 9866 í¬íŠ¸ í•„ìš”
- Workerì˜ docker-composeì—ì„œ 9866 í¬íŠ¸ê°€ ë…¸ì¶œë˜ì§€ ì•ŠìŒ
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ë°ì´í„° íë¦„                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  [NameNode] â†â”€â”€â”€â”€ ë©”íƒ€ë°ì´í„° â”€â”€â”€â”€â†’ [DataNode]               â”‚
â”‚      â†‘              (ì •ìƒ)            :9866                  â”‚
â”‚      â”‚                                  â†‘                    â”‚
â”‚  ë©”íƒ€ë°ì´í„°                              â”‚                    â”‚
â”‚   ì¡°íšŒ                            Connection refused         â”‚
â”‚      â”‚                                  â”‚                    â”‚
â”‚  [Spark] â”€â”€â”€â”€â”€â”€â”€ ë°ì´í„° ì“°ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                  (í¬íŠ¸ ë¯¸ë…¸ì¶œ)                               â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•´ê²°:**
docker-compose.worker.ymlì—ì„œ DataNode í¬íŠ¸ ì¶”ê°€ ë…¸ì¶œ:
```yaml
datanode:
  image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
  container_name: datanode
  ports:
    - "9864:9864"   # HTTP (Web UI)
    - "9866:9866"   # ë°ì´í„° ì „ì†¡ â† í•„ìˆ˜!
    - "9867:9867"   # IPC
  environment:
    - CORE_CONF_fs_defaultFS=hdfs://192.168.55.114:9000
    - HDFS_CONF_dfs_replication=2
    - HDFS_CONF_dfs_datanode_address=0.0.0.0:9866
    - HDFS_CONF_dfs_datanode_http_address=0.0.0.0:9864
    - HDFS_CONF_dfs_datanode_ipc_address=0.0.0.0:9867
```

**DataNode í¬íŠ¸ ì„¤ëª…:**

| í¬íŠ¸ | ìš©ë„ | í•„ìˆ˜ ì—¬ë¶€ |
|------|------|----------|
| 9864 | Web UI (HTTP) | ì„ íƒ |
| 9866 | ë°ì´í„° ì „ì†¡ | **í•„ìˆ˜** |
| 9867 | IPC í†µì‹  | ê¶Œì¥ |

**ì ìš©:**
```bash
# Workerì—ì„œ ì‹¤í–‰ (ë¦¬ëˆ…ìŠ¤ A, B ë‘˜ ë‹¤)
docker compose -f docker-compose.worker.yml down
docker compose -f docker-compose.worker.yml up -d
```

**í™•ì¸:**
```bash
# í¬íŠ¸ ë…¸ì¶œ í™•ì¸
docker compose -f docker-compose.worker.yml ps
# 9866 í¬íŠ¸ê°€ ë³´ì—¬ì•¼ í•¨
```

---

### ë¶„ì‚° í™˜ê²½ HDFS íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ìš”ì•½

ë¶„ì‚° í™˜ê²½ì—ì„œ HDFS êµ¬ì„± ì‹œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œë“¤ì…ë‹ˆë‹¤.

#### ë¬¸ì œ 1: DataNodeê°€ NameNodeì— ë“±ë¡ ì‹¤íŒ¨

**ì¦ìƒ:**
```bash
docker exec namenode hdfs dfsadmin -report
# Live datanodes (0) - DataNodeê°€ ì—†ìŒ
```

**Worker ë¡œê·¸:**
```
ERROR datanode.DataNode: Initialization failed for Block pool...
Datanode denied communication with namenode because hostname cannot be resolved
(ip=192.168.55.158, hostname=192.168.55.158)
```

**ì›ì¸:**
- NameNodeê°€ DataNodeì˜ IPë¥¼ í˜¸ìŠ¤íŠ¸ëª…ìœ¼ë¡œ ì—­ë°©í–¥ DNS ì¡°íšŒ ì‹œë„
- ë¶„ì‚° í™˜ê²½ì—ì„œ DNS ì„œë²„ê°€ ì—†ìœ¼ë©´ í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨

**í•´ê²°:**
docker-compose.master.ymlì˜ namenodeì— ì„¤ì • ì¶”ê°€:
```yaml
namenode:
  environment:
    # ê¸°ì¡´ ì„¤ì •...
    - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
```

> í™˜ê²½ë³€ìˆ˜ ë³€í™˜ ê·œì¹™:
> - `.` â†’ `_`
> - `-` â†’ `___`
> - `dfs.namenode.datanode.registration.ip-hostname-check`
> - â†’ `HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check`

---

#### ë¬¸ì œ 2: Sparkì—ì„œ DataNode ì—°ê²° ì‹¤íŒ¨

**ì¦ìƒ:**
```
WARN DataStreamer: Exception in createBlockOutputStream
java.net.ConnectException: Connection refused
WARN DataStreamer: Excluding datanode DatanodeInfoWithStorage[192.168.55.9:9866...]
ERROR: File could only be written to 0 of the 1 minReplication nodes.
There are 2 datanode(s) running and 2 node(s) are excluded in this operation.
```

**ì›ì¸:**
- NameNode â†” DataNode ë©”íƒ€ë°ì´í„° í†µì‹ ì€ ì •ìƒ
- Spark â†’ DataNode ì§ì ‘ ë°ì´í„° ì“°ê¸° ì‹œ 9866 í¬íŠ¸ í•„ìš”
- Workerì˜ docker-composeì—ì„œ 9866 í¬íŠ¸ê°€ ë…¸ì¶œë˜ì§€ ì•ŠìŒ
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HDFS ë°ì´í„° íë¦„                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  [Client/Spark]                                              â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”‚ 1. íŒŒì¼ ìƒì„± ìš”ì²­                                    â”‚
â”‚       â–¼                                                      â”‚
â”‚  [NameNode]                                                  â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”‚ 2. DataNode ìœ„ì¹˜ ë°˜í™˜                                â”‚
â”‚       â–¼                                                      â”‚
â”‚  [Client/Spark]                                              â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”‚ 3. DataNodeì— ì§ì ‘ ë°ì´í„° ì“°ê¸° (:9866)              â”‚
â”‚       â–¼                                                      â”‚
â”‚  [DataNode] â† ì—¬ê¸°ì„œ Connection refused ë°œìƒ                â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•´ê²°:**
docker-compose.worker.ymlì—ì„œ DataNode í¬íŠ¸ ì¶”ê°€:
```yaml
datanode:
  ports:
    - "9864:9864"   # HTTP (Web UI)
    - "9866:9866"   # ë°ì´í„° ì „ì†¡ â† í•„ìˆ˜!
    - "9867:9867"   # IPC
  environment:
    - HDFS_CONF_dfs_datanode_address=0.0.0.0:9866
    - HDFS_CONF_dfs_datanode_http_address=0.0.0.0:9864
    - HDFS_CONF_dfs_datanode_ipc_address=0.0.0.0:9867
```

---

#### ë¬¸ì œ 3: Worker í¬íŠ¸ ì¶©ëŒ

**ì¦ìƒ:**
```
Error response from daemon: Bind for 0.0.0.0:8081 failed: port is already allocated
```

**ì›ì¸:**
- Worker PCì—ì„œ 8081 í¬íŠ¸ê°€ ì´ë¯¸ ì‚¬ìš© ì¤‘

**í•´ê²°:**
docker-compose.worker.ymlì—ì„œ í¬íŠ¸ ë³€ê²½:
```yaml
spark-worker:
  ports:
    - "10000:8081"   # ì™¸ë¶€ í¬íŠ¸ë¥¼ 10000ìœ¼ë¡œ ë³€ê²½
```

---

#### ë¶„ì‚° í™˜ê²½ ì²´í¬ë¦¬ìŠ¤íŠ¸

| í•­ëª© | Master ì„¤ì • | Worker ì„¤ì • |
|------|------------|-------------|
| **hostname ì²´í¬ ë¹„í™œì„±í™”** | `HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false` | - |
| **DataNode ë°ì´í„° í¬íŠ¸** | - | `ports: 9866:9866` |
| **DataNode ì£¼ì†Œ ë°”ì¸ë”©** | - | `HDFS_CONF_dfs_datanode_address=0.0.0.0:9866` |
| **NameNode ì£¼ì†Œ** | `CORE_CONF_fs_defaultFS=hdfs://192.168.55.114:9000` | `CORE_CONF_fs_defaultFS=hdfs://192.168.55.114:9000` |

#### ì—°ê²° í™•ì¸ ëª…ë ¹ì–´
```bash
# 1. DataNode ë“±ë¡ í™•ì¸ (Masterì—ì„œ)
docker exec namenode hdfs dfsadmin -report

# 2. HDFS íŒŒì¼ ì“°ê¸° í…ŒìŠ¤íŠ¸ (Masterì—ì„œ)
docker exec namenode hdfs dfs -mkdir -p /test
docker exec namenode hdfs dfs -touchz /test/hello.txt
docker exec namenode hdfs dfs -ls /test

# 3. Spark Worker í™•ì¸
http://192.168.55.114:8082
```

---

#### ë¬¸ì œ 4: Docker ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬ë¡œ ì¸í•œ ì—°ê²° ë¬¸ì œ

**ì¦ìƒ:**
```
WARN DataStreamer: Exception in createBlockOutputStream
java.io.EOFException: Unexpected EOF while trying to read response from server
WARN TaskSchedulerImpl: Initial job has not accepted any resources; 
check your cluster UI to ensure that workers are registered and have sufficient resources
```

**ì›ì¸:**
- Docker ê¸°ë³¸ bridge ë„¤íŠ¸ì›Œí¬ëŠ” ì»¨í…Œì´ë„ˆë¥¼ ê²©ë¦¬í•¨
- Masterì™€ Workerê°€ ì„œë¡œ ë‹¤ë¥¸ PCì— ìˆì„ ë•Œ Docker ë‚´ë¶€ IPë¡œ í†µì‹  ì‹œë„
- DataNode/Spark Workerê°€ ë‚´ë¶€ IP(172.x.x.x)ë¥¼ Masterì— ë³´ê³ 
- Masterê°€ í•´ë‹¹ ë‚´ë¶€ IPë¡œ ì ‘ê·¼ ì‹œë„ â†’ ì‹¤íŒ¨
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ìƒí™©                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Master (192.168.55.114)                                     â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”‚ DataNode ìœ„ì¹˜ ì¡°íšŒ                                   â”‚
â”‚       â–¼                                                      â”‚
â”‚  NameNode: "DataNodeëŠ” 172.19.0.3:9866ì— ìˆì–´"              â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”‚ 172.19.0.3:9866 ì ‘ê·¼ ì‹œë„                           â”‚
â”‚       â–¼                                                      â”‚
â”‚  âŒ ì‹¤íŒ¨ (172.x.x.xëŠ” Worker PC ë‚´ë¶€ Docker IP)             â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•´ê²°:**
Workerì˜ docker-compose.worker.ymlì—ì„œ `network_mode: host` ì‚¬ìš©:
```yaml
services:
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://192.168.55.114:9000
      - HDFS_CONF_dfs_replication=2
      - HDFS_CONF_dfs_datanode_address=0.0.0.0:9866
      - HDFS_CONF_dfs_datanode_http_address=0.0.0.0:9864
      - HDFS_CONF_dfs_datanode_ipc_address=0.0.0.0:9867
      - HDFS_CONF_dfs_datanode_use_datanode_hostname=false
      - HDFS_CONF_dfs_client_use_datanode_hostname=false
    volumes:
      - datanode_data:/hadoop/dfs/data
    network_mode: host    # â† í•µì‹¬!

  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker
    environment:
      - SPARK_MASTER=spark://192.168.55.114:7077
    volumes:
      - ./spark-jobs:/opt/spark-jobs
    network_mode: host    # â† í•µì‹¬!

volumes:
  datanode_data:
```

**network_mode: host ì„¤ëª…:**

| ëª¨ë“œ | ì„¤ëª… | IP ì˜ˆì‹œ |
|------|------|---------|
| bridge (ê¸°ë³¸) | Docker ê°€ìƒ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš© | 172.19.0.3 |
| host | í˜¸ìŠ¤íŠ¸ ë„¤íŠ¸ì›Œí¬ ì§ì ‘ ì‚¬ìš© | 192.168.55.158 |

**ì£¼ì˜ì‚¬í•­:**
- `network_mode: host` ì‚¬ìš© ì‹œ `ports` ì„¤ì • ë¶ˆí•„ìš” (ì¶©ëŒ ë°œìƒ)
- ì»¨í…Œì´ë„ˆê°€ í˜¸ìŠ¤íŠ¸ì˜ ëª¨ë“  í¬íŠ¸ì— ì§ì ‘ ë°”ì¸ë”©ë¨
- Linuxì—ì„œë§Œ ì§€ì› (Mac/Windows ë¯¸ì§€ì›)

**ì ìš©:**
```bash
# Workerì—ì„œ ì‹¤í–‰ (ë¦¬ëˆ…ìŠ¤ A, B ë‘˜ ë‹¤)
docker compose -f docker-compose.worker.yml down
docker compose -f docker-compose.worker.yml up -d
```

**í™•ì¸:**
```bash
# Masterì—ì„œ DataNode í™•ì¸
docker exec namenode hdfs dfsadmin -report
# Live datanodesì— ì‹¤ì œ IP(192.168.55.x)ê°€ ë³´ì—¬ì•¼ í•¨

# Spark UIì—ì„œ Worker í™•ì¸
http://192.168.55.114:8082
# Workersì— ì‹¤ì œ IPê°€ ë³´ì—¬ì•¼ í•¨
```

---

#### ë¬¸ì œ 4: Docker ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬ë¡œ ì¸í•œ ì—°ê²° ë¬¸ì œ

**ì¦ìƒ:**
```
WARN DataStreamer: Exception in createBlockOutputStream
java.io.EOFException: Unexpected EOF while trying to read response from server
WARN TaskSchedulerImpl: Initial job has not accepted any resources; 
check your cluster UI to ensure that workers are registered and have sufficient resources
```

**ì›ì¸:**
- Docker ê¸°ë³¸ bridge ë„¤íŠ¸ì›Œí¬ëŠ” ì»¨í…Œì´ë„ˆë¥¼ ê²©ë¦¬í•¨
- Masterì™€ Workerê°€ ì„œë¡œ ë‹¤ë¥¸ PCì— ìˆì„ ë•Œ Docker ë‚´ë¶€ IPë¡œ í†µì‹  ì‹œë„
- DataNode/Spark Workerê°€ ë‚´ë¶€ IP(172.x.x.x)ë¥¼ Masterì— ë³´ê³ 
- Masterê°€ í•´ë‹¹ ë‚´ë¶€ IPë¡œ ì ‘ê·¼ ì‹œë„ â†’ ì‹¤íŒ¨
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ìƒí™©                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Master (192.168.55.114)                                     â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”‚ DataNode ìœ„ì¹˜ ì¡°íšŒ                                   â”‚
â”‚       â–¼                                                      â”‚
â”‚  NameNode: "DataNodeëŠ” 172.19.0.3:9866ì— ìˆì–´"              â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”‚ 172.19.0.3:9866 ì ‘ê·¼ ì‹œë„                           â”‚
â”‚       â–¼                                                      â”‚
â”‚  âŒ ì‹¤íŒ¨ (172.x.x.xëŠ” Worker PC ë‚´ë¶€ Docker IP)             â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•´ê²°:**
Workerì˜ docker-compose.worker.ymlì—ì„œ `network_mode: host` ì‚¬ìš©:
```yaml
services:
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://192.168.55.114:9000
      - HDFS_CONF_dfs_replication=2
      - HDFS_CONF_dfs_datanode_address=0.0.0.0:9866
      - HDFS_CONF_dfs_datanode_http_address=0.0.0.0:9864
      - HDFS_CONF_dfs_datanode_ipc_address=0.0.0.0:9867
      - HDFS_CONF_dfs_datanode_use_datanode_hostname=false
      - HDFS_CONF_dfs_client_use_datanode_hostname=false
    volumes:
      - datanode_data:/hadoop/dfs/data
    network_mode: host    # â† í•µì‹¬!

  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker
    environment:
      - SPARK_MASTER=spark://192.168.55.114:7077
    volumes:
      - ./spark-jobs:/opt/spark-jobs
    network_mode: host    # â† í•µì‹¬!

volumes:
  datanode_data:
```

**network_mode: host ì„¤ëª…:**

| ëª¨ë“œ | ì„¤ëª… | IP ì˜ˆì‹œ |
|------|------|---------|
| bridge (ê¸°ë³¸) | Docker ê°€ìƒ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš© | 172.19.0.3 |
| host | í˜¸ìŠ¤íŠ¸ ë„¤íŠ¸ì›Œí¬ ì§ì ‘ ì‚¬ìš© | 192.168.55.158 |

**ì£¼ì˜ì‚¬í•­:**
- `network_mode: host` ì‚¬ìš© ì‹œ `ports` ì„¤ì • ë¶ˆí•„ìš” (ì¶©ëŒ ë°œìƒ)
- ì»¨í…Œì´ë„ˆê°€ í˜¸ìŠ¤íŠ¸ì˜ ëª¨ë“  í¬íŠ¸ì— ì§ì ‘ ë°”ì¸ë”©ë¨
- Linuxì—ì„œë§Œ ì§€ì› (Mac/Windows ë¯¸ì§€ì›)

**ì ìš©:**
```bash
# Workerì—ì„œ ì‹¤í–‰ (ë¦¬ëˆ…ìŠ¤ A, B ë‘˜ ë‹¤)
docker compose -f docker-compose.worker.yml down
docker compose -f docker-compose.worker.yml up -d
```

**í™•ì¸:**
```bash
# Masterì—ì„œ DataNode í™•ì¸
docker exec namenode hdfs dfsadmin -report
# Live datanodesì— ì‹¤ì œ IP(192.168.55.x)ê°€ ë³´ì—¬ì•¼ í•¨

# Spark UIì—ì„œ Worker í™•ì¸
http://192.168.55.114:8082
# Workersì— ì‹¤ì œ IPê°€ ë³´ì—¬ì•¼ í•¨
```

---

### ë¶„ì‚° í™˜ê²½ ë„¤íŠ¸ì›Œí¬ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì¢…í•©

ë¶„ì‚° í™˜ê²½ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë„¤íŠ¸ì›Œí¬ ê´€ë ¨ ë¬¸ì œë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

#### ë¬¸ì œ 5: Spark Driver í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨

**ì¦ìƒ:**
```
WARN TaskSchedulerImpl: Initial job has not accepted any resources;
check your cluster UI to ensure that workers are registered and have sufficient resources
```

Worker ë¡œê·¸:
```
--driver-url "spark://CoarseGrainedScheduler@jun-Victus-by-HP-Gaming-Laptop-16-r0xxx.local:44691"
```

**ì›ì¸:**
- Spark Masterê°€ `network_mode: host`ë¡œ ì‹¤í–‰ë˜ë©´ì„œ í˜¸ìŠ¤íŠ¸ëª… ì‚¬ìš©
- Workerì—ì„œ Masterì˜ í˜¸ìŠ¤íŠ¸ëª…ì„ í•´ì„ ëª»í•¨

**í•´ê²°:**
Workerì˜ docker-composeì— `extra_hosts` ì¶”ê°€:
```yaml
services:
  spark-worker:
    extra_hosts:
      - "jun-Victus-by-HP-Gaming-Laptop-16-r0xxx.local:192.168.55.114"
```

ë˜ëŠ” Worker PCì˜ /etc/hostsì— ì¶”ê°€:
```bash
echo "192.168.55.114 jun-Victus-by-HP-Gaming-Laptop-16-r0xxx.local" | sudo tee -a /etc/hosts
```

---

#### ë¬¸ì œ 6: DataNode ê°„ ë³µì œ ì‹¤íŒ¨

**ì¦ìƒ:**
```
Starting thread to transfer blk_xxx to 192.168.55.158:9866
java.nio.channels.UnresolvedAddressException
```

**ì›ì¸:**
- HDFS ë³µì œ íŒ©í„°ê°€ 2ë¡œ ì„¤ì •ë¨
- DataNodeê°€ ë‹¤ë¥¸ DataNodeë¡œ ë¸”ë¡ ë³µì œ ì‹œë„
- Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ ë‹¤ë¥¸ Worker IPë¥¼ í•´ì„ ëª»í•¨

**í•´ê²°:**
ê° Workerì˜ docker-composeì— ë‹¤ë¥¸ Worker IP ì¶”ê°€:
```yaml
# Worker 1 (192.168.55.158)
services:
  datanode:
    extra_hosts:
      - "worker2:192.168.55.9"

# Worker 2 (192.168.55.9)
services:
  datanode:
    extra_hosts:
      - "worker1:192.168.55.158"
```

---

#### ë¬¸ì œ 7: Kafka ì˜¤í”„ì…‹ ë¶ˆì¼ì¹˜

**ì¦ìƒ:**
```
ERROR: Partition logs.raw-0's offset was changed from 8400 to 5100,
some data may have been missed.
```

**ì›ì¸:**
- Spark Streaming Jobì´ ë¹„ì •ìƒ ì¢…ë£Œ
- ì²´í¬í¬ì¸íŠ¸ì˜ ì˜¤í”„ì…‹ê³¼ Kafkaì˜ ì‹¤ì œ ì˜¤í”„ì…‹ ë¶ˆì¼ì¹˜
- Kafka ë°ì´í„°ê°€ retention ì •ì±…ìœ¼ë¡œ ì‚­ì œë¨

**í•´ê²°:**

1. ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ:
```bash
docker exec namenode hdfs dfs -rm -r /checkpoints/raw_logs
docker exec namenode hdfs dfs -mkdir -p /checkpoints/raw_logs
```

2. Spark Jobì— failOnDataLoss ì˜µì…˜ ì¶”ê°€:
```python
kafka_df = spark.readStream \
    .format("kafka") \
    .option("failOnDataLoss", "false") \  # ë°ì´í„° ì†ì‹¤ í—ˆìš©
    .load()
```

---

#### ë¬¸ì œ 8: Spark Job IP vs í˜¸ìŠ¤íŠ¸ëª… ë¬¸ì œ

**ì¦ìƒ:**
```
java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode
```

**ì›ì¸:**
- Spark Masterê°€ `network_mode: host`ë¡œ ë³€ê²½ë˜ë©´ì„œ Docker ë‚´ë¶€ DNS ì‚¬ìš© ë¶ˆê°€
- Spark Jobì—ì„œ `namenode`, `kafka` ë“± Docker ì„œë¹„ìŠ¤ëª… ì‚¬ìš©

**í•´ê²°:**
Spark Job íŒŒì¼ì—ì„œ ì„œë¹„ìŠ¤ëª… ëŒ€ì‹  ì‹¤ì œ IP ì‚¬ìš©:
```python
# ìˆ˜ì • ì „
.config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000")
.option("kafka.bootstrap.servers", "kafka:9092")

# ìˆ˜ì • í›„
.config("spark.hadoop.fs.defaultFS", "hdfs://192.168.55.114:9000")
.option("kafka.bootstrap.servers", "192.168.55.114:9092")
```

---

### ë¶„ì‚° í™˜ê²½ íŒŒì¼ êµ¬ì¡°
```
deploy/
â”œâ”€â”€ docker-compose.master.yml    # Master ë…¸ë“œìš©
â”œâ”€â”€ docker-compose.worker1.yml   # Worker 1 (192.168.55.158)ìš©
â””â”€â”€ docker-compose.worker2.yml   # Worker 2 (192.168.55.9)ìš©
```

### ë¶„ì‚° í™˜ê²½ ì‹¤í–‰ ìˆœì„œ
```bash
# 1. Master ë¨¼ì € ì‹¤í–‰
cd deploy
docker compose -f docker-compose.master.yml up -d

# 2. Worker 1 ì‹¤í–‰ (192.168.55.158ì—ì„œ)
docker compose -f docker-compose.worker1.yml up -d

# 3. Worker 2 ì‹¤í–‰ (192.168.55.9ì—ì„œ)
docker compose -f docker-compose.worker2.yml up -d

# 4. ìƒíƒœ í™•ì¸
docker exec namenode hdfs dfsadmin -report
# Spark UI: http://192.168.55.114:8082
```

### ë„¤íŠ¸ì›Œí¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

| í•­ëª© | Master | Worker 1 | Worker 2 |
|------|--------|----------|----------|
| **Master í˜¸ìŠ¤íŠ¸ëª… ë“±ë¡** | - | extra_hosts ë˜ëŠ” /etc/hosts | extra_hosts ë˜ëŠ” /etc/hosts |
| **ë‹¤ë¥¸ Worker IP ë“±ë¡** | - | worker2 IP | worker1 IP |
| **network_mode** | host (spark-masterë§Œ) | host | host |
| **Spark Job IP ì‚¬ìš©** | 192.168.55.114 | - | - |
