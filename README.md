# Distributed Log Pipeline

> ë¶„ì‚°ì²˜ë¦¬ ì‹œìŠ¤í…œ(Kubernetes, Hadoop, Spark)ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ì‹¤ìŠµ í”„ë¡œì íŠ¸

---

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

### ëª©ì 
Kubernetes, Hadoop, Sparkë¥¼ í™œìš©í•œ ë¶„ì‚° ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ì§ì ‘ êµ¬ì¶•í•˜ê³  ìš´ì˜í•´ë³´ë©° ë¶„ì‚°ì²˜ë¦¬ ì‹œìŠ¤í…œì˜ í•µì‹¬ ê°œë…ì„ í•™ìŠµí•©ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥
- ë°°ì¹˜ ê¸°ë°˜ ë¡œê·¸/ì´ë²¤íŠ¸ ë°ì´í„° ìë™ ìƒì„±
- ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ë° ë©”ì‹œì§€ í ì²˜ë¦¬
- ë¶„ì‚° ì €ì¥ì†Œ(HDFS)ì— ë°ì´í„° ì ì¬
- Sparkë¥¼ í™œìš©í•œ ì‹¤ì‹œê°„/ë°°ì¹˜ ë°ì´í„° ë¶„ì„
- Grafana ê¸°ë°˜ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ì „ì²´ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         System Architecture                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚       â”‚            ë¦¬ëˆ…ìŠ¤ ìš°ë¶„íˆ¬ - ë…¸íŠ¸ë¶ (Master)               â”‚
â”‚   Data          â”‚       â”‚                                         â”‚
â”‚   Generator     â”‚       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   Server        â”‚ â”€â”€â”€â”€â–¶ â”‚  â”‚   Java Backend (Spring Boot)      â”‚  â”‚
â”‚                 â”‚ HTTP  â”‚  â”‚   â€¢ REST API ìˆ˜ì§‘ ì„œë²„             â”‚  â”‚
â”‚  (Python)       â”‚       â”‚  â”‚   â€¢ Kafka Producer                â”‚  â”‚
â”‚  â€¢ FastAPI      â”‚       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â€¢ ë°°ì¹˜ ìŠ¤ì¼€ì¤„ëŸ¬ â”‚       â”‚                    â”‚                    â”‚
â”‚  â€¢ ë°ì´í„° ìƒì„±ê¸° â”‚       â”‚                    â–¼                    â”‚
â”‚                 â”‚       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚  â”‚   Kubernetes Control Plane        â”‚  â”‚
                          â”‚  â”‚   â€¢ API Server                    â”‚  â”‚
                          â”‚  â”‚   â€¢ Scheduler                     â”‚  â”‚
                          â”‚  â”‚   â€¢ etcd                          â”‚  â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                          â”‚                    â”‚                    â”‚
                          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                          â”‚  â”‚   Kafka (KRaft ëª¨ë“œ)               â”‚  â”‚
                          â”‚  â”‚   â€¢ ë©”ì‹œì§€ í                      â”‚  â”‚
                          â”‚  â”‚   â€¢ ì‹¤ì‹œê°„ ë°ì´í„° ë²„í¼ë§           â”‚  â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                          â”‚                    â”‚                    â”‚
                          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                          â”‚  â”‚   Spark Driver + Airflow          â”‚  â”‚
                          â”‚  â”‚   â€¢ ì‘ì—… ìŠ¤ì¼€ì¤„ë§                  â”‚  â”‚
                          â”‚  â”‚   â€¢ ë¶„ì„ Job ê´€ë¦¬                  â”‚  â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                          â”‚                    â”‚                    â”‚
                          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                          â”‚  â”‚   Monitoring Stack                â”‚  â”‚
                          â”‚  â”‚   â€¢ Grafana (ì‹œê°í™”)              â”‚  â”‚
                          â”‚  â”‚   â€¢ Prometheus (ë©”íŠ¸ë¦­ ìˆ˜ì§‘)       â”‚  â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚                                       â”‚
                          â–¼                                       â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   ë¦¬ëˆ…ìŠ¤ PC - A      â”‚             â”‚   ë¦¬ëˆ…ìŠ¤ PC - B      â”‚
               â”‚   (Worker Node 1)   â”‚             â”‚   (Worker Node 2)   â”‚
               â”‚                     â”‚             â”‚                     â”‚
               â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚             â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
               â”‚ â”‚ K8s Worker      â”‚ â”‚             â”‚ â”‚ K8s Worker      â”‚ â”‚
               â”‚ â”‚ (kubelet)       â”‚ â”‚             â”‚ â”‚ (kubelet)       â”‚ â”‚
               â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
               â”‚                     â”‚             â”‚                     â”‚
               â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚             â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
               â”‚ â”‚ HDFS DataNode   â”‚ â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ â”‚ HDFS DataNode   â”‚ â”‚
               â”‚ â”‚ (ë°ì´í„° ì €ì¥)    â”‚ â”‚  Replicationâ”‚ â”‚ (ë°ì´í„° ì €ì¥)    â”‚ â”‚
               â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
               â”‚                     â”‚             â”‚                     â”‚
               â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚             â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
               â”‚ â”‚ Spark Executor  â”‚ â”‚             â”‚ â”‚ Spark Executor  â”‚ â”‚
               â”‚ â”‚ (ë°ì´í„° ì²˜ë¦¬)    â”‚ â”‚             â”‚ â”‚ (ë°ì´í„° ì²˜ë¦¬)    â”‚ â”‚
               â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ë°ì´í„° í”Œë¡œìš°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Data Flow                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. ë°ì´í„° ìƒì„± (Python Generator)
   â”‚
   â”‚  ë°°ì¹˜ ìŠ¤ì¼€ì¤„ (ë§¤ Nì´ˆ/ë¶„ë§ˆë‹¤)
   â”‚  â€¢ ì‹œìŠ¤í…œ ë¡œê·¸ ìƒì„±
   â”‚  â€¢ ì‚¬ìš©ì í™œë™ ì´ë²¤íŠ¸ ìƒì„±
   â–¼
2. ë°ì´í„° ìˆ˜ì§‘ (Java Spring Boot)
   â”‚
   â”‚  REST APIë¡œ ìˆ˜ì‹ 
   â”‚  â€¢ ë°ì´í„° ê²€ì¦
   â”‚  â€¢ Kafkaë¡œ ë°œí–‰
   â–¼
3. ë©”ì‹œì§€ í (Kafka)
   â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                      â”‚                      â”‚
   â–¼                      â–¼                      â–¼
4a. ì‹¤ì‹œê°„ ì²˜ë¦¬       4b. ë°°ì¹˜ ì €ì¥          4c. HDFS ì ì¬
    (Spark Streaming)     (Kafka â†’ HDFS)         (ì›ë³¸ ë³´ê´€)
    â”‚                      â”‚                      â”‚
    â”‚ â€¢ 5ì´ˆ ìœˆë„ìš° ì§‘ê³„     â”‚ â€¢ ì‹œê°„ë³„ íŒŒí‹°ì…˜      â”‚ â€¢ ì¥ê¸° ë³´ê´€
    â”‚ â€¢ ì´ìƒ íƒì§€          â”‚ â€¢ ì••ì¶• ì €ì¥          â”‚ â€¢ ì¬ì²˜ë¦¬ìš©
    â–¼                      â–¼                      â–¼
5. ê²°ê³¼ ì €ì¥
   â”‚
   â”œâ”€â”€ Prometheus (ë©”íŠ¸ë¦­)
   â”œâ”€â”€ HDFS (ë¶„ì„ ê²°ê³¼)
   â””â”€â”€ PostgreSQL (ì§‘ê³„ ë°ì´í„°)
   â”‚
   â–¼
6. ì‹œê°í™” (Grafana)
   â”‚
   â€¢ ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ
   â€¢ ì•Œë¦¼ ì„¤ì •
```

---

## ğŸ–¥ï¸ PCë³„ ì—­í•  ë° êµ¬ì„±

### ì—­í•  ë¶„ë°°

| PC | ì—­í•  | ì£¼ìš” ì»´í¬ë„ŒíŠ¸ | ê¶Œì¥ ì‚¬ì–‘ |
|-----|------|-------------|----------|
| **ë…¸íŠ¸ë¶** | Master + Generator | K8s Control Plane, Java Backend, Kafka, Spark Driver, Monitoring | 8GB+ RAM, 4+ Core |
| **ë¦¬ëˆ…ìŠ¤ A** | Worker Node 1 | K8s Worker, HDFS DataNode, Spark Executor | 4GB+ RAM, 2+ Core |
| **ë¦¬ëˆ…ìŠ¤ B** | Worker Node 2 | K8s Worker, HDFS DataNode, Spark Executor | 4GB+ RAM, 2+ Core |

### ë„¤íŠ¸ì›Œí¬ êµ¬ì„±

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Network Topology                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         192.168.x.0/24 (ì˜ˆì‹œ)
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚             â”‚
    â–¼             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚Master â”‚    â”‚Worker1â”‚    â”‚Worker2â”‚
â”‚  .10  â”‚    â”‚  .11  â”‚    â”‚  .12  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”˜

í•„ìš” í¬íŠ¸:
â€¢ 6443  : Kubernetes API
â€¢ 9092  : Kafka
â€¢ 9870  : HDFS NameNode Web UI
â€¢ 8080  : Spark Master Web UI
â€¢ 3000  : Grafana
â€¢ 8081  : Java Backend API
```

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
distributed-log-pipeline/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml              # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©
â”œâ”€â”€ .env.example                    # í™˜ê²½ë³€ìˆ˜ í…œí”Œë¦¿
â”‚
â”œâ”€â”€ docs/                           # ë¬¸ì„œ
â”‚   â”œâ”€â”€ SETUP_MASTER.md            # ë§ˆìŠ¤í„° ë…¸ë“œ ì„¤ì • ê°€ì´ë“œ
â”‚   â”œâ”€â”€ SETUP_WORKER.md            # ì›Œì»¤ ë…¸ë“œ ì„¤ì • ê°€ì´ë“œ
â”‚   â”œâ”€â”€ TROUBLESHOOTING.md         # ë¬¸ì œ í•´ê²° ê°€ì´ë“œ
â”‚   â””â”€â”€ ARCHITECTURE.md            # ìƒì„¸ ì•„í‚¤í…ì²˜ ë¬¸ì„œ
â”‚
â”œâ”€â”€ generator/                      # ë°ì´í„° ìƒì„±ê¸° (Python)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                # FastAPI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”‚   â”‚   â”œâ”€â”€ scheduler.py           # ë°°ì¹˜ ìŠ¤ì¼€ì¤„ëŸ¬
â”‚   â”‚   â”œâ”€â”€ generators/
â”‚   â”‚   â”‚   â”œâ”€â”€ log_generator.py   # ë¡œê·¸ ë°ì´í„° ìƒì„±
â”‚   â”‚   â”‚   â””â”€â”€ event_generator.py # ì´ë²¤íŠ¸ ë°ì´í„° ìƒì„±
â”‚   â”‚   â””â”€â”€ config.py              # ì„¤ì •
â”‚   â””â”€â”€ tests/
â”‚
â”œâ”€â”€ backend/                        # ìˆ˜ì§‘ ì„œë²„ (Java Spring Boot)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ pom.xml
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ main/
â”‚   â”‚       â”œâ”€â”€ java/
â”‚   â”‚       â”‚   â””â”€â”€ com/pipeline/
â”‚   â”‚       â”‚       â”œâ”€â”€ PipelineApplication.java
â”‚   â”‚       â”‚       â”œâ”€â”€ controller/
â”‚   â”‚       â”‚       â”‚   â””â”€â”€ CollectorController.java
â”‚   â”‚       â”‚       â”œâ”€â”€ service/
â”‚   â”‚       â”‚       â”‚   â””â”€â”€ KafkaProducerService.java
â”‚   â”‚       â”‚       â”œâ”€â”€ model/
â”‚   â”‚       â”‚       â”‚   â”œâ”€â”€ LogEvent.java
â”‚   â”‚       â”‚       â”‚   â””â”€â”€ ActivityEvent.java
â”‚   â”‚       â”‚       â””â”€â”€ config/
â”‚   â”‚       â”‚           â””â”€â”€ KafkaConfig.java
â”‚   â”‚       â””â”€â”€ resources/
â”‚   â”‚           â””â”€â”€ application.yml
â”‚   â””â”€â”€ tests/
â”‚
â”œâ”€â”€ spark-jobs/                     # Spark ì‘ì—… (PySpark)
â”‚   â”œâ”€â”€ streaming/
â”‚   â”‚   â”œâ”€â”€ realtime_aggregator.py # ì‹¤ì‹œê°„ ì§‘ê³„
â”‚   â”‚   â””â”€â”€ anomaly_detector.py    # ì´ìƒ íƒì§€
â”‚   â”œâ”€â”€ batch/
â”‚   â”‚   â”œâ”€â”€ daily_report.py        # ì¼ë³„ ë¦¬í¬íŠ¸
â”‚   â”‚   â””â”€â”€ weekly_analysis.py     # ì£¼ê°„ ë¶„ì„
â”‚   â””â”€â”€ common/
â”‚       â””â”€â”€ spark_utils.py         # ê³µí†µ ìœ í‹¸
â”‚
â”œâ”€â”€ airflow/                        # ì›Œí¬í”Œë¡œìš° ìŠ¤ì¼€ì¤„ë§
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ daily_pipeline.py
â”‚   â”‚   â””â”€â”€ weekly_pipeline.py
â”‚   â””â”€â”€ plugins/
â”‚
â”œâ”€â”€ kubernetes/                     # K8s ë§¤ë‹ˆí˜ìŠ¤íŠ¸
â”‚   â”œâ”€â”€ namespace.yaml
â”‚   â”œâ”€â”€ master/                    # ë§ˆìŠ¤í„° ë…¸ë“œìš©
â”‚   â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â”‚   â”œâ”€â”€ zookeeper.yaml
â”‚   â”‚   â”‚   â””â”€â”€ kafka.yaml
â”‚   â”‚   â”œâ”€â”€ spark/
â”‚   â”‚   â”‚   â””â”€â”€ spark-master.yaml
â”‚   â”‚   â””â”€â”€ monitoring/
â”‚   â”‚       â”œâ”€â”€ prometheus.yaml
â”‚   â”‚       â””â”€â”€ grafana.yaml
â”‚   â”œâ”€â”€ worker/                    # ì›Œì»¤ ë…¸ë“œìš©
â”‚   â”‚   â”œâ”€â”€ hdfs/
â”‚   â”‚   â”‚   â””â”€â”€ datanode.yaml
â”‚   â”‚   â””â”€â”€ spark/
â”‚   â”‚       â””â”€â”€ spark-worker.yaml
â”‚   â””â”€â”€ common/                    # ê³µí†µ
â”‚       â”œâ”€â”€ configmaps.yaml
â”‚       â””â”€â”€ secrets.yaml
â”‚
â”œâ”€â”€ hadoop/                         # Hadoop ì„¤ì •
â”‚   â”œâ”€â”€ core-site.xml
â”‚   â”œâ”€â”€ hdfs-site.xml
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ start-namenode.sh
â”‚       â””â”€â”€ start-datanode.sh
â”‚
â”œâ”€â”€ monitoring/                     # ëª¨ë‹ˆí„°ë§ ì„¤ì •
â”‚   â”œâ”€â”€ grafana/
â”‚   â”‚   â”œâ”€â”€ provisioning/
â”‚   â”‚   â””â”€â”€ dashboards/
â”‚   â”‚       â”œâ”€â”€ overview.json      # ì „ì²´ í˜„í™© ëŒ€ì‹œë³´ë“œ
â”‚   â”‚       â”œâ”€â”€ kafka.json         # Kafka ëª¨ë‹ˆí„°ë§
â”‚   â”‚       â””â”€â”€ spark.json         # Spark ëª¨ë‹ˆí„°ë§
â”‚   â””â”€â”€ prometheus/
â”‚       â””â”€â”€ prometheus.yml
â”‚
â””â”€â”€ scripts/                        # ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸
    â”œâ”€â”€ setup-master.sh            # ë§ˆìŠ¤í„° ì´ˆê¸° ì„¤ì •
    â”œâ”€â”€ setup-worker.sh            # ì›Œì»¤ ì´ˆê¸° ì„¤ì •
    â”œâ”€â”€ deploy-all.sh              # ì „ì²´ ë°°í¬
    â””â”€â”€ cleanup.sh                 # ì •ë¦¬
```

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ì‚¬ì „ ìš”êµ¬ì‚¬í•­

**ëª¨ë“  PC ê³µí†µ:**
```bash
# Docker ì„¤ì¹˜
curl -fsSL https://get.docker.com | sh
sudo usermod -aG docker $USER

# kubectl ì„¤ì¹˜
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
```

### Step 1: í”„ë¡œì íŠ¸ í´ë¡  (ëª¨ë“  PC)

```bash
git clone https://github.com/your-repo/distributed-log-pipeline.git
cd distributed-log-pipeline
cp .env.example .env
# .env íŒŒì¼ì—ì„œ ê° PCì˜ IP ì£¼ì†Œ ì„¤ì •
```

### Step 2: Master ë…¸ë“œ ì„¤ì • (ë…¸íŠ¸ë¶)

```bash
# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export NODE_ROLE=master
export MASTER_IP=192.168.x.10  # ì‹¤ì œ IPë¡œ ë³€ê²½

# Kubernetes í´ëŸ¬ìŠ¤í„° ì´ˆê¸°í™”
./scripts/setup-master.sh

# ë§ˆìŠ¤í„° ì»´í¬ë„ŒíŠ¸ ë°°í¬
kubectl apply -f kubernetes/master/

# Java Backend ì‹¤í–‰
cd backend
./mvnw spring-boot:run

# ë°ì´í„° ìƒì„±ê¸° ì‹¤í–‰ (ë³„ë„ í„°ë¯¸ë„)
cd generator
pip install -r requirements.txt
python -m app.main
```

### Step 3: Worker ë…¸ë“œ ì„¤ì • (ë¦¬ëˆ…ìŠ¤ A, B)

```bash
# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export NODE_ROLE=worker
export MASTER_IP=192.168.x.10  # ë§ˆìŠ¤í„° IP

# í´ëŸ¬ìŠ¤í„° ì¡°ì¸
./scripts/setup-worker.sh

# ì›Œì»¤ ì»´í¬ë„ŒíŠ¸ ë°°í¬
kubectl apply -f kubernetes/worker/
```

### Step 4: ë™ì‘ í™•ì¸

```bash
# í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
kubectl get nodes
kubectl get pods -A

# HDFS ìƒíƒœ í™•ì¸
hdfs dfsadmin -report

# Kafka í† í”½ í™•ì¸
kafka-topics.sh --list --bootstrap-server localhost:9092

# Grafana ì ‘ì†
# ë¸Œë¼ìš°ì €ì—ì„œ http://<MASTER_IP>:3000 ì ‘ì†
# ê¸°ë³¸ ê³„ì •: admin / admin
```

---

## ğŸ“Š ìƒì„± ë°ì´í„° í˜•ì‹

### ì‹œìŠ¤í…œ ë¡œê·¸

```json
{
  "timestamp": "2024-01-15T10:30:00.123Z",
  "level": "INFO",
  "service": "api-gateway",
  "host": "worker-1",
  "message": "Request processed successfully",
  "metadata": {
    "request_id": "uuid-1234",
    "response_time_ms": 45,
    "status_code": 200,
    "endpoint": "/api/users",
    "method": "GET"
  }
}
```

### ì‚¬ìš©ì í™œë™ ì´ë²¤íŠ¸

```json
{
  "event_id": "evt-uuid-5678",
  "timestamp": "2024-01-15T10:30:05.456Z",
  "user_id": "user_12345",
  "session_id": "sess_abcde",
  "event_type": "CLICK",
  "event_data": {
    "page": "/products/123",
    "element": "add_to_cart_button",
    "position": {"x": 450, "y": 320}
  },
  "device": {
    "type": "mobile",
    "os": "iOS",
    "browser": "Safari"
  }
}
```

---

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

### ì œê³µë˜ëŠ” ëŒ€ì‹œë³´ë“œ

| ëŒ€ì‹œë³´ë“œ | ì„¤ëª… | ì£¼ìš” ë©”íŠ¸ë¦­ |
|---------|------|------------|
| **Overview** | ì‹œìŠ¤í…œ ì „ì²´ í˜„í™© | ë…¸ë“œ ìƒíƒœ, ì²˜ë¦¬ëŸ‰, ì—ëŸ¬ìœ¨ |
| **Kafka** | ë©”ì‹œì§€ í ëª¨ë‹ˆí„°ë§ | í† í”½ë³„ ì²˜ë¦¬ëŸ‰, ì§€ì—°ì‹œê°„, ì»¨ìŠˆë¨¸ ë™ |
| **Spark** | ì²˜ë¦¬ ì‘ì—… ëª¨ë‹ˆí„°ë§ | ì‘ì—… ìƒíƒœ, ì²˜ë¦¬ ì‹œê°„, ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ |
| **HDFS** | ì €ì¥ì†Œ ëª¨ë‹ˆí„°ë§ | ìš©ëŸ‰, ë³µì œ ìƒíƒœ, ë…¸ë“œ ìƒíƒœ |

### ì•Œë¦¼ ì„¤ì •

```yaml
# ê¸°ë³¸ ì œê³µ ì•Œë¦¼ ê·œì¹™
alerts:
  - name: HighErrorRate
    condition: error_rate > 5%
    duration: 5m
    
  - name: KafkaLag
    condition: consumer_lag > 10000
    duration: 10m
    
  - name: SparkJobFailed
    condition: job_status == "FAILED"
```

---

## ğŸ”§ ì£¼ìš” ì„¤ì •

### ë°°ì¹˜ ìŠ¤ì¼€ì¤„ ì„¤ì • (generator/app/config.py)

```python
BATCH_CONFIG = {
    "log_interval_seconds": 5,      # ë¡œê·¸ ìƒì„± ì£¼ê¸°
    "event_interval_seconds": 10,   # ì´ë²¤íŠ¸ ìƒì„± ì£¼ê¸°
    "batch_size": 100,              # ë°°ì¹˜ë‹¹ ë°ì´í„° ìˆ˜
    "target_backend_url": "http://master:8081/api/collect"
}
```

### Kafka í† í”½ ì„¤ì •

| í† í”½ | íŒŒí‹°ì…˜ | ë³µì œ ê³„ìˆ˜ | ë³´ì¡´ ê¸°ê°„ |
|------|--------|----------|----------|
| logs.raw | 3 | 2 | 7ì¼ |
| events.activity | 3 | 2 | 7ì¼ |
| alerts.realtime | 1 | 2 | 1ì¼ |

---

## ğŸ› ï¸ ê°œë°œ ê°€ì´ë“œ

### ë¡œì»¬ í…ŒìŠ¤íŠ¸ (Docker Compose)

```bash
# ì „ì²´ ìŠ¤íƒ ë¡œì»¬ ì‹¤í–‰ (ë‹¨ì¼ PC í…ŒìŠ¤íŠ¸ìš©)
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f

# ì¢…ë£Œ
docker-compose down -v
```

### ìƒˆë¡œìš´ Spark Job ì¶”ê°€

```python
# spark-jobs/batch/my_new_job.py

from pyspark.sql import SparkSession
from common.spark_utils import get_spark_session, read_from_hdfs

def main():
    spark = get_spark_session("MyNewJob")
    
    # HDFSì—ì„œ ë°ì´í„° ì½ê¸°
    df = read_from_hdfs(spark, "/data/logs/2024/01/15")
    
    # ì²˜ë¦¬ ë¡œì§
    result = df.groupBy("level").count()
    
    # ê²°ê³¼ ì €ì¥
    result.write.mode("overwrite").parquet("/data/results/my_job")

if __name__ == "__main__":
    main()
```

---

## ğŸ“š í•™ìŠµ ë¡œë“œë§µ

### Week 1-2: í™˜ê²½ êµ¬ì¶•
- [ ] 3ëŒ€ PC ë„¤íŠ¸ì›Œí¬ êµ¬ì„±
- [ ] Docker, kubectl ì„¤ì¹˜
- [ ] Kubernetes í´ëŸ¬ìŠ¤í„° êµ¬ì¶•
- [ ] ê¸°ë³¸ í†µì‹  í…ŒìŠ¤íŠ¸

### Week 3: ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê¸°ì´ˆ
- [ ] Kafka ì„¤ì¹˜ ë° í† í”½ ìƒì„±
- [ ] Java Backend ê°œë°œ
- [ ] Python Generator ê°œë°œ
- [ ] ê¸°ë³¸ ë°ì´í„° í”Œë¡œìš° í™•ì¸

### Week 4: HDFS ì—°ë™
- [ ] HDFS í´ëŸ¬ìŠ¤í„° êµ¬ì„±
- [ ] Kafka â†’ HDFS ì—°ë™
- [ ] ë°ì´í„° ì ì¬ í™•ì¸

### Week 5: Spark ì²˜ë¦¬
- [ ] Spark Streaming ì‘ì—… ê°œë°œ
- [ ] Batch ì‘ì—… ê°œë°œ
- [ ] Airflow ìŠ¤ì¼€ì¤„ë§

### Week 6: ëª¨ë‹ˆí„°ë§
- [ ] Prometheus + Grafana êµ¬ì„±
- [ ] ëŒ€ì‹œë³´ë“œ êµ¬ì„±
- [ ] ì•Œë¦¼ ì„¤ì •
- [ ] ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸

---

## â“ FAQ

### Q: ë‹¨ì¼ PCì—ì„œë„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆë‚˜ìš”?
A: ë„¤, `docker-compose up`ìœ¼ë¡œ ë¡œì»¬ì—ì„œ ì „ì²´ ìŠ¤íƒì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Q: ìµœì†Œ ì‚¬ì–‘ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?
A: Master 8GB RAM, Worker ê° 4GB RAMì„ ê¶Œì¥í•©ë‹ˆë‹¤. ë” ë‚®ì€ ì‚¬ì–‘ì—ì„œë„ ë™ì‘í•˜ì§€ë§Œ ì„±ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Q: Windowsì—ì„œë„ ê°€ëŠ¥í•œê°€ìš”?
A: WSL2 + Docker Desktop í™˜ê²½ì—ì„œ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë‹¨, ë„¤íŠ¸ì›Œí¬ ì„¤ì •ì´ ë³µì¡í•´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
---

## ğŸ”§ ê¸°ìˆ  ìƒì„¸ ì„¤ëª…

### PySparkëŠ” ì–´ë–»ê²Œ ë™ì‘í•˜ë‚˜ìš”?

SparkëŠ” Java/Scalaë¡œ ë§Œë“¤ì–´ì¡Œì§€ë§Œ, Pythonìœ¼ë¡œ ì‘ì„±í•œ ì½”ë“œë„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### ë™ì‘ êµ¬ì¡°
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PySpark êµ¬ì¡°                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚   Python ì½”ë“œ (ìš°ë¦¬ê°€ ì‘ì„±)                               â”‚
â”‚        â†“                                                 â”‚
â”‚   Py4J (Python â†” Java ë¸Œë¦¿ì§€)                            â”‚
â”‚        â†“                                                 â”‚
â”‚   Spark Core (Java/Scala - ì‹¤ì œ ì‹¤í–‰)                    â”‚
â”‚        â†“                                                 â”‚
â”‚   JVMì—ì„œ ë¶„ì‚° ì²˜ë¦¬                                       â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ì‹¤ì œ ì‹¤í–‰ íë¦„
```python
# ìš°ë¦¬ê°€ ì‘ì„±í•œ Python ì½”ë“œ
df.filter(col("level") == "ERROR").count()
```

ìœ„ ì½”ë“œê°€ ì‹¤í–‰ë˜ë©´:

1. **Python â†’ Py4J**: Python ì½”ë“œê°€ Py4J ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ Javaë¡œ ì „ë‹¬
2. **ì‹¤í–‰ ê³„íš ìƒì„±**: Spark(Java)ê°€ ìµœì í™”ëœ ì‹¤í–‰ ê³„íš ìƒì„±
3. **JVM ë¶„ì‚° ì²˜ë¦¬**: ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ëŠ” JVMì—ì„œ ë¶„ì‚° ì‹¤í–‰
4. **ê²°ê³¼ ë°˜í™˜**: ì²˜ë¦¬ ê²°ê³¼ë§Œ Pythonìœ¼ë¡œ ë°˜í™˜

#### ì„±ëŠ¥ ë¹„êµ

| êµ¬ë¶„ | PySpark | Scala/Java Spark |
|------|---------|------------------|
| **DataFrame API** | ê±°ì˜ ë™ì¼ | ê¸°ì¤€ |
| **RDD ì—°ì‚°** | ì•½ê°„ ëŠë¦¼ | ë¹ ë¦„ |
| **UDF (ì‚¬ìš©ì í•¨ìˆ˜)** | ëŠë¦¼ (ì§ë ¬í™” ì˜¤ë²„í—¤ë“œ) | ë¹ ë¦„ |
| **ê°œë°œ ì†ë„** | ë¹ ë¦„ | ëŠë¦¼ |

#### ì–¸ì œ ë¬´ì—‡ì„ ì„ íƒí• ê¹Œ?
```
DataFrame/SQL ìœ„ì£¼ ì‘ì—…     â†’ PySpark ì¶”ì²œ (ì¶©ë¶„í•œ ì„±ëŠ¥)
ë³µì¡í•œ ì»¤ìŠ¤í…€ ë¡œì§/UDF ë§ìŒ  â†’ Scala ê³ ë ¤
ML íŒŒì´í”„ë¼ì¸               â†’ PySpark (MLlib + Python ìƒíƒœê³„)
```

ë³¸ í”„ë¡œì íŠ¸ëŠ” DataFrame API ìœ„ì£¼ë¡œ ì‘ì—…í•˜ë¯€ë¡œ **PySparkë¡œ ì¶©ë¶„**í•©ë‹ˆë‹¤.

---

### Kafka í† í”½ íŒŒí‹°ì…˜ ì„¤ì • ì£¼ì˜ì‚¬í•­

#### ë¬¸ì œ ìƒí™©

Kafka í† í”½ì´ ì˜ë„í•œ íŒŒí‹°ì…˜ ìˆ˜ì™€ ë‹¤ë¥´ê²Œ ìƒì„±ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```
ì˜ˆìƒ: íŒŒí‹°ì…˜ 3ê°œë¡œ ë¶„ì‚° ì €ì¥
ì‹¤ì œ: íŒŒí‹°ì…˜ 1ê°œì— ëª¨ë“  ë°ì´í„° ì €ì¥
```

#### ì›ì¸
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ì„œë¹„ìŠ¤ ì‹œì‘ ìˆœì„œ ë¬¸ì œ                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Case 1: Generatorê°€ ë¨¼ì € ë°ì´í„° ì „ì†¡                    â”‚
â”‚  â†’ Kafkaê°€ í† í”½ ìë™ ìƒì„± (AUTO_CREATE_TOPICS=true)      â”‚
â”‚  â†’ ê¸°ë³¸ê°’ íŒŒí‹°ì…˜ 1ê°œë¡œ ìƒì„±                              â”‚
â”‚  â†’ Java Backendì˜ KafkaConfig ì„¤ì • ë¬´ì‹œë¨               â”‚
â”‚                                                          â”‚
â”‚  Case 2: Java Backendê°€ ë¨¼ì € ì—°ê²°                        â”‚
â”‚  â†’ KafkaConfig.javaì˜ NewTopic ì„¤ì • ì ìš©                â”‚
â”‚  â†’ íŒŒí‹°ì…˜ 3ê°œë¡œ í† í”½ ìƒì„±                                â”‚
â”‚  â†’ ë°ì´í„°ê°€ ì •ìƒì ìœ¼ë¡œ ë¶„ì‚° ì €ì¥                         â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### í•´ê²° ë°©ë²•

**ë°©ë²• 1: ì„œë¹„ìŠ¤ ì‹œì‘ ìˆœì„œ ë³´ì¥**
```yaml
# docker-compose.yml
generator:
  depends_on:
    backend:
      condition: service_healthy  # Backendê°€ healthy ìƒíƒœì¼ ë•Œë§Œ ì‹œì‘
```

**ë°©ë²• 2: í† í”½ ì‚¬ì „ ìƒì„±**
```bash
# Kafka ì»¨í…Œì´ë„ˆì—ì„œ ì§ì ‘ í† í”½ ìƒì„±
docker exec kafka /opt/kafka/bin/kafka-topics.sh \
  --create \
  --topic logs.raw \
  --partitions 3 \
  --replication-factor 1 \
  --bootstrap-server localhost:9092
```

**ë°©ë²• 3: ë³¼ë¥¨ ì´ˆê¸°í™” í›„ ì¬ì‹œì‘**
```bash
docker compose down -v  # ë³¼ë¥¨ê¹Œì§€ ì‚­ì œ
docker compose up -d    # ìƒˆë¡œ ì‹œì‘
```

#### íŒŒí‹°ì…˜ ë¶„ë°° ì›ë¦¬
```java
// KafkaProducerService.java
String key = logEvent.getService();  // key = service ì´ë¦„
kafkaTemplate.send(logsTopic, key, logEvent);
```

| Key (Service) | íŒŒí‹°ì…˜ í• ë‹¹ (hash % íŒŒí‹°ì…˜ìˆ˜) |
|---------------|------------------------------|
| api-gateway | 0, 1, ë˜ëŠ” 2 |
| user-service | 0, 1, ë˜ëŠ” 2 |
| order-service | 0, 1, ë˜ëŠ” 2 |

**ê°™ì€ keyëŠ” í•­ìƒ ê°™ì€ íŒŒí‹°ì…˜**ìœ¼ë¡œ ì „ì†¡ë˜ì–´ ë©”ì‹œì§€ ìˆœì„œê°€ ë³´ì¥ë©ë‹ˆë‹¤.

---

### Spark Streaming íŒŒí‹°ì…˜ ì €ì¥ ë¬¸ì œ í•´ê²°

#### ë¬¸ì œ í˜„ìƒ

HDFSì— ë°ì´í„°ê°€ ì €ì¥ë  ë•Œ íŒŒí‹°ì…˜ì´ ì œëŒ€ë¡œ ìƒì„±ë˜ì§€ ì•ŠëŠ” ë¬¸ì œ:
```
# ì˜ˆìƒí•œ ê²°ê³¼
/data/logs/raw/year=2026/month=1/day=11/hour=19/

# ì‹¤ì œ ê²°ê³¼
/data/logs/raw/year=__HIVE_DEFAULT_PARTITION__/month=__HIVE_DEFAULT_PARTITION__/...
```

#### ì›ì¸

Generatorê°€ ë³´ë‚´ëŠ” timestamp í˜•ì‹ê³¼ Sparkì—ì„œ íŒŒì‹±í•˜ëŠ” í˜•ì‹ì´ ë¶ˆì¼ì¹˜:
```python
# Generatorê°€ ë³´ë‚´ëŠ” í˜•ì‹ (Unix timestamp)
{
    "timestamp": 1768123166.291045000,  # epoch seconds
    ...
}

# ì²˜ìŒ Spark ì½”ë“œ (ISO í˜•ì‹ìœ¼ë¡œ íŒŒì‹± ì‹œë„)
.withColumn("event_time", to_timestamp(col("timestamp"), "yyyy-MM-dd'T'HH:mm:ss.SSSXXX"))
# â†’ íŒŒì‹± ì‹¤íŒ¨ â†’ null â†’ __HIVE_DEFAULT_PARTITION__
```

#### í•´ê²°

Unix timestampë¥¼ ì˜¬ë°”ë¥´ê²Œ íŒŒì‹±í•˜ë„ë¡ ìˆ˜ì •:
```python
# ìˆ˜ì • ì „ (ISO í˜•ì‹)
.withColumn("event_time", to_timestamp(col("timestamp"), "yyyy-MM-dd'T'HH:mm:ss.SSSXXX"))

# ìˆ˜ì • í›„ (Unix timestamp)
.withColumn("event_time", from_unixtime(col("timestamp")).cast("timestamp"))
```

ë˜í•œ ìŠ¤í‚¤ë§ˆì—ì„œ timestamp íƒ€ì…ë„ ë³€ê²½:
```python
# ìˆ˜ì • ì „
StructField("timestamp", StringType(), True)

# ìˆ˜ì • í›„
StructField("timestamp", DoubleType(), True)
```

#### ë””ë²„ê¹… ë°©ë²•
```bash
# 1. Generatorê°€ ë³´ë‚´ëŠ” ì‹¤ì œ ë°ì´í„° í˜•ì‹ í™•ì¸
curl -s http://<MASTER_IP>:8000/sample/log

# 2. Kafka UIì—ì„œ ë©”ì‹œì§€ ì§ì ‘ í™•ì¸
# http://<MASTER_IP>:8080 â†’ Topics â†’ logs.raw â†’ Messages
```

---

### Spark Job ë¦¬ì†ŒìŠ¤ ì ìœ  ë¬¸ì œ

#### ë¬¸ì œ í˜„ìƒ

Spark Job ì¬ì‹¤í–‰ ì‹œ ë¦¬ì†ŒìŠ¤ë¥¼ í• ë‹¹ë°›ì§€ ëª»í•˜ëŠ” ê²½ê³ :
```
WARN TaskSchedulerImpl: Initial job has not accepted any resources; 
check your cluster UI to ensure that workers are registered and have sufficient resources
```

#### ì›ì¸

ì´ì „ Spark Jobì´ ë¹„ì •ìƒ ì¢…ë£Œë˜ë©´ì„œ Workerì˜ ë¦¬ì†ŒìŠ¤ë¥¼ ê³„ì† ì ìœ í•˜ê³  ìˆìŒ.
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ë¦¬ì†ŒìŠ¤ ì ìœ  ìƒíƒœ                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  ì´ì „ Job (ì¢…ë£Œë¨)     í˜„ì¬ Job (ëŒ€ê¸°ì¤‘)                  â”‚
â”‚  â””â”€ Worker ë¦¬ì†ŒìŠ¤ ì ìœ   â””â”€ ë¦¬ì†ŒìŠ¤ í• ë‹¹ ëŒ€ê¸°               â”‚
â”‚     (24 cores, 1GB)       (í• ë‹¹ ë¶ˆê°€)                    â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### í•´ê²° ë°©ë²•

**ë°©ë²• 1: Spark ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘**
```bash
docker compose restart spark-master spark-worker
```

**ë°©ë²• 2: Spark Master UIì—ì„œ ì§ì ‘ ì¢…ë£Œ**
```
1. http://<MASTER_IP>:8082 ì ‘ì†
2. Running Applicationsì—ì„œ (kill) í´ë¦­
3. ìƒˆ Job ì‹¤í–‰
```

**ë°©ë²• 3: ì „ì²´ í´ëŸ¬ìŠ¤í„° ì¬ì‹œì‘**
```bash
docker compose down
docker compose up -d
```

#### ì˜ˆë°© ë°©ë²•

Spark Job ì¢…ë£Œ ì‹œ í•­ìƒ `Ctrl+C`ë¡œ ì •ìƒ ì¢…ë£Œí•˜ê³ , Spark UIì—ì„œ Applicationì´ Completedë¡œ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸.

---

### Airflow ì„¤ì • ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

#### Airflow ê¸°ë³¸ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì–¸ì–´** | Python |
| **ê¸°ë³¸ DB** | SQLite (ê°œë°œìš©) |
| **ìš´ì˜ DB** | PostgreSQL, MySQL (ê¶Œì¥) |

#### ë¬¸ì œ 1: DB ì´ˆê¸°í™” ì‹¤íŒ¨

**ì¦ìƒ:**
```
ERROR: You need to initialize the database. Please run `airflow db init`.
```

**ì›ì¸:**
- ì´ì „ ì‹¤í–‰ì—ì„œ DBê°€ ë¶ˆì™„ì „í•˜ê²Œ ìƒì„±ë¨
- `docker compose down`ì€ ë³¼ë¥¨ì„ ìœ ì§€í•˜ë¯€ë¡œ ë¶ˆì™„ì „í•œ DBê°€ ë‚¨ì•„ìˆìŒ

**í•´ê²°:**
```bash
# ë³¼ë¥¨ê¹Œì§€ ì‚­ì œí•´ì„œ ê¹¨ë—í•˜ê²Œ ì‹œì‘
docker compose down -v
docker compose up -d
```

#### ë¬¸ì œ 2: SQLite + LocalExecutor í˜¸í™˜ ë¶ˆê°€

**ì¦ìƒ:**
```
airflow.exceptions.AirflowConfigException: error: cannot use SQLite with the LocalExecutor
```

**ì›ì¸:**
- SQLiteëŠ” ë™ì‹œ ì“°ê¸°ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
- LocalExecutorëŠ” ë³‘ë ¬ ì‹¤í–‰ì„ ìœ„í•´ ë™ì‹œ DB ì ‘ê·¼ í•„ìš”
- ë”°ë¼ì„œ SQLite + LocalExecutor ì¡°í•©ì€ ë¶ˆê°€

**Executor ì¢…ë¥˜:**

| Executor | DB ìš”êµ¬ì‚¬í•­ | íŠ¹ì§• |
|----------|------------|------|
| **SequentialExecutor** | SQLite ê°€ëŠ¥ | ìˆœì°¨ ì‹¤í–‰, ê°œë°œìš© |
| **LocalExecutor** | PostgreSQL/MySQL í•„ìš” | ë³‘ë ¬ ì‹¤í–‰ |
| **CeleryExecutor** | PostgreSQL/MySQL + Redis | ë¶„ì‚° ì‹¤í–‰ |
| **KubernetesExecutor** | PostgreSQL/MySQL | K8s Podìœ¼ë¡œ ì‹¤í–‰ |

**í•´ê²°:**
```yaml
# docker-compose.yml
environment:
  # ìˆ˜ì • ì „ (ì—ëŸ¬ ë°œìƒ)
  - AIRFLOW__CORE__EXECUTOR=LocalExecutor
  
  # ìˆ˜ì • í›„ (ì •ìƒ ë™ì‘)
  - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
```

#### ë¬¸ì œ 3: DB ëª…ë ¹ì–´ ë²„ì „ ì°¨ì´

**ì¦ìƒ:**
```
DeprecationWarning: `db init` is deprecated. Use `db migrate` instead
```

**ì›ì¸:**
- Airflow 2.7.0ë¶€í„° `airflow db init`ì´ deprecated
- ìƒˆ ë²„ì „ì—ì„œëŠ” `airflow db migrate` ì‚¬ìš© ê¶Œì¥

**í•´ê²°:**
```yaml
# ë‘ ëª…ë ¹ì–´ ëª¨ë‘ ì‹¤í–‰í•˜ì—¬ í˜¸í™˜ì„± í™•ë³´
command: bash -c "airflow db init && airflow users create ... "
```

#### ìµœì¢… Airflow ì„¤ì • (docker-compose.yml)
```yaml
airflow:
  image: apache/airflow:2.7.0-python3.10
  container_name: airflow
  user: root
  ports:
    - "8084:8080"
  environment:
    - AIRFLOW__CORE__EXECUTOR=SequentialExecutor  # SQLiteì™€ í˜¸í™˜
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
    - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
    - AIRFLOW__CORE__LOAD_EXAMPLES=False
    - AIRFLOW__WEBSERVER__SECRET_KEY=mysecretkey123
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./spark-jobs:/opt/spark-jobs
    - /var/run/docker.sock:/var/run/docker.sock  # Docker ëª…ë ¹ ì‹¤í–‰ìš©
  command: bash -c "airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true && (airflow webserver &) && airflow scheduler"
```

#### Airflow ì ‘ì† ì •ë³´
```
URL: http://<MASTER_IP>:8084
ID: admin
PW: admin
```
